#!/usr/bin/env python3
"""
éªŒè¯å¤šGPUåˆ†å¸ƒå¼å·¥ä½œåŸç†
====================
éªŒè¯æ¯ä¸ªWorkerç¡®å®è¿è¡Œåœ¨ä¸åŒçš„ç‰©ç†GPUä¸Š
"""

import sys
import os
import torch
import logging
import time
import ray

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@ray.remote(num_gpus=1)
class GPUWorker:
    """ç®€å•çš„GPU Workeræ¥éªŒè¯GPUåˆ†é…"""
    
    def __init__(self, worker_id):
        self.worker_id = worker_id
        
    def get_gpu_info(self):
        """è·å–å½“å‰Workerçš„GPUä¿¡æ¯"""
        import torch
        import os
        
        result = {
            'worker_id': self.worker_id,
            'process_id': os.getpid(),
            'cuda_visible_devices': os.environ.get('CUDA_VISIBLE_DEVICES', 'Not set'),
            'cuda_available': torch.cuda.is_available(),
            'cuda_device_count': torch.cuda.device_count() if torch.cuda.is_available() else 0,
        }
        
        if torch.cuda.is_available() and torch.cuda.device_count() > 0:
            # åœ¨Workerå†…éƒ¨ï¼Œè®¾å¤‡æ€»æ˜¯cuda:0
            device = torch.cuda.current_device()
            props = torch.cuda.get_device_properties(device)
            
            result.update({
                'current_device': device,
                'device_name': props.name,
                'device_memory_gb': props.total_memory / (1024**3),
                'device_capability': f"{props.major}.{props.minor}",
            })
            
            # åˆ›å»ºä¸€ä¸ªå¼ é‡æ¥éªŒè¯GPUå·¥ä½œ
            test_tensor = torch.randn(100, 100).cuda()
            result['tensor_device'] = str(test_tensor.device)
            result['tensor_sum'] = float(test_tensor.sum())
            
        return result
    
    def run_simple_computation(self):
        """è¿è¡Œç®€å•è®¡ç®—æ¥éªŒè¯GPUå·¥ä½œ"""
        import torch
        import time
        
        if not torch.cuda.is_available():
            return {'error': 'CUDA not available'}
            
        start_time = time.time()
        
        # åˆ›å»ºå¤§å‹çŸ©é˜µè¿ç®—
        a = torch.randn(1000, 1000).cuda()
        b = torch.randn(1000, 1000).cuda()
        
        # æ‰§è¡ŒçŸ©é˜µä¹˜æ³•
        c = torch.matmul(a, b)
        result_sum = c.sum().item()
        
        end_time = time.time()
        
        return {
            'worker_id': self.worker_id,
            'computation_time': end_time - start_time,
            'result_sum': result_sum,
            'tensor_device': str(c.device),
        }

def test_multi_gpu_distribution():
    """æµ‹è¯•å¤šGPUåˆ†å¸ƒå¼"""
    logger.info("ğŸ”§ Testing multi-GPU distribution...")
    
    try:
        # åˆå§‹åŒ–Ray
        ray.init(
            num_cpus=8,
            num_gpus=8,
            ignore_reinit_error=True,
            log_to_driver=True,
        )
        
        logger.info("âœ… Ray initialized")
        logger.info(f"Available resources: {ray.available_resources()}")
        
        # åˆ›å»º8ä¸ªGPU Workers
        workers = []
        for i in range(8):
            worker = GPUWorker.remote(worker_id=i)
            workers.append(worker)
        
        logger.info(f"âœ… Created {len(workers)} GPU workers")
        
        # å¹¶è¡Œè·å–æ‰€æœ‰Workerçš„GPUä¿¡æ¯
        logger.info("ğŸ” Getting GPU info from all workers...")
        gpu_info_futures = [worker.get_gpu_info.remote() for worker in workers]
        gpu_infos = ray.get(gpu_info_futures)
        
        # æ‰“å°ç»“æœ
        logger.info("\nğŸ“Š GPU Distribution Results:")
        logger.info("=" * 80)
        
        for info in gpu_infos:
            logger.info(f"Worker {info['worker_id']}:")
            logger.info(f"  Process ID: {info['process_id']}")
            logger.info(f"  CUDA_VISIBLE_DEVICES: {info['cuda_visible_devices']}")
            logger.info(f"  CUDA devices visible: {info['cuda_device_count']}")
            logger.info(f"  Current device: cuda:{info.get('current_device', 'N/A')}")
            logger.info(f"  Device name: {info.get('device_name', 'N/A')}")
            logger.info(f"  Tensor device: {info.get('tensor_device', 'N/A')}")
            logger.info("  " + "-" * 50)
        
        # éªŒè¯æ¯ä¸ªWorkerç¡®å®ä½¿ç”¨ä¸åŒçš„ç‰©ç†GPU
        visible_devices = [info['cuda_visible_devices'] for info in gpu_infos]
        unique_devices = set(visible_devices)
        
        logger.info(f"\nâœ… Unique physical GPUs used: {len(unique_devices)}")
        logger.info(f"Expected: 8, Actual: {len(unique_devices)}")
        
        if len(unique_devices) == 8:
            logger.info("ğŸ‰ SUCCESS: Each worker is using a different physical GPU!")
        else:
            logger.error("âŒ FAILED: Workers are not properly distributed across GPUs")
            
        # è¿è¡Œå¹¶è¡Œè®¡ç®—æµ‹è¯•
        logger.info("\nğŸš€ Running parallel computation test...")
        compute_futures = [worker.run_simple_computation.remote() for worker in workers]
        compute_results = ray.get(compute_futures)
        
        logger.info("ğŸ’» Parallel Computation Results:")
        total_time = 0
        for result in compute_results:
            if 'error' not in result:
                logger.info(f"  Worker {result['worker_id']}: {result['computation_time']:.3f}s on {result['tensor_device']}")
                total_time += result['computation_time']
            else:
                logger.error(f"  Worker error: {result['error']}")
        
        avg_time = total_time / len(compute_results)
        logger.info(f"Average computation time per worker: {avg_time:.3f}s")
        logger.info("âœ… All workers completed parallel computation successfully!")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ Multi-GPU test failed: {e}")
        logger.exception("Full traceback:")
        return False
    finally:
        try:
            ray.shutdown()
            logger.info("âœ… Ray shutdown completed")
        except:
            pass

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("ğŸ§ª å¤šGPUåˆ†å¸ƒå¼éªŒè¯æµ‹è¯•")
    logger.info("=" * 60)
    
    success = test_multi_gpu_distribution()
    
    logger.info("\nğŸ“Š Test Results:")
    logger.info("=" * 60)
    
    if success:
        logger.info("âœ… Multi-GPU Distribution Test: PASS")
        logger.info("ğŸ‰ æ¯ä¸ªWorkerç¡®å®è¿è¡Œåœ¨ä¸åŒçš„ç‰©ç†GPUä¸Šï¼")
        logger.info("ğŸ’¡ è™½ç„¶æ¯ä¸ªWorkerå†…éƒ¨ä½¿ç”¨cuda:0ï¼Œä½†å®ƒä»¬å¯¹åº”ä¸åŒçš„ç‰©ç†GPU")
    else:
        logger.error("âŒ Multi-GPU Distribution Test: FAIL")

if __name__ == "__main__":
    main() 