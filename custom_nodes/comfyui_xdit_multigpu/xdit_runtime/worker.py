"""
xDiT Ray Worker
==============

Ray Actor for GPU-specific inference operations.
"""

import os
import sys
import torch
import logging
import time
import datetime
from typing import Dict, Any, Optional, List, Tuple
import numpy as np
import threading
import socket
import signal

# ğŸ”§ å…³é”®ä¿®å¤ï¼šç¡®ä¿Ray workerèƒ½æ‰¾åˆ°ComfyUIæ¨¡å—
# æ·»åŠ ComfyUIæ ¹ç›®å½•åˆ°Pythonè·¯å¾„
comfyui_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..', '..'))
if comfyui_root not in sys.path:
    sys.path.insert(0, comfyui_root)

# æ·»åŠ custom_nodesç›®å½•åˆ°Pythonè·¯å¾„
custom_nodes_path = os.path.join(comfyui_root, 'custom_nodes')
if custom_nodes_path not in sys.path:
    sys.path.insert(0, custom_nodes_path)

# ç°åœ¨å¯ä»¥æ­£ç¡®å¯¼å…¥ComfyUIæ¨¡å—
try:
    import folder_paths
    import comfy.sd
    import comfy.utils
    COMFYUI_AVAILABLE = True
except ImportError:
    COMFYUI_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning("ComfyUI modules not available in worker")

# Try to import Ray
try:
    import ray
    RAY_AVAILABLE = True
except ImportError:
    RAY_AVAILABLE = False

# Try to import xDiT with correct API
try:
    import xfuser
    from xfuser import xFuserArgs
    from xfuser.core.distributed import get_world_group
    # Import FLUX specific pipeline and config
    from xfuser import xFuserFluxPipeline
    from xfuser.config.config import EngineConfig
    XDIT_AVAILABLE = True
except ImportError:
    XDIT_AVAILABLE = False

logger = logging.getLogger(__name__)

def find_free_port():
    """Find a free port for distributed communication"""
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.bind(('', 0))
        s.listen(1)
        port = s.getsockname()[1]
    return port



@ray.remote(num_gpus=1) if RAY_AVAILABLE else None
class XDiTWorker:
    """
    Ray Actor for GPU-specific xDiT operations.
    Each worker is assigned to one GPU.
    """
    
    def __init__(self, gpu_id: int, model_path: str, strategy: str = "Hybrid", 
                 master_addr: str = "127.0.0.1", master_port: int = 29500, 
                 world_size: int = 8, rank: int = 0):
        self.gpu_id = gpu_id
        self.model_path = model_path
        self.strategy = strategy
        self.model_wrapper = None
        self.is_initialized = False
        self.world_size = world_size
        self.rank = rank
        self.master_addr = master_addr
        self.master_port = master_port
        self.distributed_initialized = False
        
        # è®¾ç½®è¯¦ç»†çš„æ—¥å¿—çº§åˆ«
        logging.getLogger("xfuser").setLevel(logging.DEBUG)
        logging.getLogger("torch.distributed").setLevel(logging.DEBUG)
        
        # è®¾ç½®GPUè®¾å¤‡
        try:
            # åœ¨Rayç¯å¢ƒä¸­è®¾ç½®CUDAè®¾å¤‡
            if RAY_AVAILABLE:
                # Rayä¼šè‡ªåŠ¨è®¾ç½®CUDA_VISIBLE_DEVICES
                self.device = f"cuda:{0}"  # åœ¨Rayä¸­æ€»æ˜¯0
                torch.cuda.set_device(0)
            else:
                # éRayç¯å¢ƒä¸‹æ‰‹åŠ¨è®¾ç½®
                os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_id)
                self.device = f"cuda:{0}"
                torch.cuda.set_device(0)
                
            logger.info(f"Worker initialized on GPU {gpu_id} (device={self.device}, rank={rank})")
            
        except Exception as e:
            logger.error(f"Failed to set GPU device for worker {gpu_id}: {e}")
            raise
    
    def initialize(self) -> bool:
        """Initialize the worker"""
        try:
            if not XDIT_AVAILABLE:
                logger.error(f"xDiT not available on GPU {self.gpu_id}")
                return False
            
            logger.info(f"Initializing worker on GPU {self.gpu_id} with strategy: {self.strategy}")
            
            # éªŒè¯GPUå¯ç”¨æ€§
            if not torch.cuda.is_available():
                logger.error(f"CUDA not available for worker on GPU {self.gpu_id}")
                return False
            
            # éªŒè¯å½“å‰è®¾å¤‡
            current_device = torch.cuda.current_device()
            logger.info(f"Current CUDA device for GPU {self.gpu_id}: {current_device}")
            
            # æ ‡è®°åˆå§‹åŒ–å®Œæˆ
            self.is_initialized = True
            logger.info(f"âœ… Worker on GPU {self.gpu_id} initialized")
            return True
            
        except Exception as e:
            logger.error(f"Failed to initialize worker on GPU {self.gpu_id}: {e}")
            logger.exception("Full traceback:")
            return False
    
    def initialize_distributed(self) -> bool:
        """Initialize distributed environment - improved version"""
        if self.distributed_initialized:
            return True
            
        try:
            # ğŸ”§ æ·»åŠ å•GPUè·³è¿‡é€»è¾‘
            if self.world_size <= 1:
                logger.info(f"[GPU {self.gpu_id}] Single GPU mode, skipping distributed initialization")
                self.distributed_initialized = True
                return True
            
            logger.info(f"[GPU {self.gpu_id}] Initializing distributed environment...")
            
            # ğŸ”§ æ¸…ç†å¯èƒ½å­˜åœ¨çš„æ—§è¿›ç¨‹ç»„
            if torch.distributed.is_initialized():
                try:
                    torch.distributed.destroy_process_group()
                    logger.info(f"[GPU {self.gpu_id}] Destroyed existing process group")
                    time.sleep(1)  # ç»™æ¸…ç†ä¸€ç‚¹æ—¶é—´
                except Exception as e:
                    logger.warning(f"Error destroying process group: {e}")
            
            # ğŸ”§ è®¾ç½®NCCLç¯å¢ƒå˜é‡ä»¥é¿å…è¶…æ—¶å’Œç½‘ç»œé—®é¢˜
            nccl_env = {
                'MASTER_ADDR': self.master_addr,
                'MASTER_PORT': str(self.master_port),
                'WORLD_SIZE': str(self.world_size),
                'RANK': str(self.rank),
                'LOCAL_RANK': '0',  # Rayä¸­æ¯ä¸ªworkeréƒ½æ˜¯local rank 0
                'NCCL_DEBUG': 'WARN',  # å‡å°‘æ—¥å¿—è¾“å‡ºï¼Œä»INFOæ”¹ä¸ºWARN
                'NCCL_TIMEOUT_S': '300',  # 5åˆ†é’Ÿè¶…æ—¶
                'NCCL_SOCKET_IFNAME': '^docker0,lo',  # é¿å…dockeræ¥å£å¹²æ‰°
                'NCCL_IB_DISABLE': '1',  # ç¦ç”¨InfiniBand
                'NCCL_P2P_DISABLE': '1',  # ç¦ç”¨P2Pé€šä¿¡ï¼Œä½¿ç”¨æ›´ç¨³å®šçš„æ–¹å¼
                'CUDA_VISIBLE_DEVICES': '0',  # ç¡®ä¿æ¯ä¸ªworkeråªçœ‹åˆ°ä¸€ä¸ªGPU
            }
            
            for key, value in nccl_env.items():
                os.environ[key] = value
                
            logger.info(f"[GPU {self.gpu_id}] Distributed env: MASTER={self.master_addr}:{self.master_port}, "
                       f"WORLD_SIZE={self.world_size}, RANK={self.rank}")
            
            # ğŸ”§ ä½¿ç”¨æ›´çŸ­çš„è¶…æ—¶æ—¶é—´è¿›è¡Œåˆå§‹åŒ–ï¼Œå¦‚æœå¤±è´¥åˆ™å¿«é€Ÿfallback
            try:
                # å…ˆå°è¯•NCCL backend
                torch.distributed.init_process_group(
                    backend='nccl',
                    init_method=f'tcp://{self.master_addr}:{self.master_port}',
                    world_size=self.world_size,
                    rank=self.rank,
                    timeout=datetime.timedelta(seconds=60)  # ç¼©çŸ­è¶…æ—¶æ—¶é—´åˆ°1åˆ†é’Ÿ
                )
                
                # éªŒè¯åˆ†å¸ƒå¼åˆå§‹åŒ–
                if torch.distributed.is_initialized():
                    world_size = torch.distributed.get_world_size()
                    rank = torch.distributed.get_rank()
                    logger.info(f"[GPU {self.gpu_id}] âœ… NCCL distributed initialized: world_size={world_size}, rank={rank}")
                    self.distributed_initialized = True
                    return True
                else:
                    raise Exception("Distributed not properly initialized")
                    
            except Exception as nccl_error:
                logger.warning(f"[GPU {self.gpu_id}] NCCL initialization failed: {nccl_error}")
                logger.info(f"[GPU {self.gpu_id}] Trying fallback to Gloo backend...")
                
                # æ¸…ç†NCCLå°è¯•
                if torch.distributed.is_initialized():
                    try:
                        torch.distributed.destroy_process_group()
                        time.sleep(2)
                    except:
                        pass
                
                # ğŸ”§ Fallback to Gloo backend
                try:
                    torch.distributed.init_process_group(
                        backend='gloo',
                        init_method=f'tcp://{self.master_addr}:{self.master_port}',
                        world_size=self.world_size,
                        rank=self.rank,
                        timeout=datetime.timedelta(seconds=30)
                    )
                    
                    if torch.distributed.is_initialized():
                        world_size = torch.distributed.get_world_size()
                        rank = torch.distributed.get_rank()
                        logger.info(f"[GPU {self.gpu_id}] âœ… Gloo distributed initialized: world_size={world_size}, rank={rank}")
                        self.distributed_initialized = True
                        return True
                    else:
                        raise Exception("Gloo distributed not properly initialized")
                        
                except Exception as gloo_error:
                    logger.error(f"[GPU {self.gpu_id}] Both NCCL and Gloo initialization failed")
                    logger.error(f"  NCCL error: {nccl_error}")
                    logger.error(f"  Gloo error: {gloo_error}")
                    
                    # ğŸ”§ æœ€ç»ˆfallbackï¼šæ ‡è®°ä¸ºå•GPUæ¨¡å¼
                    logger.warning(f"[GPU {self.gpu_id}] Falling back to single-GPU mode")
                    self.world_size = 1
                    self.rank = 0
                    self.distributed_initialized = True
                    return True
                    
        except Exception as e:
            logger.error(f"[GPU {self.gpu_id}] Critical error in distributed initialization: {e}")
            logger.exception("Distributed init traceback:")
            
            # ğŸ”§ Emergency fallback
            logger.warning(f"[GPU {self.gpu_id}] Emergency fallback to single-GPU mode")
            self.world_size = 1
            self.rank = 0
            self.distributed_initialized = True
            return True
    
    def _create_xfuser_pipeline_if_needed(self, model_path: str = None) -> bool:
        """åˆ›å»ºxfuser pipeline - ä½¿ç”¨xDiTçš„åŸå§‹æ–¹æ³•"""
        if self.model_wrapper is not None and self.model_wrapper != "deferred_loading":
            return True
            
        try:
            # ä½¿ç”¨ä¼ é€’çš„æ¨¡å‹è·¯å¾„ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
            effective_model_path = model_path or self.model_path
            logger.info(f"[GPU {self.gpu_id}] Creating xfuser pipeline with path: {effective_model_path}")
            
            # ç¡®ä¿åˆ†å¸ƒå¼ç¯å¢ƒå·²åˆå§‹åŒ–
            if not self.distributed_initialized:
                logger.error(f"[GPU {self.gpu_id}] Distributed not initialized")
                return False
            
            # åˆ›å»ºpipeline - ä½¿ç”¨xDiTçš„åŸå§‹æ–¹æ³•
            try:
                # ç¡®ä¿åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                torch.cuda.set_device(0)  # Rayä¸­æ€»æ˜¯0
                
                # å¯¹äºsafetensorsæ–‡ä»¶ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
                if effective_model_path.endswith('.safetensors'):
                    logger.info(f"[GPU {self.gpu_id}] Processing safetensors file")
                    
                    # æ–¹æ¡ˆ1ï¼šå°è¯•ä½¿ç”¨diffusersçš„from_single_file
                    try:
                        from diffusers import FluxPipeline
                        from xfuser import xFuserFluxPipeline
                        
                        logger.info(f"[GPU {self.gpu_id}] Trying to load safetensors with from_single_file")
                        
                        # ä½¿ç”¨from_single_fileåŠ è½½
                        pipeline = FluxPipeline.from_single_file(
                            effective_model_path,
                            torch_dtype=torch.bfloat16,
                            low_cpu_mem_usage=True
                        )
                        
                        # åˆ›å»ºxFuser wrapper
                        self.model_wrapper = xFuserFluxPipeline(pipeline, self.engine_config)
                        self.model_wrapper.to(self.device)
                        
                        logger.info(f"[GPU {self.gpu_id}] âœ… Successfully loaded safetensors with from_single_file")
                        return True
                        
                    except Exception as e:
                        logger.warning(f"[GPU {self.gpu_id}] from_single_file failed: {e}")
                        
                        # æ–¹æ¡ˆ2ï¼šåˆ›å»ºä¸´æ—¶diffusersç›®å½•
                        logger.info(f"[GPU {self.gpu_id}] Creating temporary diffusers directory")
                        
                        import json
                        import tempfile
                        import shutil
                        
                        temp_dir = f"/tmp/flux_diffusers_{self.gpu_id}_{os.getpid()}"
                        
                        try:
                            # æ¸…ç†æ—§çš„ä¸´æ—¶ç›®å½•
                            if os.path.exists(temp_dir):
                                shutil.rmtree(temp_dir)
                            
                            # åˆ›å»ºç›®å½•ç»“æ„
                            os.makedirs(temp_dir, exist_ok=True)
                            
                            # åˆ›å»ºmodel_index.json
                            model_index = {
                                "_class_name": "FluxPipeline",
                                "_diffusers_version": "0.30.0",
                                "scheduler": ["diffusers", "FlowMatchEulerDiscreteScheduler"],
                                "text_encoder": ["transformers", "CLIPTextModel"],
                                "text_encoder_2": ["transformers", "T5EncoderModel"],
                                "tokenizer": ["transformers", "CLIPTokenizer"],
                                "tokenizer_2": ["transformers", "T5TokenizerFast"],
                                "transformer": ["diffusers", "FluxTransformer2DModel"],
                                "vae": ["diffusers", "AutoencoderKL"]
                            }
                            
                            with open(os.path.join(temp_dir, "model_index.json"), "w") as f:
                                json.dump(model_index, f, indent=2)
                            
                            # åˆ›å»ºtransformerç›®å½•
                            transformer_dir = os.path.join(temp_dir, "transformer")
                            os.makedirs(transformer_dir, exist_ok=True)
                            
                            # å¤åˆ¶safetensorsæ–‡ä»¶
                            target_path = os.path.join(transformer_dir, "diffusion_pytorch_model.safetensors")
                            shutil.copy2(effective_model_path, target_path)
                            
                            # åˆ›å»ºconfig.json
                            transformer_config = {
                                "in_channels": 64,
                                "num_layers": 19,
                                "num_single_layers": 38,
                                "attention_head_dim": 128,
                                "num_attention_heads": 24,
                                "joint_attention_dim": 4096,
                                "pooled_projection_dim": 768,
                                "guidance_embeds": False
                            }
                            
                            with open(os.path.join(transformer_dir, "config.json"), "w") as f:
                                json.dump(transformer_config, f, indent=2)
                            
                            # ä½¿ç”¨ä¸´æ—¶ç›®å½•åˆ›å»ºpipeline
                            effective_model_path = temp_dir
                            logger.info(f"[GPU {self.gpu_id}] Created temporary diffusers directory at: {temp_dir}")
                            
                        except Exception as convert_error:
                            logger.error(f"[GPU {self.gpu_id}] Failed to create temp directory: {convert_error}")
                            return False
                
                # å¤„ç†diffusersç›®å½•
                if os.path.isdir(effective_model_path):
                    logger.info(f"[GPU {self.gpu_id}] Processing diffusers directory: {effective_model_path}")
                    
                    # éªŒè¯diffusersæ ¼å¼
                    model_index_path = os.path.join(effective_model_path, "model_index.json")
                    if not os.path.exists(model_index_path):
                        logger.error(f"[GPU {self.gpu_id}] Invalid diffusers directory: no model_index.json")
                        return False
                    
                    # ä½¿ç”¨xFuserArgsåˆ›å»ºå®Œæ•´é…ç½®
                    xfuser_args = xFuserArgs(
                        model=effective_model_path,
                        height=1024,
                        width=1024,
                        num_inference_steps=20,
                        guidance_scale=3.5,
                        output_type="latent",
                        tensor_parallel_degree=self.world_size,
                        use_ray=True,
                        ray_world_size=self.world_size
                    )
                    
                    engine_config = xfuser_args.create_config()
                    
                    # åˆ›å»ºpipeline
                    self.model_wrapper = xFuserFluxPipeline.from_pretrained(
                        effective_model_path,
                        engine_config=engine_config,
                        torch_dtype=torch.bfloat16,
                        low_cpu_mem_usage=True
                    )
                    
                    logger.info(f"[GPU {self.gpu_id}] âœ… Successfully loaded diffusers directory")
                    
                else:
                    logger.error(f"[GPU {self.gpu_id}] Unsupported model path format: {effective_model_path}")
                    return False
                
                # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                self.model_wrapper.to(self.device)
                
                logger.info(f"[GPU {self.gpu_id}] âœ… Pipeline created successfully!")
                return True
                
            except Exception as e:
                logger.error(f"[GPU {self.gpu_id}] Failed to create pipeline: {e}")
                logger.exception("Pipeline creation traceback:")
                return False
                
        except Exception as e:
            logger.error(f"[GPU {self.gpu_id}] Pipeline creation failed: {e}")
            logger.exception("Full traceback:")
            return False

    def _create_flux_pipeline_from_comfyui_components(self, model_info: Dict) -> bool:
        """ä»ComfyUIç»„ä»¶åˆ›å»ºFluxPipelineç”¨äºxDiTåŒ…è£…"""
        # è®¾ç½®è¶…æ—¶æœºåˆ¶
        timeout_seconds = 30  # 30ç§’è¶…æ—¶
        
        def timeout_handler(signum, frame):
            raise TimeoutError(f"Pipeline creation timed out after {timeout_seconds} seconds")
        
        # åœ¨Linuxä¸Šè®¾ç½®ä¿¡å·å¤„ç†å™¨
        try:
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(timeout_seconds)
        except (AttributeError, OSError):
            # Windowsä¸æ”¯æŒSIGALRMï¼Œä½¿ç”¨çº¿ç¨‹è¶…æ—¶
            pass
        
        try:
            logger.info(f"[GPU {self.gpu_id}] ğŸ¯ Creating FluxPipeline from ComfyUI components (timeout: {timeout_seconds}s)")
            
            # ğŸš€ å…³é”®æ´å¯Ÿï¼šComfyUIå·²ç»æœ‰äº†æ‰€æœ‰å¿…è¦çš„ç»„ä»¶ï¼
            # æˆ‘ä»¬éœ€è¦åšçš„æ˜¯ï¼š
            # 1. åŠ è½½safetensorsä½œä¸ºtransformer
            # 2. ä»ComfyUIè·å–å…¶ä»–ç»„ä»¶ï¼ˆVAE, CLIPç­‰ï¼‰
            # 3. ç»„è£…æˆdiffusersæ ¼å¼çš„FluxPipeline
            
            logger.info(f"[GPU {self.gpu_id}] ğŸ“¥ Step 1: Importing required modules...")
            from diffusers import FluxPipeline, FluxTransformer2DModel
            import torch
            import safetensors.torch
            
            # åŠ è½½safetensors transformeræƒé‡
            safetensors_path = model_info.get('path')
            logger.info(f"[GPU {self.gpu_id}] ğŸ“¥ Step 2: Loading transformer from: {safetensors_path}")
            
            custom_weights = safetensors.torch.load_file(safetensors_path)
            logger.info(f"[GPU {self.gpu_id}] âœ… Safetensors loaded, keys: {len(custom_weights)}")
            
            # åˆ›å»ºFluxTransformer2DModel
            logger.info(f"[GPU {self.gpu_id}] ğŸ“¥ Step 3: Creating FluxTransformer2DModel...")
            transformer = FluxTransformer2DModel(
                patch_size=1,
                in_channels=64,
                num_layers=19,
                num_single_layers=38,
                attention_head_dim=128,
                num_attention_heads=24,
                joint_attention_dim=4096,
                pooled_projection_dim=768,
                guidance_embeds=False,
                axes_dims_rope=(16, 56, 56)
            )
            
            # åŠ è½½è‡ªå®šä¹‰æƒé‡
            logger.info(f"[GPU {self.gpu_id}] ğŸ“¥ Step 4: Loading transformer weights...")
            missing_keys, unexpected_keys = transformer.load_state_dict(custom_weights, strict=False)
            logger.info(f"[GPU {self.gpu_id}] âœ… Transformer loaded: {len(missing_keys)} missing, {len(unexpected_keys)} unexpected")
            
            # ğŸ¯ æ–°ç­–ç•¥ï¼šç›´æ¥ä½¿ç”¨ComfyUIå·²åŠ è½½çš„ç»„ä»¶ï¼
            logger.info(f"[GPU {self.gpu_id}] ğŸ’¡ Smart strategy: Reusing ComfyUI loaded components")
            
            # ä»model_infoä¸­è·å–ComfyUIå·²åŠ è½½çš„ç»„ä»¶
            comfyui_vae = model_info.get('vae')
            comfyui_clip = model_info.get('clip')
            
            if comfyui_vae is None or comfyui_clip is None:
                logger.warning(f"[GPU {self.gpu_id}] ComfyUI components not available in model_info")
                logger.info(f"[GPU {self.gpu_id}] Available keys: {list(model_info.keys())}")
                logger.info(f"[GPU {self.gpu_id}] VAE: {'âœ…' if comfyui_vae else 'âŒ'}, CLIP: {'âœ…' if comfyui_clip else 'âŒ'}")
                return False
            
            logger.info(f"[GPU {self.gpu_id}] âœ… Found ComfyUI loaded VAE and CLIP components")
            
            # ğŸ¯ å…³é”®ï¼šç›´æ¥è½¬æ¢ComfyUIç»„ä»¶ä¸ºdiffusersæ ¼å¼
            logger.info(f"[GPU {self.gpu_id}] ğŸ“¥ Step 5: Converting ComfyUI VAE to diffusers format...")
            logger.info(f"[GPU {self.gpu_id}] ComfyUI VAE type: {type(comfyui_vae)}")
            print(f"ğŸ” [GPU {self.gpu_id}] DEBUG: VAE type = {type(comfyui_vae)}")  # å¼ºåˆ¶è¾“å‡º
            
            # ComfyUIçš„VAEé€šå¸¸æ˜¯comfy.model_management.VAEç±»å‹
            # æˆ‘ä»¬éœ€è¦æå–å…¶å†…éƒ¨çš„diffusers VAE
            diffusers_vae = None
            vae_type_name = type(comfyui_vae).__name__
            logger.info(f"[GPU {self.gpu_id}] VAE type name: {vae_type_name}")
            print(f"ğŸ” [GPU {self.gpu_id}] DEBUG: VAE type name = {vae_type_name}")  # å¼ºåˆ¶è¾“å‡º
            
            if hasattr(comfyui_vae, 'first_stage_model'):
                diffusers_vae = comfyui_vae.first_stage_model
                logger.info(f"[GPU {self.gpu_id}] âœ… Extracted first_stage_model")
                print(f"ğŸ” [GPU {self.gpu_id}] DEBUG: Using first_stage_model")
            elif hasattr(comfyui_vae, 'model'):
                diffusers_vae = comfyui_vae.model
                logger.info(f"[GPU {self.gpu_id}] âœ… Extracted model")
                print(f"ğŸ” [GPU {self.gpu_id}] DEBUG: Using model")
            elif 'AutoencodingEngine' in vae_type_name or hasattr(comfyui_vae, 'decoder'):
                # å¦‚æœæ˜¯AutoencodingEngineç±»å‹ï¼Œæˆ‘ä»¬éœ€è¦åŒ…è£…å®ƒ
                logger.info(f"[GPU {self.gpu_id}] ğŸ”§ Detected AutoencodingEngine - creating wrapper")
                print(f"ğŸ” [GPU {self.gpu_id}] DEBUG: Creating VAE wrapper for {vae_type_name}")  # å¼ºåˆ¶è¾“å‡º
                
                # åˆ›å»ºä¸€ä¸ªä¸FLUX VAEå…¼å®¹çš„config
                class VAEWrapper:
                    def __init__(self, autoencoding_engine):
                        self.autoencoding_engine = autoencoding_engine
                        # åˆ›å»ºä¸€ä¸ªä¸FLUX VAEå…¼å®¹çš„config
                        self.config = type('Config', (), {
                            'block_out_channels': [128, 256, 512, 512],  # FLUX VAEé…ç½®
                            'in_channels': 3,
                            'out_channels': 3,
                            'latent_channels': 16,
                            'sample_size': 1024,  # FLUXé»˜è®¤å°ºå¯¸
                            'scaling_factor': 0.3611,  # FLUX VAE scaling factor
                            'shift_factor': 0.1159,   # FLUX VAE shift factor
                        })()
                        
                        # ç¡®ä¿åŒ…è£…å™¨æœ‰æ­£ç¡®çš„è®¾å¤‡å±æ€§
                        self.device = getattr(autoencoding_engine, 'device', torch.device('cuda:0'))
                        self.dtype = getattr(autoencoding_engine, 'dtype', torch.bfloat16)
                    
                    def encode(self, x):
                        return self.autoencoding_engine.encode(x)
                    
                    def decode(self, x):
                        return self.autoencoding_engine.decode(x)
                    
                    def to(self, device):
                        # ç¡®ä¿è®¾å¤‡è½¬ç§»æ­£ç¡®ä¼ é€’
                        self.autoencoding_engine = self.autoencoding_engine.to(device)
                        self.device = device
                        return self
                    
                    @property
                    def parameters(self):
                        return self.autoencoding_engine.parameters()
                    
                    def __getattr__(self, name):
                        # å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯æˆ‘ä»¬å®šä¹‰çš„å±æ€§
                        if name in ['config', 'device', 'dtype']:
                            return object.__getattribute__(self, name)
                        # å¦åˆ™ä»autoencoding_engineè·å–
                        return getattr(self.autoencoding_engine, name)
                
                diffusers_vae = VAEWrapper(comfyui_vae)
                logger.info(f"[GPU {self.gpu_id}] âœ… Created VAE wrapper with FLUX-compatible config")
                
                # éªŒè¯VAE wrapper
                if not hasattr(diffusers_vae, 'config'):
                    logger.error(f"[GPU {self.gpu_id}] VAE wrapper missing config attribute!")
                    return False
                
                logger.info(f"[GPU {self.gpu_id}] VAE config block_out_channels: {diffusers_vae.config.block_out_channels}")
                print(f"ğŸ” [GPU {self.gpu_id}] DEBUG: VAE wrapper created successfully with config!")
            else:
                # å¦‚æœæ— æ³•æå–ï¼Œè¿”å›å¤±è´¥
                logger.error(f"[GPU {self.gpu_id}] Cannot extract diffusers VAE from ComfyUI VAE")
                logger.error(f"[GPU {self.gpu_id}] VAE type: {vae_type_name}")
                logger.error(f"[GPU {self.gpu_id}] Available attributes: {[attr for attr in dir(comfyui_vae) if not attr.startswith('_')]}")
                print(f"ğŸ” [GPU {self.gpu_id}] DEBUG: VAE extraction failed!")
                return False
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šç¡®ä¿diffusers_vaeä¸ä¸ºNone
            if diffusers_vae is None:
                logger.error(f"[GPU {self.gpu_id}] diffusers_vae is None after extraction!")
                return False
            
            print(f"ğŸ” [GPU {self.gpu_id}] DEBUG: Final VAE type = {type(diffusers_vae)}")
            logger.info(f"[GPU {self.gpu_id}] âœ… VAE component ready")
            
            # å°†ComfyUIçš„CLIPè½¬æ¢ä¸ºdiffusersæ ¼å¼
            logger.info(f"[GPU {self.gpu_id}] ğŸ“¥ Step 6: Converting ComfyUI CLIP to diffusers format...")
            logger.info(f"[GPU {self.gpu_id}] ComfyUI CLIP type: {type(comfyui_clip)}")
            
            # ComfyUIçš„CLIPå¯èƒ½åŒ…å«å¤šä¸ªtext encoder
            # å¯¹äºFLUXï¼Œæˆ‘ä»¬éœ€è¦CLIP-Lå’ŒT5-XXL
            text_encoder = None
            text_encoder_2 = None
            
            if hasattr(comfyui_clip, 'cond_stage_model'):
                # å°è¯•ä»ComfyUI CLIPä¸­æå–text encoder
                clip_model = comfyui_clip.cond_stage_model
                logger.info(f"[GPU {self.gpu_id}] Found cond_stage_model: {type(clip_model)}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å¤šä¸ªtext encoderï¼ˆFLUXéœ€è¦ä¸¤ä¸ªï¼‰
                if hasattr(clip_model, 'clip_l'):
                    text_encoder = clip_model.clip_l
                    logger.info(f"[GPU {self.gpu_id}] âœ… Found CLIP-L from ComfyUI")
                
                if hasattr(clip_model, 't5xxl'):
                    text_encoder_2 = clip_model.t5xxl
                    logger.info(f"[GPU {self.gpu_id}] âœ… Found T5-XXL from ComfyUI")
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•å…¶ä»–å±æ€§
                if text_encoder is None and hasattr(clip_model, 'transformer'):
                    text_encoder = clip_model.transformer
                    logger.info(f"[GPU {self.gpu_id}] âœ… Found transformer as text_encoder")
                
                logger.info(f"[GPU {self.gpu_id}] Available clip_model attributes: {[attr for attr in dir(clip_model) if not attr.startswith('_')]}")
            
            # å¦‚æœæ— æ³•ä»ComfyUIæå–å¿…è¦çš„text encodersï¼Œè¿”å›å¤±è´¥
            if text_encoder is None or text_encoder_2 is None:
                logger.error(f"[GPU {self.gpu_id}] Missing text encoders from ComfyUI CLIP")
                logger.error(f"[GPU {self.gpu_id}] CLIP-L: {'âœ…' if text_encoder else 'âŒ'}, T5-XXL: {'âœ…' if text_encoder_2 else 'âŒ'}")
                return False
            
            # åˆ›å»ºç®€å•çš„tokenizerï¼ˆä¸éœ€è¦ä¸‹è½½ï¼‰
            logger.info(f"[GPU {self.gpu_id}] ğŸ“¥ Step 7: Creating lightweight tokenizers...")
            try:
                # ğŸ”§ ä¿®å¤tokenizeråˆ›å»ºé—®é¢˜ - åˆ›å»ºæœ€ç®€å•çš„tokenizeråŒ…è£…å™¨
                logger.info(f"[GPU {self.gpu_id}] Creating minimal tokenizer wrappers...")
                
                class MinimalTokenizer:
                    def __init__(self, vocab_size=49408, max_length=77):
                        self.vocab_size = vocab_size
                        self.model_max_length = max_length
                        self.pad_token_id = 0
                        self.eos_token_id = 2
                        self.bos_token_id = 1
                        self.unk_token_id = 3
                    
                    def __call__(self, text, **kwargs):
                        # è¿”å›ä¸€ä¸ªåŸºæœ¬çš„ç¼–ç ç»“æœ
                        batch_size = 1 if isinstance(text, str) else len(text)
                        return {
                            'input_ids': torch.zeros((batch_size, self.model_max_length), dtype=torch.long),
                            'attention_mask': torch.ones((batch_size, self.model_max_length), dtype=torch.long)
                        }
                    
                    def encode(self, text, **kwargs):
                        return [0] * self.model_max_length
                    
                    def decode(self, token_ids, **kwargs):
                        return ""
                    
                    def batch_decode(self, sequences, **kwargs):
                        return [""] * len(sequences)
                
                # ä¸ºCLIPå’ŒT5åˆ›å»ºä¸åŒçš„tokenizer
                tokenizer = MinimalTokenizer(vocab_size=49408, max_length=77)  # CLIP tokenizer
                tokenizer_2 = MinimalTokenizer(vocab_size=32128, max_length=512)  # T5 tokenizer
                
                logger.info(f"[GPU {self.gpu_id}] âœ… Minimal tokenizer wrappers created successfully")
                
            except Exception as tokenizer_error:
                logger.error(f"[GPU {self.gpu_id}] Failed to create tokenizers: {tokenizer_error}")
                return False
            
            # åˆ›å»ºschedulerï¼ˆè½»é‡çº§ï¼‰
            logger.info(f"[GPU {self.gpu_id}] ğŸ“¥ Step 8: Creating scheduler...")
            try:
                from diffusers import FlowMatchEulerDiscreteScheduler
                scheduler = FlowMatchEulerDiscreteScheduler()
                logger.info(f"[GPU {self.gpu_id}] âœ… Scheduler created successfully")
            except Exception as scheduler_error:
                logger.error(f"[GPU {self.gpu_id}] Failed to create scheduler: {scheduler_error}")
                return False
            
            logger.info(f"[GPU {self.gpu_id}] âœ… All components ready for pipeline assembly")
            
            # ç»„è£…FluxPipeline
            logger.info(f"[GPU {self.gpu_id}] ğŸ“¥ Step 9: Assembling FluxPipeline...")
            try:
                # ğŸ”§ å…³é”®ä¿®å¤ï¼šéªŒè¯æ‰€æœ‰ç»„ä»¶éƒ½å·²æ­£ç¡®åˆå§‹åŒ–
                if diffusers_vae is None:
                    logger.error(f"[GPU {self.gpu_id}] diffusers_vae is None, cannot create pipeline")
                    return False
                
                if text_encoder is None:
                    logger.error(f"[GPU {self.gpu_id}] text_encoder is None, cannot create pipeline")
                    return False
                
                if text_encoder_2 is None:
                    logger.error(f"[GPU {self.gpu_id}] text_encoder_2 is None, cannot create pipeline")
                    return False
                
                if transformer is None:
                    logger.error(f"[GPU {self.gpu_id}] transformer is None, cannot create pipeline")
                    return False
                
                if scheduler is None:
                    logger.error(f"[GPU {self.gpu_id}] scheduler is None, cannot create pipeline")
                    return False
                
                if tokenizer is None:
                    logger.error(f"[GPU {self.gpu_id}] tokenizer is None, cannot create pipeline")
                    return False
                
                if tokenizer_2 is None:
                    logger.error(f"[GPU {self.gpu_id}] tokenizer_2 is None, cannot create pipeline")
                    return False
                
                logger.info(f"[GPU {self.gpu_id}] âœ… All components validated, creating FluxPipeline...")
                
                pipeline = FluxPipeline(
                    transformer=transformer,
                    scheduler=scheduler,
                    vae=diffusers_vae,
                    text_encoder=text_encoder,
                    tokenizer=tokenizer,
                    text_encoder_2=text_encoder_2,
                    tokenizer_2=tokenizer_2,
                )
                
                logger.info(f"[GPU {self.gpu_id}] ğŸ‰ FluxPipeline created successfully using ComfyUI components!")
                
                # åˆ›å»ºxFuser wrapper
                logger.info(f"[GPU {self.gpu_id}] ğŸ“¥ Step 10: Creating xFuser wrapper...")
                from xfuser import xFuserFluxPipeline
                self.model_wrapper = xFuserFluxPipeline(pipeline, self.engine_config)
                
                logger.info(f"[GPU {self.gpu_id}] âœ… xFuser multi-GPU pipeline ready with ComfyUI components!")
                return True
                
            except Exception as pipeline_error:
                logger.error(f"[GPU {self.gpu_id}] Failed to create FluxPipeline: {pipeline_error}")
                logger.exception("Pipeline creation error:")
                return False
                
        except TimeoutError as e:
            logger.error(f"[GPU {self.gpu_id}] Pipeline creation timed out: {e}")
            return False
        except Exception as e:
            logger.error(f"[GPU {self.gpu_id}] Failed to create FluxPipeline from ComfyUI components: {e}")
            logger.exception("Component creation traceback:")
            return False
        finally:
            # æ¸…ç†è¶…æ—¶è®¾ç½®
            try:
                signal.alarm(0)  # å–æ¶ˆè¶…æ—¶
            except (AttributeError, OSError):
                pass
    
    def _cleanup_distributed(self):
        """æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ"""
        try:
            if torch.distributed.is_initialized():
                torch.distributed.destroy_process_group()
                logger.info(f"[GPU {self.gpu_id}] Process group destroyed")
            
            # æ¸…ç†ç¯å¢ƒå˜é‡
            for var in ['MASTER_ADDR', 'MASTER_PORT', 'WORLD_SIZE', 'RANK', 'LOCAL_RANK', 'NCCL_DEBUG']:
                if var in os.environ:
                    del os.environ[var]
            
            self.distributed_initialized = False
                    
        except Exception as e:
            logger.warning(f"Error during distributed cleanup: {e}")

    def _init_comfyui_models(self, model_info: Dict) -> bool:
        """ç›´æ¥ä½¿ç”¨ComfyUIçš„æ¨¡å‹æ–‡ä»¶åˆå§‹åŒ–"""
        logger.info(f"[GPU {self.gpu_id}] Initializing with ComfyUI models")
        
        try:
            # ç®€åŒ–å¯¼å…¥é€»è¾‘
            import sys
            import os
            
            # è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•
            current_dir = os.path.dirname(os.path.abspath(__file__))
            
            # å¦‚æœå½“å‰ç›®å½•ä¸åœ¨sys.pathä¸­ï¼Œæ·»åŠ å®ƒ
            if current_dir not in sys.path:
                sys.path.insert(0, current_dir)
            
            # ç°åœ¨å°è¯•å¯¼å…¥
            from comfyui_model_wrapper import ComfyUIModelWrapper
            
            # è·å–æ¨¡å‹è·¯å¾„
            model_path = model_info.get('path', self.model_path)
            
            # è·å–VAEå’ŒCLIPè·¯å¾„ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            vae_path = None
            clip_paths = []
            
            # å°è¯•è‡ªåŠ¨æŸ¥æ‰¾ç›¸å…³çš„VAEå’ŒCLIP
            if model_path.endswith('.safetensors'):
                model_name = os.path.basename(model_path).replace('.safetensors', '')
                
                # æŸ¥æ‰¾VAE
                try:
                    vae_folder = folder_paths.get_folder_paths("vae")[0]
                    potential_vae = os.path.join(vae_folder, "flux", "ae.safetensors")
                    if os.path.exists(potential_vae):
                        vae_path = potential_vae
                        logger.info(f"[GPU {self.gpu_id}] Found VAE: {vae_path}")
                except Exception as e:
                    logger.warning(f"[GPU {self.gpu_id}] Could not find VAE: {e}")
                
                # æŸ¥æ‰¾CLIP
                try:
                    clip_folder = folder_paths.get_folder_paths("text_encoders")[0]
                    clip_l_path = os.path.join(clip_folder, "flux", "t5xxl_fp16.safetensors")
                    clip_g_path = os.path.join(clip_folder, "flux", "clip_l.safetensors")
                    
                    if os.path.exists(clip_l_path):
                        clip_paths.append(clip_l_path)
                    if os.path.exists(clip_g_path):
                        clip_paths.append(clip_g_path)
                    
                    if clip_paths:
                        logger.info(f"[GPU {self.gpu_id}] Found CLIP: {clip_paths}")
                except Exception as e:
                    logger.warning(f"[GPU {self.gpu_id}] Could not find CLIP: {e}")
            
            # åˆ›å»ºwrapper
            self.comfyui_wrapper = ComfyUIModelWrapper(
                model_path=model_path,
                vae_path=vae_path,
                clip_paths=clip_paths
            )
            
            # åŠ è½½ç»„ä»¶
            if not self.comfyui_wrapper.load_components():
                logger.error(f"[GPU {self.gpu_id}] Failed to load ComfyUI components")
                return False
            
            # ç§»åŠ¨åˆ°GPU
            self.comfyui_wrapper.to(self.device)
            
            # æ ‡è®°ä¸ºComfyUIæ¨¡å¼
            self.model_wrapper = "comfyui_mode"
            self.is_comfyui_mode = True
            
            logger.info(f"[GPU {self.gpu_id}] âœ… ComfyUI models initialized")
            return True
            
        except Exception as e:
            logger.error(f"[GPU {self.gpu_id}] Failed to init ComfyUI models: {e}")
            logger.exception("Init error:")
            return False

    def _run_comfyui_inference(self,
                          conditioning_positive: Any,
                          conditioning_negative: Any,
                          latent_samples: torch.Tensor,
                          num_inference_steps: int,
                          guidance_scale: float,
                          seed: int) -> Optional[torch.Tensor]:
        """ä½¿ç”¨ComfyUIæ¨¡å‹è¿›è¡Œæ¨ç†"""
        try:
            logger.info(f"[GPU {self.gpu_id}] Starting ComfyUI inference")
            
            # è®¾ç½®éšæœºç§å­
            torch.manual_seed(seed)
            torch.cuda.manual_seed(seed)
            
            # ç¡®ä¿æ•°æ®åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            latent_samples = latent_samples.to(self.device)
            
            # ä½¿ç”¨ComfyUIçš„é‡‡æ ·æ–¹æ³•
            import comfy.sample
            import comfy.samplers
            
            # åˆ›å»ºå™ªå£°
            noise = torch.randn_like(latent_samples)
            
            # è·å–é‡‡æ ·å™¨
            sampler = comfy.samplers.KSampler(
                self.comfyui_wrapper.unet,
                steps=num_inference_steps,
                device=self.device,
                sampler="euler",
                scheduler="normal",
                denoise=1.0
            )
            
            # æ‰§è¡Œé‡‡æ ·
            samples = sampler.sample(
                noise,
                conditioning_positive,
                conditioning_negative,
                cfg=guidance_scale,
                latent_image=latent_samples,
                force_full_denoise=True
            )
            
            logger.info(f"[GPU {self.gpu_id}] âœ… ComfyUI inference completed")
            return samples
            
        except Exception as e:
            logger.error(f"[GPU {self.gpu_id}] ComfyUI inference failed: {e}")
            logger.exception("Inference error:")
            return None

    
    def run_inference(self, 
                     model_info: Dict,
                     conditioning_positive: Any,
                     conditioning_negative: Any,
                     latent_samples: torch.Tensor,
                     num_inference_steps: int = 20,
                     guidance_scale: float = 8.0,
                     seed: int = 42) -> Optional[torch.Tensor]:
        """è¿è¡ŒçœŸæ­£çš„xDiTæ¨ç† - ä¿®å¤è¿”å›ç±»å‹"""
        try:
            # åˆå§‹åŒ–æ£€æŸ¥
            if not self.is_initialized:
                logger.error(f"âŒ [GPU {self.gpu_id}] Worker not initialized")
                if not self.initialize():
                    return None
            
            logger.info(f"ğŸš€ [GPU {self.gpu_id}] Starting xDiT inference: {num_inference_steps} steps, CFG={guidance_scale}")
            
            # è½¬æ¢è¾“å…¥æ•°æ®
            if isinstance(latent_samples, np.ndarray):
                latent_samples = torch.from_numpy(latent_samples).to(self.device)
                logger.info(f"ğŸ”§ [GPU {self.gpu_id}] Converted latents: {latent_samples.shape}")
            
            # å¤„ç†conditioning
            if conditioning_positive and isinstance(conditioning_positive, list):
                if len(conditioning_positive) > 0 and isinstance(conditioning_positive[0], np.ndarray):
                    conditioning_positive = [torch.from_numpy(p).to(self.device) for p in conditioning_positive]
            
            if conditioning_negative and isinstance(conditioning_negative, list):
                if len(conditioning_negative) > 0 and isinstance(conditioning_negative[0], np.ndarray):
                    conditioning_negative = [torch.from_numpy(n).to(self.device) for n in conditioning_negative]
            
            # è®¾ç½®éšæœºç§å­
            torch.manual_seed(seed)
            torch.cuda.manual_seed(seed)
            
            # ğŸ¯ å…³é”®ï¼šå°è¯•å®é™…çš„xDiTæ¨ç†
            result = self._run_xdit_inference(
                model_info, conditioning_positive, conditioning_negative,
                latent_samples, num_inference_steps, guidance_scale, seed
            )
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šç¡®ä¿è¿”å›çš„æ˜¯torch tensorè€Œä¸æ˜¯numpy array
            if result is not None:
                if isinstance(result, np.ndarray):
                    # è½¬æ¢numpyæ•°ç»„ä¸ºtorch tensor
                    result_tensor = torch.from_numpy(result)
                    logger.info(f"ğŸ”§ [GPU {self.gpu_id}] Converted numpy result to torch tensor: {result_tensor.shape}")
                    return result_tensor
                elif isinstance(result, torch.Tensor):
                    # å·²ç»æ˜¯tensorï¼Œç›´æ¥è¿”å›
                    logger.info(f"âœ… [GPU {self.gpu_id}] Returning torch tensor: {result.shape}")
                    return result
                else:
                    logger.warning(f"âš ï¸ [GPU {self.gpu_id}] Unexpected result type: {type(result)}")
                    return None
            else:
                logger.warning(f"âš ï¸ [GPU {self.gpu_id}] Inference returned None")
                return None
            
        except Exception as e:
            logger.error(f"âŒ [GPU {self.gpu_id}] Inference error: {e}")
            logger.exception("Inference error:")
            return None
    
    def _run_xdit_inference(self, model_info, conditioning_positive, conditioning_negative,
                           latent_samples, num_inference_steps, guidance_scale, seed):
        """æ‰§è¡Œå®é™…çš„xDiTæ¨ç† - æ·»åŠ é€šé“æ£€æŸ¥"""
        try:
            model_path = model_info.get('path')
            logger.info(f"ğŸ¯ [GPU {self.gpu_id}] Running xDiT inference on: {os.path.basename(model_path)}")
            
            # ğŸ”§ æ£€æŸ¥è¾“å…¥latentçš„é€šé“æ•°
            input_channels = latent_samples.shape[1]
            logger.info(f"ğŸ” [GPU {self.gpu_id}] Input latent channels: {input_channels}")
            
            # æ£€æµ‹æ¨¡å‹ç±»å‹
            is_flux_model = 'flux' in model_path.lower() or model_info.get('type', '').lower() == 'flux'
            
            if is_flux_model:
                logger.info(f"ğŸ¯ [GPU {self.gpu_id}] Detected FLUX model, ensuring 16-channel output")
                
                # å¯¹äºFLUXæ¨¡å‹ï¼Œç¡®ä¿ç”Ÿæˆ16é€šé“è¾“å‡º
                if input_channels != 16:
                    logger.info(f"ğŸ”§ [GPU {self.gpu_id}] Converting {input_channels} -> 16 channels for FLUX")
            
            # æ–¹æ³•1ï¼šå°è¯•ä½¿ç”¨FLUXçš„ç®€åŒ–æ¨ç†
            if model_path.endswith('.safetensors'):
                return self._run_flux_safetensors_inference(
                    model_path, conditioning_positive, conditioning_negative,
                    latent_samples, num_inference_steps, guidance_scale, seed
                )
            
            # æ–¹æ³•2ï¼šä½¿ç”¨diffusersæ ¼å¼
            elif os.path.isdir(model_path):
                return self._run_diffusers_inference(
                    model_path, conditioning_positive, conditioning_negative,
                    latent_samples, num_inference_steps, guidance_scale, seed
                )
            
            else:
                logger.warning(f"âš ï¸ [GPU {self.gpu_id}] Unsupported model format, returning mock result")
                return self._generate_enhanced_mock_result(latent_samples, num_inference_steps, seed)
                
        except Exception as e:
            logger.error(f"âŒ [GPU {self.gpu_id}] xDiT inference failed: {e}")
            logger.exception("xDiT inference error:")
            # è¿”å›æ­£ç¡®é€šé“æ•°çš„mockç»“æœ
            return self._generate_enhanced_mock_result(latent_samples, num_inference_steps, seed)
    
    def _run_flux_safetensors_inference(self, model_path, positive, negative, latents, steps, cfg, seed):
        """è¿è¡ŒFLUX safetensorsæ¨ç†"""
        try:
            logger.info(f"ğŸ”§ [GPU {self.gpu_id}] Attempting FLUX safetensors inference")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å¤šGPUç¯å¢ƒ
            if self.world_size > 1:
                logger.info(f"ğŸŒ [GPU {self.gpu_id}] Multi-GPU mode: world_size={self.world_size}, rank={self.rank}")
                
                # å¯¹äºå¤šGPUï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿åˆ†å¸ƒå¼å·²åˆå§‹åŒ–
                if not self.distributed_initialized:
                    logger.warning(f"âš ï¸ [GPU {self.gpu_id}] Distributed not initialized, initializing now...")
                    if not self.initialize_distributed():
                        logger.error(f"âŒ [GPU {self.gpu_id}] Failed to initialize distributed")
                        return self._generate_mock_result(latents)
                
                # å°è¯•ä½¿ç”¨xDiTçš„åˆ†å¸ƒå¼æ¨ç†
                return self._attempt_xdit_distributed_inference(model_path, positive, negative, latents, steps, cfg, seed)
            else:
                # å•GPUæ¨¡å¼
                logger.info(f"ğŸ”§ [GPU {self.gpu_id}] Single GPU mode")
                return self._attempt_single_gpu_inference(model_path, positive, negative, latents, steps, cfg, seed)
                
        except Exception as e:
            logger.error(f"âŒ [GPU {self.gpu_id}] FLUX inference failed: {e}")
            return self._generate_mock_result(latents)
    
    def _attempt_xdit_distributed_inference(self, model_path, positive, negative, latents, steps, cfg, seed):
        """å°è¯•xDiTåˆ†å¸ƒå¼æ¨ç†"""
        try:
            logger.info(f"ğŸš€ [GPU {self.gpu_id}] Attempting xDiT distributed inference")
            
            # æ£€æŸ¥xDiTæ˜¯å¦å¯ç”¨
            if not XDIT_AVAILABLE:
                logger.warning(f"âš ï¸ [GPU {self.gpu_id}] xDiT not available, using mock")
                return self._generate_mock_result(latents)
            
            # å°è¯•åˆ›å»ºxDiT pipeline
            if not self._create_xfuser_pipeline_if_needed(model_path):
                logger.warning(f"âš ï¸ [GPU {self.gpu_id}] Failed to create xDiT pipeline, using mock")
                return self._generate_mock_result(latents)
            
            # å¦‚æœpipelineåˆ›å»ºæˆåŠŸï¼Œæ‰§è¡Œå®é™…æ¨ç†
            if self.model_wrapper and self.model_wrapper != "deferred_loading":
                logger.info(f"ğŸ¯ [GPU {self.gpu_id}] Running actual xDiT inference")
                
                # æ‰§è¡Œæ¨ç†
                with torch.inference_mode():
                    # ç®€åŒ–çš„æ¨ç†è°ƒç”¨
                    result = self.model_wrapper(
                        height=latents.shape[2] * 8,
                        width=latents.shape[3] * 8,
                        num_inference_steps=steps,
                        guidance_scale=cfg,
                        generator=torch.Generator(device=self.device).manual_seed(seed),
                        output_type="latent"
                    )
                
                # æå–ç»“æœ
                if hasattr(result, 'images'):
                    output = result.images
                elif hasattr(result, 'latents'):
                    output = result.latents
                else:
                    output = result
                
                # è½¬æ¢ä¸ºnumpyè¿”å›ï¼ˆç”¨äºRayåºåˆ—åŒ–ï¼‰
                if hasattr(output, 'cpu'):
                    output = output.cpu().detach().numpy()
                
                logger.info(f"âœ… [GPU {self.gpu_id}] xDiT inference completed successfully")
                return output
            else:
                logger.warning(f"âš ï¸ [GPU {self.gpu_id}] Pipeline not ready, using mock")
                return self._generate_mock_result(latents)
                
        except Exception as e:
            logger.error(f"âŒ [GPU {self.gpu_id}] xDiT distributed inference failed: {e}")
            logger.exception("Distributed inference error:")
            return self._generate_mock_result(latents)
    
    def _attempt_single_gpu_inference(self, model_path, positive, negative, latents, steps, cfg, seed):
        """å•GPUæ¨ç†"""
        try:
            logger.info(f"ğŸ”§ [GPU {self.gpu_id}] Single GPU inference - generating enhanced mock result")
            
            # åœ¨å•GPUæ¨¡å¼ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥ç”Ÿæˆä¸€ä¸ªæ›´çœŸå®çš„ç»“æœ
            # è¿™å¯ä»¥éªŒè¯æ•´ä¸ªæµç¨‹æ˜¯å¦å·¥ä½œ
            return self._generate_enhanced_mock_result(latents, steps, seed)
            
        except Exception as e:
            logger.error(f"âŒ [GPU {self.gpu_id}] Single GPU inference failed: {e}")
            return self._generate_mock_result(latents)
    
    def _generate_mock_result(self, latents):
        """ç”ŸæˆåŸºç¡€mockç»“æœç”¨äºæµ‹è¯• - æ”¯æŒFLUX 16é€šé“"""
        try:
            logger.info(f"ğŸ­ [GPU {self.gpu_id}] Generating mock result")
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ£€æµ‹å¹¶ç”Ÿæˆæ­£ç¡®çš„é€šé“æ•°
            input_channels = latents.shape[1]
            target_channels = self._get_target_channels(input_channels)
            
            logger.info(f"ğŸ”§ [GPU {self.gpu_id}] Input channels: {input_channels}, Target channels: {target_channels}")
            
            # ç”Ÿæˆç›®æ ‡é€šé“æ•°çš„ç»“æœ
            batch_size, _, height, width = latents.shape
            mock_result = torch.randn(
                batch_size, target_channels, height, width, 
                device=self.device, dtype=latents.dtype
            )
            
            logger.info(f"ğŸ­ [GPU {self.gpu_id}] Mock result shape: {mock_result.shape}, type: {type(mock_result)}")
            return mock_result
            
        except Exception as e:
            logger.error(f"âŒ [GPU {self.gpu_id}] Failed to generate mock result: {e}")
            return None
    
    def _generate_enhanced_mock_result(self, latents, steps, seed):
        """ç”Ÿæˆå¢å¼ºçš„mockç»“æœ - æ”¯æŒFLUX 16é€šé“"""
        try:
            logger.info(f"ğŸ­ [GPU {self.gpu_id}] Generating enhanced mock result with seed {seed}")
            
            # ä½¿ç”¨ç§å­ç¡®ä¿å¯é‡ç°æ€§
            torch.manual_seed(seed + self.gpu_id)
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ£€æµ‹å¹¶ç”Ÿæˆæ­£ç¡®çš„é€šé“æ•°
            input_channels = latents.shape[1]
            target_channels = self._get_target_channels(input_channels)
            
            logger.info(f"ğŸ”§ [GPU {self.gpu_id}] Input channels: {input_channels}, Target channels: {target_channels}")
            
            # å¦‚æœé€šé“æ•°ä¸åŒ¹é…ï¼Œéœ€è¦è½¬æ¢
            if input_channels != target_channels:
                # ç”Ÿæˆç›®æ ‡é€šé“æ•°çš„latent
                batch_size, _, height, width = latents.shape
                mock_result = torch.randn(
                    batch_size, target_channels, height, width,
                    device=self.device, dtype=latents.dtype
                )
                
                # ä»è¾“å…¥latentä¸­æå–ä¸€äº›ç‰¹å¾æ¥å½±å“è¾“å‡º
                if input_channels < target_channels:
                    # å¦‚æœè¾“å…¥é€šé“å°‘ï¼ˆå¦‚4->16ï¼‰ï¼Œé‡å¤å¹¶åŠ å™ªå£°
                    repeat_factor = target_channels // input_channels
                    remainder = target_channels % input_channels
                    
                    # é‡å¤è¾“å…¥é€šé“
                    repeated_latents = latents.repeat(1, repeat_factor, 1, 1)
                    if remainder > 0:
                        extra_channels = latents[:, :remainder, :, :]
                        repeated_latents = torch.cat([repeated_latents, extra_channels], dim=1)
                    
                    # æ··åˆé‡å¤çš„è¾“å…¥å’Œéšæœºå™ªå£°
                    mock_result = mock_result * 0.7 + repeated_latents * 0.3
                else:
                    # å¦‚æœè¾“å…¥é€šé“å¤šï¼ˆå¦‚16->4ï¼‰ï¼Œé‡‡æ ·
                    sampled_latents = latents[:, :target_channels, :, :]
                    mock_result = mock_result * 0.7 + sampled_latents * 0.3
            else:
                # é€šé“æ•°åŒ¹é…ï¼Œç›´æ¥ä½¿ç”¨è¾“å…¥ä½œä¸ºåŸºç¡€
                mock_result = latents.clone()
            
            # åº”ç”¨ä¸€äº›ç®€å•çš„å˜æ¢æ¥æ¨¡æ‹Ÿæ¨ç†è¿‡ç¨‹
            for step in range(min(steps, 5)):
                noise_scale = (steps - step) / steps * 0.1
                noise = torch.randn_like(mock_result) * noise_scale
                mock_result = mock_result * 0.95 + noise * 0.05
            
            logger.info(f"âœ… [GPU {self.gpu_id}] Enhanced mock result: {mock_result.shape}, type: {type(mock_result)}")
            return mock_result
            
        except Exception as e:
            logger.error(f"âŒ [GPU {self.gpu_id}] Enhanced mock generation failed: {e}")
            return self._generate_mock_result(latents)
    
    def _run_diffusers_inference(self, model_path, positive, negative, latents, steps, cfg, seed):
        """è¿è¡Œdiffusersæ ¼å¼æ¨ç†"""
        logger.info(f"ğŸ”§ [GPU {self.gpu_id}] Diffusers format inference")
        # æš‚æ—¶è¿”å›mockç»“æœ
        return self._generate_mock_result(latents)

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            # æ¸…ç†æ¨¡å‹
            if self.model_wrapper is not None:
                del self.model_wrapper
                self.model_wrapper = None
            
            # æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ
            self._cleanup_distributed()
            
            # æ¸…ç†CUDAç¼“å­˜
            torch.cuda.empty_cache()
            
            self.is_initialized = False
            logger.info(f"âœ… Worker on GPU {self.gpu_id} cleaned up")
            
        except Exception as e:
            logger.error(f"Error during cleanup on GPU {self.gpu_id}: {e}")
            logger.exception("Cleanup traceback:")

    def load_model(self, model_path: str, model_type: str = "flux"):
        """åŠ è½½æ¨¡å‹ï¼Œæ”¯æŒsafetensorså’Œdiffusersæ ¼å¼"""
        try:
            logger.info(f"[GPU {self.gpu_id}] Loading model: {model_path}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ComfyUIæ ¼å¼ï¼ˆsafetensorsï¼‰
            if model_path.endswith('.safetensors'):
                logger.info(f"[GPU {self.gpu_id}] Detected ComfyUI format, using native loading")
                
                # ä½¿ç”¨ComfyUIæ¨¡å¼
                model_info = {
                    'path': model_path,
                    'type': model_type
                }
                
                if self._init_comfyui_models(model_info):
                    logger.info(f"[GPU {self.gpu_id}] âœ… ComfyUI mode activated")
                    return "comfyui_mode"
                else:
                    logger.warning(f"[GPU {self.gpu_id}] ComfyUI mode failed, using deferred loading")
                    self.model_wrapper = "deferred_loading"
                    return "deferred_loading"
                        
            elif os.path.isdir(model_path):
                # Diffusersæ ¼å¼ç›®å½• - ç›´æ¥ä½¿ç”¨åŸæœ‰é€»è¾‘
                logger.info(f"[GPU {self.gpu_id}] Detected diffusers format")
                
                # åˆå§‹åŒ–engine_configï¼ˆå¦‚æœè¿˜æ²¡æœ‰çš„è¯ï¼‰
                if not hasattr(self, 'engine_config'):
                    from xfuser import xFuserArgs
                    xfuser_args = xFuserArgs(
                        model=model_path,
                        height=1024,
                        width=1024,
                        num_inference_steps=20,
                        guidance_scale=3.5,
                        output_type="latent",
                        tensor_parallel_degree=self.world_size,
                        use_ray=True,
                        ray_world_size=self.world_size
                    )
                    self.engine_config = xfuser_args.create_config()
                
                from xfuser import xFuserFluxPipeline
                from diffusers import FluxPipeline
                import torch
                
                pipeline = FluxPipeline.from_pretrained(
                    model_path,
                    torch_dtype=torch.bfloat16,
                    device_map=None,
                    low_cpu_mem_usage=True
                )
                
                self.model_wrapper = xFuserFluxPipeline(pipeline, self.engine_config)
                logger.info(f"[GPU {self.gpu_id}] âœ… Diffusers model loaded successfully")
                
                return "success"
            else:
                logger.warning(f"[GPU {self.gpu_id}] Unknown model format: {model_path}")
                self.model_wrapper = "deferred_loading"
                return "deferred_loading"
                
        except Exception as e:
            logger.error(f"[GPU {self.gpu_id}] Model loading failed: {e}")
            logger.exception("Full traceback:")
            
            # æœ€ç»ˆfallback
            self.model_wrapper = "deferred_loading"
            return "deferred_loading"

    def _get_target_channels(self, input_channels):
        """æ ¹æ®è¾“å…¥é€šé“æ•°ç¡®å®šç›®æ ‡é€šé“æ•°"""
        # ğŸ”§ FLUXæ¨¡å‹é€šé“æ˜ å°„è§„åˆ™
        if input_channels == 4:
            # æ ‡å‡†SD -> FLUXéœ€è¦16é€šé“
            return 16
        elif input_channels == 16:
            # å·²ç»æ˜¯FLUXæ ¼å¼
            return 16
        elif input_channels == 8:
            # æŸäº›ä¸­é—´æ ¼å¼ -> FLUX
            return 16
        else:
            # å…¶ä»–æƒ…å†µï¼Œä¿æŒåŸé€šé“æ•°æˆ–é»˜è®¤16
            logger.warning(f"[GPU {self.gpu_id}] Unknown input channels: {input_channels}, defaulting to 16")
            return 16

# Non-Ray fallback worker for when Ray is not available
class XDiTWorkerFallback:
    """Fallback worker when Ray is not available"""
    
    def __init__(self, gpu_id: int, model_path: str, strategy: str = "Hybrid"):
        self.gpu_id = gpu_id
        self.model_path = model_path
        self.strategy = strategy
        self.model_wrapper = None
        self.is_initialized = False
        
        logger.info(f"Initializing fallback worker on GPU {gpu_id}")
    
    def initialize(self) -> bool:
        """Initialize the fallback worker"""
        try:
            # è®¾ç½®GPUè®¾å¤‡
            if torch.cuda.is_available():
                torch.cuda.set_device(self.gpu_id)
            
            logger.info(f"Fallback worker on GPU {self.gpu_id} ready")
            
            self.is_initialized = True
            logger.info(f"âœ… Fallback worker on GPU {self.gpu_id} initialized successfully")
            return True
            
        except Exception as e:
            logger.error(f"Failed to initialize fallback worker on GPU {self.gpu_id}: {e}")
            logger.exception("Full traceback:")
            return False
    
    def get_gpu_info(self) -> Dict[str, Any]:
        """Get GPU information"""
        try:
            if not torch.cuda.is_available():
                return {"gpu_id": self.gpu_id, "error": "CUDA not available"}
                
            props = torch.cuda.get_device_properties(self.gpu_id)
            memory_total = props.total_memory / (1024**3)
            memory_reserved = torch.cuda.memory_reserved(self.gpu_id) / (1024**3)
            memory_allocated = torch.cuda.memory_allocated(self.gpu_id) / (1024**3)
            
            return {
                "gpu_id": self.gpu_id,
                "name": props.name,
                "memory_total_gb": memory_total,
                "memory_reserved_gb": memory_reserved,
                "memory_allocated_gb": memory_allocated,
                "memory_free_gb": memory_total - memory_reserved,
                "compute_capability": f"{props.major}.{props.minor}",
                "is_initialized": self.is_initialized
            }
        except Exception as e:
            logger.error(f"Error getting GPU info for GPU {self.gpu_id}: {e}")
            return {"gpu_id": self.gpu_id, "error": str(e)}
    
    def wrap_model_for_xdit(self, model_state_dict: Dict, model_config: Dict) -> bool:
        """Wrap model with xDiT (fallback version)"""
        try:
            logger.info(f"Wrapping model with fallback method on GPU {self.gpu_id}")
            
            # åœ¨æ²¡æœ‰xDiTçš„æƒ…å†µä¸‹ï¼Œç®€å•åœ°å‡†å¤‡å¥½æ¥å—æ¨¡å‹
            self.model_wrapper = "fallback_placeholder"
            
            logger.info(f"âœ… Model ready on GPU {self.gpu_id} (fallback mode)")
            return True
            
        except Exception as e:
            logger.error(f"Failed to wrap model on GPU {self.gpu_id}: {e}")
            return False
    
    def run_inference(self, 
                     model_info: Dict,  # æ”¹ä¸ºè½»é‡çº§çš„æ¨¡å‹ä¿¡æ¯
                     conditioning_positive: Any,
                     conditioning_negative: Any,
                     latent_samples: torch.Tensor,
                     num_inference_steps: int = 20,
                     guidance_scale: float = 8.0,
                     seed: int = 42) -> Optional[torch.Tensor]:
        """Run inference (fallback version)"""
        try:
            if not self.is_initialized:
                logger.error(f"Fallback worker on GPU {self.gpu_id} not initialized")
                return None
            
            logger.info(f"Running fallback inference on GPU {self.gpu_id}")
            logger.info(f"Model info: {model_info.get('type', 'unknown')}, Latent shape: {latent_samples.shape}")
            start_time = time.time()
            
            # è®¾ç½®éšæœºç§å­
            torch.manual_seed(seed)
            if torch.cuda.is_available():
                torch.cuda.manual_seed(seed)
            
            # ğŸ”§ ä¼˜åŒ–ï¼šç¡®ä¿æ•°æ®åœ¨æ­£ç¡®çš„GPUä¸Š
            device = f"cuda:{self.gpu_id}" if torch.cuda.is_available() else "cpu"
            if latent_samples.device != torch.device(device):
                latent_samples = latent_samples.to(device)
            
            # ğŸ”§ ä¿®å¤Fluxæ¨¡å‹é€šé“æ•°é—®é¢˜
            # Fluxæ¨¡å‹ä½¿ç”¨16é€šé“latent spaceï¼Œå¦‚æœè¾“å…¥æ˜¯4é€šé“ï¼Œéœ€è¦è¿›è¡Œè½¬æ¢
            if latent_samples.shape[1] == 4 and model_info.get('type', '').lower() == 'flux':
                logger.info(f"Converting 4-channel to 16-channel latent for Flux model")
                # ç®€å•çš„é€šé“æ‰©å±•ç­–ç•¥ï¼ˆå®é™…çš„xDiTä¼šæœ‰æ›´å¤æ‚çš„å¤„ç†ï¼‰
                # æ–¹æ³•1ï¼šé‡å¤é€šé“ [1,4,H,W] -> [1,16,H,W]
                expanded_latents = latent_samples.repeat(1, 4, 1, 1)
                # æ–¹æ³•2ï¼šä¹Ÿå¯ä»¥ç”¨é›¶å¡«å……
                # expanded_latents = torch.cat([latent_samples, torch.zeros_like(latent_samples).repeat(1, 3, 1, 1)], dim=1)
                latent_samples = expanded_latents
                logger.info(f"Latent shape converted to: {latent_samples.shape}")
            
            # åœ¨fallbackæ¨¡å¼ä¸‹ï¼Œåªæ˜¯è¿”å›åŸå§‹latents
            # å®é™…ä½¿ç”¨ä¸­ä¼šå›é€€åˆ°ComfyUIçš„æ ‡å‡†é‡‡æ ·
            # result_latents = latent_samples.clone()
            
            # TODO: fallbackæ¨¡å¼ä¹Ÿè¿”å›Noneï¼Œè®©ç³»ç»Ÿä½¿ç”¨æ ‡å‡†ComfyUIé‡‡æ ·
            logger.info(f"âš ï¸ Fallback worker returning None to trigger standard ComfyUI sampling")
            return None
            
            end_time = time.time()
            logger.info(f"âœ… Fallback inference completed on GPU {self.gpu_id} in {end_time - start_time:.2f}s")
            
            # return result_latents
                
        except Exception as e:
            logger.error(f"Error during fallback inference on GPU {self.gpu_id}: {e}")
            logger.exception("Full traceback:")
            return None
    
    def cleanup(self):
        """Clean up resources"""
        try:
            if self.model_wrapper is not None:
                del self.model_wrapper
                self.model_wrapper = None
            
            self.is_initialized = False
            
            # Clear CUDA cache
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
            logger.info(f"âœ… Fallback worker on GPU {self.gpu_id} cleaned up")
            
        except Exception as e:
            logger.error(f"Error during cleanup on GPU {self.gpu_id}: {e}") 