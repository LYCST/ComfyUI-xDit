"""
xDiT Ray Worker
==============

Ray Actor for GPU-specific inference operations.
"""

import os
import torch
import logging
import time
import datetime
from typing import Dict, Any, Optional, List, Tuple
import numpy as np
import threading
import socket

# Try to import Ray
try:
    import ray
    RAY_AVAILABLE = True
except ImportError:
    RAY_AVAILABLE = False

# Try to import xDiT with correct API
try:
    import xfuser
    from xfuser import xFuserArgs
    from xfuser.core.distributed import get_world_group
    # Import FLUX specific pipeline and config
    from xfuser import xFuserFluxPipeline
    from xfuser.config.config import EngineConfig
    XDIT_AVAILABLE = True
except ImportError:
    XDIT_AVAILABLE = False

logger = logging.getLogger(__name__)

def find_free_port():
    """Find a free port for distributed communication"""
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.bind(('', 0))
        s.listen(1)
        port = s.getsockname()[1]
    return port

@ray.remote(num_gpus=1) if RAY_AVAILABLE else None
class XDiTWorker:
    """
    Ray Actor for GPU-specific xDiT operations.
    Each worker is assigned to one GPU.
    """
    
    def __init__(self, gpu_id: int, model_path: str, strategy: str = "Hybrid", 
                 master_addr: str = "127.0.0.1", master_port: int = 29500, 
                 world_size: int = 8, rank: int = 0):
        self.gpu_id = gpu_id
        self.model_path = model_path
        self.strategy = strategy
        self.model_wrapper = None
        self.is_initialized = False
        self.world_size = world_size
        self.rank = rank
        self.master_addr = master_addr
        self.master_port = master_port
        self.distributed_initialized = False
        
        # è®¾ç½®è¯¦ç»†çš„æ—¥å¿—çº§åˆ«
        logging.getLogger("xfuser").setLevel(logging.DEBUG)
        logging.getLogger("torch.distributed").setLevel(logging.DEBUG)
        
        # è®¾ç½®GPUè®¾å¤‡
        try:
            # åœ¨Rayç¯å¢ƒä¸­è®¾ç½®CUDAè®¾å¤‡
            if RAY_AVAILABLE:
                # Rayä¼šè‡ªåŠ¨è®¾ç½®CUDA_VISIBLE_DEVICES
                self.device = f"cuda:{0}"  # åœ¨Rayä¸­æ€»æ˜¯0
                torch.cuda.set_device(0)
            else:
                # éRayç¯å¢ƒä¸‹æ‰‹åŠ¨è®¾ç½®
                os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_id)
                self.device = f"cuda:{0}"
                torch.cuda.set_device(0)
                
            logger.info(f"Worker initialized on GPU {gpu_id} (device={self.device}, rank={rank})")
            
        except Exception as e:
            logger.error(f"Failed to set GPU device for worker {gpu_id}: {e}")
            raise
    
    def initialize(self) -> bool:
        """Initialize the worker"""
        try:
            if not XDIT_AVAILABLE:
                logger.error(f"xDiT not available on GPU {self.gpu_id}")
                return False
            
            logger.info(f"Initializing worker on GPU {self.gpu_id} with strategy: {self.strategy}")
            
            # éªŒè¯GPUå¯ç”¨æ€§
            if not torch.cuda.is_available():
                logger.error(f"CUDA not available for worker on GPU {self.gpu_id}")
                return False
            
            # éªŒè¯å½“å‰è®¾å¤‡
            current_device = torch.cuda.current_device()
            logger.info(f"Current CUDA device for GPU {self.gpu_id}: {current_device}")
            
            # æ ‡è®°åˆå§‹åŒ–å®Œæˆ
            self.is_initialized = True
            logger.info(f"âœ… Worker on GPU {self.gpu_id} initialized")
            return True
            
        except Exception as e:
            logger.error(f"Failed to initialize worker on GPU {self.gpu_id}: {e}")
            logger.exception("Full traceback:")
            return False
    
    def initialize_distributed(self) -> bool:
        """Initialize distributed environment - called after all workers are ready"""
        if self.distributed_initialized:
            return True
            
        try:
            logger.info(f"[GPU {self.gpu_id}] Initializing distributed environment...")
            
            # æ¸…ç†å¯èƒ½å­˜åœ¨çš„æ—§è¿›ç¨‹ç»„
            if torch.distributed.is_initialized():
                try:
                    torch.distributed.destroy_process_group()
                    logger.info(f"[GPU {self.gpu_id}] Destroyed existing process group")
                except Exception as e:
                    logger.warning(f"Error destroying process group: {e}")
            
            # è®¾ç½®åˆ†å¸ƒå¼ç¯å¢ƒå˜é‡
            os.environ['MASTER_ADDR'] = self.master_addr
            os.environ['MASTER_PORT'] = str(self.master_port)
            os.environ['WORLD_SIZE'] = str(self.world_size)
            os.environ['RANK'] = str(self.rank)
            os.environ['LOCAL_RANK'] = '0'  # Rayä¸­æ¯ä¸ªworkeréƒ½æ˜¯local rank 0
            os.environ['NCCL_DEBUG'] = 'INFO'  # å¯ç”¨NCCLè°ƒè¯•
            
            logger.info(f"[GPU {self.gpu_id}] Distributed env: MASTER={self.master_addr}:{self.master_port}, "
                       f"WORLD_SIZE={self.world_size}, RANK={self.rank}")
            
            # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
            torch.distributed.init_process_group(
                backend='nccl',
                init_method=f'tcp://{self.master_addr}:{self.master_port}',
                world_size=self.world_size,
                rank=self.rank,
                timeout=datetime.timedelta(minutes=10)  # å¢åŠ è¶…æ—¶æ—¶é—´
            )
            
            # éªŒè¯åˆ†å¸ƒå¼åˆå§‹åŒ–
            if torch.distributed.is_initialized():
                world_size = torch.distributed.get_world_size()
                rank = torch.distributed.get_rank()
                logger.info(f"[GPU {self.gpu_id}] âœ… Distributed initialized: world_size={world_size}, rank={rank}")
                self.distributed_initialized = True
                return True
            else:
                logger.error(f"[GPU {self.gpu_id}] Distributed initialization failed")
                return False
                
        except Exception as e:
            logger.error(f"[GPU {self.gpu_id}] Failed to initialize distributed: {e}")
            logger.exception("Distributed init traceback:")
            self._cleanup_distributed()
            return False
    
    def _create_xfuser_pipeline_if_needed(self, model_path: str = None) -> bool:
        """å»¶è¿Ÿåˆ›å»ºxfuser pipelineï¼ˆä»…åœ¨ç¬¬ä¸€æ¬¡æ¨ç†æ—¶è°ƒç”¨ï¼‰"""
        if self.model_wrapper is not None:
            return True
            
        try:
            # ä½¿ç”¨ä¼ é€’çš„æ¨¡å‹è·¯å¾„ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
            effective_model_path = model_path or self.model_path
            logger.info(f"[GPU {self.gpu_id}] Creating xfuser pipeline with path: {effective_model_path}")
            
            # ç¡®ä¿åˆ†å¸ƒå¼ç¯å¢ƒå·²åˆå§‹åŒ–
            if not self.distributed_initialized:
                logger.error(f"[GPU {self.gpu_id}] Distributed not initialized")
                return False
            
            # åˆ›å»ºpipeline
            try:
                # ç¡®ä¿åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                torch.cuda.set_device(0)  # Rayä¸­æ€»æ˜¯0
                
                # ğŸ”§ æ£€æŸ¥æ¨¡å‹è·¯å¾„ç±»å‹å¹¶ç›¸åº”å¤„ç†
                if effective_model_path.endswith('.safetensors'):
                    logger.info(f"[GPU {self.gpu_id}] Processing safetensors file: {effective_model_path}")
                    
                    # å¯¹äºsafetensorsæ–‡ä»¶ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ComfyUIçš„æ–¹å¼åŠ è½½ï¼Œç„¶åè½¬æ¢ä¸ºxDiTå¯ç”¨çš„æ ¼å¼
                    # è¿™é‡Œæˆ‘ä»¬å…ˆå°è¯•ç›´æ¥ä½¿ç”¨safetensorsè·¯å¾„ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›Falseè§¦å‘fallback
                    try:
                        # å°è¯•ç›´æ¥ä½¿ç”¨safetensorsè·¯å¾„åˆ›å»ºpipeline
                        # æ³¨æ„ï¼šè¿™å¯èƒ½éœ€è¦xDiT/xFuseræ”¯æŒç›´æ¥åŠ è½½safetensors
                        logger.info(f"[GPU {self.gpu_id}] Attempting to load safetensors directly with xFuser")
                        
                        # ä½¿ç”¨xFuserArgsåˆ›å»ºé…ç½®ï¼Œä½†å…ˆæ£€æŸ¥æ˜¯å¦æ”¯æŒsafetensors
                        xfuser_args = xFuserArgs(
                            model=effective_model_path,
                            height=1024,
                            width=1024,
                            num_inference_steps=20,
                            guidance_scale=3.5,
                            output_type="latent",
                            tensor_parallel_degree=self.world_size,
                            use_ray=True,
                            ray_world_size=self.world_size
                        )
                        
                        engine_config = xfuser_args.create_config()
                        
                        # å°è¯•åˆ›å»ºpipeline
                        self.model_wrapper = xFuserFluxPipeline.from_pretrained(
                            effective_model_path,
                            engine_config=engine_config,
                            torch_dtype=torch.bfloat16,
                            low_cpu_mem_usage=True
                        )
                        
                        logger.info(f"[GPU {self.gpu_id}] âœ… Successfully loaded safetensors with xFuser")
                        
                    except Exception as safetensors_error:
                        logger.warning(f"[GPU {self.gpu_id}] Direct safetensors loading failed: {safetensors_error}")
                        logger.info(f"[GPU {self.gpu_id}] xFuser may not support direct safetensors loading")
                        # å¯¹äºsafetensorsï¼Œæˆ‘ä»¬è¿”å›Falseè®©ç³»ç»Ÿfallbackåˆ°ComfyUIåŸç”Ÿæ–¹å¼
                        return False
                        
                elif os.path.isdir(effective_model_path):
                    logger.info(f"[GPU {self.gpu_id}] Processing diffusers directory: {effective_model_path}")
                    
                    # éªŒè¯diffusersæ ¼å¼
                    model_index_path = os.path.join(effective_model_path, "model_index.json")
                    if not os.path.exists(model_index_path):
                        logger.error(f"[GPU {self.gpu_id}] Invalid diffusers directory: no model_index.json")
                        return False
                    
                    # ä½¿ç”¨xFuserArgsåˆ›å»ºå®Œæ•´é…ç½®
                    xfuser_args = xFuserArgs(
                        model=effective_model_path,
                        height=1024,
                        width=1024,
                        num_inference_steps=20,
                        guidance_scale=3.5,
                        output_type="latent",
                        tensor_parallel_degree=self.world_size,
                        use_ray=True,
                        ray_world_size=self.world_size
                    )
                    
                    engine_config = xfuser_args.create_config()
                    
                    # åˆ›å»ºpipeline
                    self.model_wrapper = xFuserFluxPipeline.from_pretrained(
                        effective_model_path,
                        engine_config=engine_config,
                        torch_dtype=torch.bfloat16,
                        low_cpu_mem_usage=True
                    )
                    
                    logger.info(f"[GPU {self.gpu_id}] âœ… Successfully loaded diffusers directory")
                    
                else:
                    logger.error(f"[GPU {self.gpu_id}] Unsupported model path format: {effective_model_path}")
                    return False
                
                # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                self.model_wrapper.to(self.device)
                
                logger.info(f"[GPU {self.gpu_id}] âœ… Pipeline created on {self.device}")
                return True
                
            except Exception as e:
                logger.error(f"[GPU {self.gpu_id}] Failed to create pipeline: {e}")
                logger.exception("Pipeline creation traceback:")
                self._cleanup_distributed()
                return False
                
        except Exception as e:
            logger.error(f"[GPU {self.gpu_id}] Pipeline creation failed: {e}")
            logger.exception("Full traceback:")
            self._cleanup_distributed()
            return False
    
    def _cleanup_distributed(self):
        """æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ"""
        try:
            if torch.distributed.is_initialized():
                torch.distributed.destroy_process_group()
                logger.info(f"[GPU {self.gpu_id}] Process group destroyed")
            
            # æ¸…ç†ç¯å¢ƒå˜é‡
            for var in ['MASTER_ADDR', 'MASTER_PORT', 'WORLD_SIZE', 'RANK', 'LOCAL_RANK', 'NCCL_DEBUG']:
                if var in os.environ:
                    del os.environ[var]
            
            self.distributed_initialized = False
                    
        except Exception as e:
            logger.warning(f"Error during distributed cleanup: {e}")
    
    def run_inference(self, 
                     model_info: Dict,
                     conditioning_positive: Any,
                     conditioning_negative: Any,
                     latent_samples: torch.Tensor,
                     num_inference_steps: int = 20,
                     guidance_scale: float = 8.0,
                     seed: int = 42) -> Optional[torch.Tensor]:
        """è¿è¡Œæ¨ç†"""
        try:
            if not self.is_initialized:
                logger.error(f"Worker on GPU {self.gpu_id} not initialized")
                return None
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†å¸ƒå¼æ¨ç†
            if self.world_size > 1:
                # å¤šGPUæ¨¡å¼ï¼šç¡®ä¿åˆ†å¸ƒå¼å·²åˆå§‹åŒ–
                if not self.distributed_initialized:
                    logger.error(f"[GPU {self.gpu_id}] Distributed not initialized for multi-GPU inference")
                    return None
                
                # åˆ›å»ºpipelineï¼Œä¼ é€’æ­£ç¡®çš„æ¨¡å‹è·¯å¾„
                model_path = model_info.get('path', self.model_path)
                if not self._create_xfuser_pipeline_if_needed(model_path):
                    logger.warning(f"âš ï¸ Pipeline creation failed on GPU {self.gpu_id}")
                    return None
            else:
                # å•GPUæ¨¡å¼ï¼šç›´æ¥è¿è¡Œ
                logger.info(f"[GPU {self.gpu_id}] Running single-GPU inference")
            
            logger.info(f"Running inference on GPU {self.gpu_id}: {num_inference_steps} steps")
            start_time = time.time()
            
            # è®¾ç½®éšæœºç§å­
            torch.manual_seed(seed)
            torch.cuda.manual_seed(seed)
            
            # ç¡®ä¿æ•°æ®åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            latent_samples = latent_samples.to(self.device)
            
            # å¤„ç†conditioning
            prompt_embeds = None
            pooled_prompt_embeds = None
            if conditioning_positive and len(conditioning_positive) > 0:
                if isinstance(conditioning_positive[0], torch.Tensor):
                    prompt_embeds = conditioning_positive[0].to(self.device)
                    if len(conditioning_positive) > 1:
                        pooled_prompt_embeds = conditioning_positive[1].to(self.device)
            
            negative_prompt_embeds = None
            negative_pooled_prompt_embeds = None
            if conditioning_negative and len(conditioning_negative) > 0:
                if isinstance(conditioning_negative[0], torch.Tensor):
                    negative_prompt_embeds = conditioning_negative[0].to(self.device)
                    if len(conditioning_negative) > 1:
                        negative_pooled_prompt_embeds = conditioning_negative[1].to(self.device)
            
            # å¦‚æœæ˜¯å•GPUæˆ–åˆ†å¸ƒå¼å¤±è´¥ï¼Œè¿”å›Noneè§¦å‘fallback
            if self.world_size == 1 or not self.distributed_initialized:
                logger.info(f"[GPU {self.gpu_id}] Returning None to trigger ComfyUI fallback")
                return None
            
            # åˆ›å»ºæ¨ç†é…ç½®
            model_path = model_info.get('path', self.model_path)
            xfuser_args = xFuserArgs(
                model=model_path,
                height=latent_samples.shape[2] * 8,
                width=latent_samples.shape[3] * 8,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                output_type="latent",
                strategy=self.strategy,
                device=self.device,
                dtype="bfloat16"
            )
            
            engine_config = xfuser_args.create_config()
            logger.info(f"[GPU {self.gpu_id}] Config created: steps={num_inference_steps}, CFG={guidance_scale}")
            
            # è¿è¡Œæ¨ç†
            try:
                with torch.inference_mode():
                    result = self.model_wrapper(
                        prompt_embeds=prompt_embeds,
                        pooled_prompt_embeds=pooled_prompt_embeds,
                        negative_prompt_embeds=negative_prompt_embeds,
                        negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,
                        height=latent_samples.shape[2] * 8,
                        width=latent_samples.shape[3] * 8,
                        num_inference_steps=num_inference_steps,
                        guidance_scale=guidance_scale,
                        generator=torch.Generator(device=self.device).manual_seed(seed),
                        output_type="latent",
                        latents=latent_samples,
                        engine_config=engine_config
                    )
                
                # æå–ç»“æœ
                if hasattr(result, 'images'):
                    result_latents = result.images
                elif hasattr(result, 'latents'):
                    result_latents = result.latents
                else:
                    result_latents = result
                
                # ç¡®ä¿ç»“æœåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                result_latents = result_latents.to(self.device)
                
                end_time = time.time()
                logger.info(f"âœ… Inference completed on GPU {self.gpu_id} in {end_time - start_time:.2f}s")
                
                return result_latents
                
            except Exception as e:
                logger.error(f"Inference failed on GPU {self.gpu_id}: {e}")
                logger.exception("Inference traceback:")
                return None
                
        except Exception as e:
            logger.error(f"Error during inference on GPU {self.gpu_id}: {e}")
            logger.exception("Full traceback:")
            return None
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            # æ¸…ç†æ¨¡å‹
            if self.model_wrapper is not None:
                del self.model_wrapper
                self.model_wrapper = None
            
            # æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ
            self._cleanup_distributed()
            
            # æ¸…ç†CUDAç¼“å­˜
            torch.cuda.empty_cache()
            
            self.is_initialized = False
            logger.info(f"âœ… Worker on GPU {self.gpu_id} cleaned up")
            
        except Exception as e:
            logger.error(f"Error during cleanup on GPU {self.gpu_id}: {e}")
            logger.exception("Cleanup traceback:")

# Non-Ray fallback worker for when Ray is not available
class XDiTWorkerFallback:
    """Fallback worker when Ray is not available"""
    
    def __init__(self, gpu_id: int, model_path: str, strategy: str = "Hybrid"):
        self.gpu_id = gpu_id
        self.model_path = model_path
        self.strategy = strategy
        self.model_wrapper = None
        self.is_initialized = False
        
        logger.info(f"Initializing fallback worker on GPU {gpu_id}")
    
    def initialize(self) -> bool:
        """Initialize the fallback worker"""
        try:
            # è®¾ç½®GPUè®¾å¤‡
            if torch.cuda.is_available():
                torch.cuda.set_device(self.gpu_id)
            
            logger.info(f"Fallback worker on GPU {self.gpu_id} ready")
            
            self.is_initialized = True
            logger.info(f"âœ… Fallback worker on GPU {self.gpu_id} initialized successfully")
            return True
            
        except Exception as e:
            logger.error(f"Failed to initialize fallback worker on GPU {self.gpu_id}: {e}")
            logger.exception("Full traceback:")
            return False
    
    def get_gpu_info(self) -> Dict[str, Any]:
        """Get GPU information"""
        try:
            if not torch.cuda.is_available():
                return {"gpu_id": self.gpu_id, "error": "CUDA not available"}
                
            props = torch.cuda.get_device_properties(self.gpu_id)
            memory_total = props.total_memory / (1024**3)
            memory_reserved = torch.cuda.memory_reserved(self.gpu_id) / (1024**3)
            memory_allocated = torch.cuda.memory_allocated(self.gpu_id) / (1024**3)
            
            return {
                "gpu_id": self.gpu_id,
                "name": props.name,
                "memory_total_gb": memory_total,
                "memory_reserved_gb": memory_reserved,
                "memory_allocated_gb": memory_allocated,
                "memory_free_gb": memory_total - memory_reserved,
                "compute_capability": f"{props.major}.{props.minor}",
                "is_initialized": self.is_initialized
            }
        except Exception as e:
            logger.error(f"Error getting GPU info for GPU {self.gpu_id}: {e}")
            return {"gpu_id": self.gpu_id, "error": str(e)}
    
    def wrap_model_for_xdit(self, model_state_dict: Dict, model_config: Dict) -> bool:
        """Wrap model with xDiT (fallback version)"""
        try:
            logger.info(f"Wrapping model with fallback method on GPU {self.gpu_id}")
            
            # åœ¨æ²¡æœ‰xDiTçš„æƒ…å†µä¸‹ï¼Œç®€å•åœ°å‡†å¤‡å¥½æ¥å—æ¨¡å‹
            self.model_wrapper = "fallback_placeholder"
            
            logger.info(f"âœ… Model ready on GPU {self.gpu_id} (fallback mode)")
            return True
            
        except Exception as e:
            logger.error(f"Failed to wrap model on GPU {self.gpu_id}: {e}")
            return False
    
    def run_inference(self, 
                     model_info: Dict,  # æ”¹ä¸ºè½»é‡çº§çš„æ¨¡å‹ä¿¡æ¯
                     conditioning_positive: Any,
                     conditioning_negative: Any,
                     latent_samples: torch.Tensor,
                     num_inference_steps: int = 20,
                     guidance_scale: float = 8.0,
                     seed: int = 42) -> Optional[torch.Tensor]:
        """Run inference (fallback version)"""
        try:
            if not self.is_initialized:
                logger.error(f"Fallback worker on GPU {self.gpu_id} not initialized")
                return None
            
            logger.info(f"Running fallback inference on GPU {self.gpu_id}")
            logger.info(f"Model info: {model_info.get('type', 'unknown')}, Latent shape: {latent_samples.shape}")
            start_time = time.time()
            
            # è®¾ç½®éšæœºç§å­
            torch.manual_seed(seed)
            if torch.cuda.is_available():
                torch.cuda.manual_seed(seed)
            
            # ğŸ”§ ä¼˜åŒ–ï¼šç¡®ä¿æ•°æ®åœ¨æ­£ç¡®çš„GPUä¸Š
            device = f"cuda:{self.gpu_id}" if torch.cuda.is_available() else "cpu"
            if latent_samples.device != torch.device(device):
                latent_samples = latent_samples.to(device)
            
            # ğŸ”§ ä¿®å¤Fluxæ¨¡å‹é€šé“æ•°é—®é¢˜
            # Fluxæ¨¡å‹ä½¿ç”¨16é€šé“latent spaceï¼Œå¦‚æœè¾“å…¥æ˜¯4é€šé“ï¼Œéœ€è¦è¿›è¡Œè½¬æ¢
            if latent_samples.shape[1] == 4 and model_info.get('type', '').lower() == 'flux':
                logger.info(f"Converting 4-channel to 16-channel latent for Flux model")
                # ç®€å•çš„é€šé“æ‰©å±•ç­–ç•¥ï¼ˆå®é™…çš„xDiTä¼šæœ‰æ›´å¤æ‚çš„å¤„ç†ï¼‰
                # æ–¹æ³•1ï¼šé‡å¤é€šé“ [1,4,H,W] -> [1,16,H,W]
                expanded_latents = latent_samples.repeat(1, 4, 1, 1)
                # æ–¹æ³•2ï¼šä¹Ÿå¯ä»¥ç”¨é›¶å¡«å……
                # expanded_latents = torch.cat([latent_samples, torch.zeros_like(latent_samples).repeat(1, 3, 1, 1)], dim=1)
                latent_samples = expanded_latents
                logger.info(f"Latent shape converted to: {latent_samples.shape}")
            
            # åœ¨fallbackæ¨¡å¼ä¸‹ï¼Œåªæ˜¯è¿”å›åŸå§‹latents
            # å®é™…ä½¿ç”¨ä¸­ä¼šå›é€€åˆ°ComfyUIçš„æ ‡å‡†é‡‡æ ·
            # result_latents = latent_samples.clone()
            
            # TODO: fallbackæ¨¡å¼ä¹Ÿè¿”å›Noneï¼Œè®©ç³»ç»Ÿä½¿ç”¨æ ‡å‡†ComfyUIé‡‡æ ·
            logger.info(f"âš ï¸ Fallback worker returning None to trigger standard ComfyUI sampling")
            return None
            
            end_time = time.time()
            logger.info(f"âœ… Fallback inference completed on GPU {self.gpu_id} in {end_time - start_time:.2f}s")
            
            # return result_latents
                
        except Exception as e:
            logger.error(f"Error during fallback inference on GPU {self.gpu_id}: {e}")
            logger.exception("Full traceback:")
            return None
    
    def cleanup(self):
        """Clean up resources"""
        try:
            if self.model_wrapper is not None:
                del self.model_wrapper
                self.model_wrapper = None
            
            self.is_initialized = False
            
            # Clear CUDA cache
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
            logger.info(f"âœ… Fallback worker on GPU {self.gpu_id} cleaned up")
            
        except Exception as e:
            logger.error(f"Error during cleanup on GPU {self.gpu_id}: {e}") 