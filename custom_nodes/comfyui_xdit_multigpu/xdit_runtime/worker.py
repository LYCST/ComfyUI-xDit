"""
xDiT Ray Worker
==============

Ray Actor for GPU-specific inference operations.
"""

import os
import torch
import logging
import time
import datetime
from typing import Dict, Any, Optional, List, Tuple
import numpy as np
import threading
import socket
import signal

# Try to import Ray
try:
    import ray
    RAY_AVAILABLE = True
except ImportError:
    RAY_AVAILABLE = False

# Try to import xDiT with correct API
try:
    import xfuser
    from xfuser import xFuserArgs
    from xfuser.core.distributed import get_world_group
    # Import FLUX specific pipeline and config
    from xfuser import xFuserFluxPipeline
    from xfuser.config.config import EngineConfig
    XDIT_AVAILABLE = True
except ImportError:
    XDIT_AVAILABLE = False

logger = logging.getLogger(__name__)

def find_free_port():
    """Find a free port for distributed communication"""
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.bind(('', 0))
        s.listen(1)
        port = s.getsockname()[1]
    return port

@ray.remote(num_gpus=1) if RAY_AVAILABLE else None
class XDiTWorker:
    """
    Ray Actor for GPU-specific xDiT operations.
    Each worker is assigned to one GPU.
    """
    
    def __init__(self, gpu_id: int, model_path: str, strategy: str = "Hybrid", 
                 master_addr: str = "127.0.0.1", master_port: int = 29500, 
                 world_size: int = 8, rank: int = 0):
        self.gpu_id = gpu_id
        self.model_path = model_path
        self.strategy = strategy
        self.model_wrapper = None
        self.is_initialized = False
        self.world_size = world_size
        self.rank = rank
        self.master_addr = master_addr
        self.master_port = master_port
        self.distributed_initialized = False
        
        # è®¾ç½®è¯¦ç»†çš„æ—¥å¿—çº§åˆ«
        logging.getLogger("xfuser").setLevel(logging.DEBUG)
        logging.getLogger("torch.distributed").setLevel(logging.DEBUG)
        
        # è®¾ç½®GPUè®¾å¤‡
        try:
            # åœ¨Rayç¯å¢ƒä¸­è®¾ç½®CUDAè®¾å¤‡
            if RAY_AVAILABLE:
                # Rayä¼šè‡ªåŠ¨è®¾ç½®CUDA_VISIBLE_DEVICES
                self.device = f"cuda:{0}"  # åœ¨Rayä¸­æ€»æ˜¯0
                torch.cuda.set_device(0)
            else:
                # éRayç¯å¢ƒä¸‹æ‰‹åŠ¨è®¾ç½®
                os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_id)
                self.device = f"cuda:{0}"
                torch.cuda.set_device(0)
                
            logger.info(f"Worker initialized on GPU {gpu_id} (device={self.device}, rank={rank})")
            
        except Exception as e:
            logger.error(f"Failed to set GPU device for worker {gpu_id}: {e}")
            raise
    
    def initialize(self) -> bool:
        """Initialize the worker"""
        try:
            if not XDIT_AVAILABLE:
                logger.error(f"xDiT not available on GPU {self.gpu_id}")
                return False
            
            logger.info(f"Initializing worker on GPU {self.gpu_id} with strategy: {self.strategy}")
            
            # éªŒè¯GPUå¯ç”¨æ€§
            if not torch.cuda.is_available():
                logger.error(f"CUDA not available for worker on GPU {self.gpu_id}")
                return False
            
            # éªŒè¯å½“å‰è®¾å¤‡
            current_device = torch.cuda.current_device()
            logger.info(f"Current CUDA device for GPU {self.gpu_id}: {current_device}")
            
            # æ ‡è®°åˆå§‹åŒ–å®Œæˆ
            self.is_initialized = True
            logger.info(f"âœ… Worker on GPU {self.gpu_id} initialized")
            return True
            
        except Exception as e:
            logger.error(f"Failed to initialize worker on GPU {self.gpu_id}: {e}")
            logger.exception("Full traceback:")
            return False
    
    def initialize_distributed(self) -> bool:
        """Initialize distributed environment - called after all workers are ready"""
        if self.distributed_initialized:
            return True
            
        try:
            logger.info(f"[GPU {self.gpu_id}] Initializing distributed environment...")
            
            # æ¸…ç†å¯èƒ½å­˜åœ¨çš„æ—§è¿›ç¨‹ç»„
            if torch.distributed.is_initialized():
                try:
                    torch.distributed.destroy_process_group()
                    logger.info(f"[GPU {self.gpu_id}] Destroyed existing process group")
                except Exception as e:
                    logger.warning(f"Error destroying process group: {e}")
            
            # è®¾ç½®åˆ†å¸ƒå¼ç¯å¢ƒå˜é‡
            os.environ['MASTER_ADDR'] = self.master_addr
            os.environ['MASTER_PORT'] = str(self.master_port)
            os.environ['WORLD_SIZE'] = str(self.world_size)
            os.environ['RANK'] = str(self.rank)
            os.environ['LOCAL_RANK'] = '0'  # Rayä¸­æ¯ä¸ªworkeréƒ½æ˜¯local rank 0
            os.environ['NCCL_DEBUG'] = 'INFO'  # å¯ç”¨NCCLè°ƒè¯•
            
            logger.info(f"[GPU {self.gpu_id}] Distributed env: MASTER={self.master_addr}:{self.master_port}, "
                       f"WORLD_SIZE={self.world_size}, RANK={self.rank}")
            
            # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
            torch.distributed.init_process_group(
                backend='nccl',
                init_method=f'tcp://{self.master_addr}:{self.master_port}',
                world_size=self.world_size,
                rank=self.rank,
                timeout=datetime.timedelta(minutes=10)  # å¢åŠ è¶…æ—¶æ—¶é—´
            )
            
            # éªŒè¯åˆ†å¸ƒå¼åˆå§‹åŒ–
            if torch.distributed.is_initialized():
                world_size = torch.distributed.get_world_size()
                rank = torch.distributed.get_rank()
                logger.info(f"[GPU {self.gpu_id}] âœ… Distributed initialized: world_size={world_size}, rank={rank}")
                self.distributed_initialized = True
                return True
            else:
                logger.error(f"[GPU {self.gpu_id}] Distributed initialization failed")
                return False
                
        except Exception as e:
            logger.error(f"[GPU {self.gpu_id}] Failed to initialize distributed: {e}")
            logger.exception("Distributed init traceback:")
            self._cleanup_distributed()
            return False
    
    def _create_xfuser_pipeline_if_needed(self, model_path: str = None) -> bool:
        """åˆ›å»ºxfuser pipeline - ä½¿ç”¨xDiTçš„åŸå§‹æ–¹æ³•"""
        if self.model_wrapper is not None:
            return True
            
        try:
            # ä½¿ç”¨ä¼ é€’çš„æ¨¡å‹è·¯å¾„ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
            effective_model_path = model_path or self.model_path
            logger.info(f"[GPU {self.gpu_id}] Creating xfuser pipeline with path: {effective_model_path}")
            
            # ç¡®ä¿åˆ†å¸ƒå¼ç¯å¢ƒå·²åˆå§‹åŒ–
            if not self.distributed_initialized:
                logger.error(f"[GPU {self.gpu_id}] Distributed not initialized")
                return False
            
            # åˆ›å»ºpipeline - ä½¿ç”¨xDiTçš„åŸå§‹æ–¹æ³•
            try:
                # ç¡®ä¿åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                torch.cuda.set_device(0)  # Rayä¸­æ€»æ˜¯0
                
                # ğŸ¯ å…³é”®ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨xDiTçš„åŸå§‹æ–¹æ³•
                # å¯¹äºsafetensorsæ–‡ä»¶ï¼ŒxDiTåº”è¯¥èƒ½å¤Ÿç›´æ¥å¤„ç†
                if effective_model_path.endswith('.safetensors'):
                    logger.info(f"[GPU {self.gpu_id}] Loading safetensors with xDiT: {effective_model_path}")
                    
                    # ä½¿ç”¨xFuserArgsåˆ›å»ºé…ç½®
                    xfuser_args = xFuserArgs(
                        model=effective_model_path,  # ç›´æ¥ä¼ é€’safetensorsè·¯å¾„
                        height=1024,
                        width=1024,
                        num_inference_steps=20,
                        guidance_scale=3.5,
                        output_type="latent",
                        tensor_parallel_degree=self.world_size,
                        use_ray=True,
                        ray_world_size=self.world_size
                    )
                    
                    engine_config = xfuser_args.create_config()
                    
                    # ğŸ¯ ç›´æ¥ä½¿ç”¨xDiTçš„from_pretrainedæ–¹æ³•
                    # xDiTåº”è¯¥èƒ½å¤Ÿå¤„ç†safetensorsæ–‡ä»¶
                    self.model_wrapper = xFuserFluxPipeline.from_pretrained(
                        effective_model_path,
                        engine_config=engine_config,
                        torch_dtype=torch.bfloat16,
                        low_cpu_mem_usage=True
                    )
                    
                    logger.info(f"[GPU {self.gpu_id}] âœ… Successfully loaded safetensors with xDiT")
                    
                elif os.path.isdir(effective_model_path):
                    logger.info(f"[GPU {self.gpu_id}] Processing diffusers directory: {effective_model_path}")
                    
                    # éªŒè¯diffusersæ ¼å¼
                    model_index_path = os.path.join(effective_model_path, "model_index.json")
                    if not os.path.exists(model_index_path):
                        logger.error(f"[GPU {self.gpu_id}] Invalid diffusers directory: no model_index.json")
                        return False
                    
                    # ä½¿ç”¨xFuserArgsåˆ›å»ºå®Œæ•´é…ç½®
                    xfuser_args = xFuserArgs(
                        model=effective_model_path,
                        height=1024,
                        width=1024,
                        num_inference_steps=20,
                        guidance_scale=3.5,
                        output_type="latent",
                        tensor_parallel_degree=self.world_size,
                        use_ray=True,
                        ray_world_size=self.world_size
                    )
                    
                    engine_config = xfuser_args.create_config()
                    
                    # åˆ›å»ºpipeline
                    self.model_wrapper = xFuserFluxPipeline.from_pretrained(
                        effective_model_path,
                        engine_config=engine_config,
                        torch_dtype=torch.bfloat16,
                        low_cpu_mem_usage=True
                    )
                    
                    logger.info(f"[GPU {self.gpu_id}] âœ… Successfully loaded diffusers directory")
                    
                else:
                    logger.error(f"[GPU {self.gpu_id}] Unsupported model path format: {effective_model_path}")
                    return False
                
                # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                self.model_wrapper.to(self.device)
                
                logger.info(f"[GPU {self.gpu_id}] âœ… Pipeline created successfully!")
                return True
                
            except Exception as e:
                logger.error(f"[GPU {self.gpu_id}] Failed to create pipeline: {e}")
                logger.exception("Pipeline creation traceback:")
                return False
                
        except Exception as e:
            logger.error(f"[GPU {self.gpu_id}] Pipeline creation failed: {e}")
            logger.exception("Full traceback:")
            return False
    
    def _create_flux_pipeline_from_comfyui_components(self, model_info: Dict) -> bool:
        """ä»ComfyUIç»„ä»¶åˆ›å»ºFluxPipelineç”¨äºxDiTåŒ…è£…"""
        # è®¾ç½®è¶…æ—¶æœºåˆ¶
        timeout_seconds = 30  # 30ç§’è¶…æ—¶
        
        def timeout_handler(signum, frame):
            raise TimeoutError(f"Pipeline creation timed out after {timeout_seconds} seconds")
        
        # åœ¨Linuxä¸Šè®¾ç½®ä¿¡å·å¤„ç†å™¨
        try:
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(timeout_seconds)
        except (AttributeError, OSError):
            # Windowsä¸æ”¯æŒSIGALRMï¼Œä½¿ç”¨çº¿ç¨‹è¶…æ—¶
            pass
        
        try:
            logger.info(f"[GPU {self.gpu_id}] ğŸ¯ Creating FluxPipeline from ComfyUI components (timeout: {timeout_seconds}s)")
            
            # ğŸš€ å…³é”®æ´å¯Ÿï¼šComfyUIå·²ç»æœ‰äº†æ‰€æœ‰å¿…è¦çš„ç»„ä»¶ï¼
            # æˆ‘ä»¬éœ€è¦åšçš„æ˜¯ï¼š
            # 1. åŠ è½½safetensorsä½œä¸ºtransformer
            # 2. ä»ComfyUIè·å–å…¶ä»–ç»„ä»¶ï¼ˆVAE, CLIPç­‰ï¼‰
            # 3. ç»„è£…æˆdiffusersæ ¼å¼çš„FluxPipeline
            
            logger.info(f"[GPU {self.gpu_id}] ğŸ“¥ Step 1: Importing required modules...")
            from diffusers import FluxPipeline, FluxTransformer2DModel
            import torch
            import safetensors.torch
            
            # åŠ è½½safetensors transformeræƒé‡
            safetensors_path = model_info.get('path')
            logger.info(f"[GPU {self.gpu_id}] ğŸ“¥ Step 2: Loading transformer from: {safetensors_path}")
            
            custom_weights = safetensors.torch.load_file(safetensors_path)
            logger.info(f"[GPU {self.gpu_id}] âœ… Safetensors loaded, keys: {len(custom_weights)}")
            
            # åˆ›å»ºFluxTransformer2DModel
            logger.info(f"[GPU {self.gpu_id}] ğŸ“¥ Step 3: Creating FluxTransformer2DModel...")
            transformer = FluxTransformer2DModel(
                patch_size=1,
                in_channels=64,
                num_layers=19,
                num_single_layers=38,
                attention_head_dim=128,
                num_attention_heads=24,
                joint_attention_dim=4096,
                pooled_projection_dim=768,
                guidance_embeds=False,
                axes_dims_rope=(16, 56, 56)
            )
            
            # åŠ è½½è‡ªå®šä¹‰æƒé‡
            logger.info(f"[GPU {self.gpu_id}] ğŸ“¥ Step 4: Loading transformer weights...")
            missing_keys, unexpected_keys = transformer.load_state_dict(custom_weights, strict=False)
            logger.info(f"[GPU {self.gpu_id}] âœ… Transformer loaded: {len(missing_keys)} missing, {len(unexpected_keys)} unexpected")
            
            # ğŸ¯ æ–°ç­–ç•¥ï¼šç›´æ¥ä½¿ç”¨ComfyUIå·²åŠ è½½çš„ç»„ä»¶ï¼
            logger.info(f"[GPU {self.gpu_id}] ğŸ’¡ Smart strategy: Reusing ComfyUI loaded components")
            
            # ä»model_infoä¸­è·å–ComfyUIå·²åŠ è½½çš„ç»„ä»¶
            comfyui_vae = model_info.get('vae')
            comfyui_clip = model_info.get('clip')
            
            if comfyui_vae is None or comfyui_clip is None:
                logger.warning(f"[GPU {self.gpu_id}] ComfyUI components not available in model_info")
                logger.info(f"[GPU {self.gpu_id}] Available keys: {list(model_info.keys())}")
                logger.info(f"[GPU {self.gpu_id}] VAE: {'âœ…' if comfyui_vae else 'âŒ'}, CLIP: {'âœ…' if comfyui_clip else 'âŒ'}")
                return False
            
            logger.info(f"[GPU {self.gpu_id}] âœ… Found ComfyUI loaded VAE and CLIP components")
            
            # ğŸ¯ å…³é”®ï¼šç›´æ¥è½¬æ¢ComfyUIç»„ä»¶ä¸ºdiffusersæ ¼å¼
            logger.info(f"[GPU {self.gpu_id}] ğŸ“¥ Step 5: Converting ComfyUI VAE to diffusers format...")
            logger.info(f"[GPU {self.gpu_id}] ComfyUI VAE type: {type(comfyui_vae)}")
            print(f"ğŸ” [GPU {self.gpu_id}] DEBUG: VAE type = {type(comfyui_vae)}")  # å¼ºåˆ¶è¾“å‡º
            
            # ComfyUIçš„VAEé€šå¸¸æ˜¯comfy.model_management.VAEç±»å‹
            # æˆ‘ä»¬éœ€è¦æå–å…¶å†…éƒ¨çš„diffusers VAE
            diffusers_vae = None
            vae_type_name = type(comfyui_vae).__name__
            logger.info(f"[GPU {self.gpu_id}] VAE type name: {vae_type_name}")
            print(f"ğŸ” [GPU {self.gpu_id}] DEBUG: VAE type name = {vae_type_name}")  # å¼ºåˆ¶è¾“å‡º
            
            if hasattr(comfyui_vae, 'first_stage_model'):
                diffusers_vae = comfyui_vae.first_stage_model
                logger.info(f"[GPU {self.gpu_id}] âœ… Extracted first_stage_model")
                print(f"ğŸ” [GPU {self.gpu_id}] DEBUG: Using first_stage_model")
            elif hasattr(comfyui_vae, 'model'):
                diffusers_vae = comfyui_vae.model
                logger.info(f"[GPU {self.gpu_id}] âœ… Extracted model")
                print(f"ğŸ” [GPU {self.gpu_id}] DEBUG: Using model")
            elif 'AutoencodingEngine' in vae_type_name or hasattr(comfyui_vae, 'decoder'):
                # å¦‚æœæ˜¯AutoencodingEngineç±»å‹ï¼Œæˆ‘ä»¬éœ€è¦åŒ…è£…å®ƒ
                logger.info(f"[GPU {self.gpu_id}] ğŸ”§ Detected AutoencodingEngine - creating wrapper")
                print(f"ğŸ” [GPU {self.gpu_id}] DEBUG: Creating VAE wrapper for {vae_type_name}")  # å¼ºåˆ¶è¾“å‡º
                
                # åˆ›å»ºä¸€ä¸ªä¸FLUX VAEå…¼å®¹çš„config
                class VAEWrapper:
                    def __init__(self, autoencoding_engine):
                        self.autoencoding_engine = autoencoding_engine
                        # åˆ›å»ºä¸€ä¸ªä¸FLUX VAEå…¼å®¹çš„config
                        self.config = type('Config', (), {
                            'block_out_channels': [128, 256, 512, 512],  # FLUX VAEé…ç½®
                            'in_channels': 3,
                            'out_channels': 3,
                            'latent_channels': 16,
                            'sample_size': 1024,  # FLUXé»˜è®¤å°ºå¯¸
                            'scaling_factor': 0.3611,  # FLUX VAE scaling factor
                            'shift_factor': 0.1159,   # FLUX VAE shift factor
                        })()
                        
                        # ç¡®ä¿åŒ…è£…å™¨æœ‰æ­£ç¡®çš„è®¾å¤‡å±æ€§
                        self.device = getattr(autoencoding_engine, 'device', torch.device('cuda:0'))
                        self.dtype = getattr(autoencoding_engine, 'dtype', torch.bfloat16)
                    
                    def encode(self, x):
                        return self.autoencoding_engine.encode(x)
                    
                    def decode(self, x):
                        return self.autoencoding_engine.decode(x)
                    
                    def to(self, device):
                        # ç¡®ä¿è®¾å¤‡è½¬ç§»æ­£ç¡®ä¼ é€’
                        self.autoencoding_engine = self.autoencoding_engine.to(device)
                        self.device = device
                        return self
                    
                    @property
                    def parameters(self):
                        return self.autoencoding_engine.parameters()
                    
                    def __getattr__(self, name):
                        # å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯æˆ‘ä»¬å®šä¹‰çš„å±æ€§
                        if name in ['config', 'device', 'dtype']:
                            return object.__getattribute__(self, name)
                        # å¦åˆ™ä»autoencoding_engineè·å–
                        return getattr(self.autoencoding_engine, name)
                
                diffusers_vae = VAEWrapper(comfyui_vae)
                logger.info(f"[GPU {self.gpu_id}] âœ… Created VAE wrapper with FLUX-compatible config")
                
                # éªŒè¯VAE wrapper
                if not hasattr(diffusers_vae, 'config'):
                    logger.error(f"[GPU {self.gpu_id}] VAE wrapper missing config attribute!")
                    return False
                
                logger.info(f"[GPU {self.gpu_id}] VAE config block_out_channels: {diffusers_vae.config.block_out_channels}")
                print(f"ğŸ” [GPU {self.gpu_id}] DEBUG: VAE wrapper created successfully with config!")
            else:
                # å¦‚æœæ— æ³•æå–ï¼Œè¿”å›å¤±è´¥
                logger.error(f"[GPU {self.gpu_id}] Cannot extract diffusers VAE from ComfyUI VAE")
                logger.error(f"[GPU {self.gpu_id}] VAE type: {vae_type_name}")
                logger.error(f"[GPU {self.gpu_id}] Available attributes: {[attr for attr in dir(comfyui_vae) if not attr.startswith('_')]}")
                print(f"ğŸ” [GPU {self.gpu_id}] DEBUG: VAE extraction failed!")
                return False
            
            print(f"ğŸ” [GPU {self.gpu_id}] DEBUG: Final VAE type = {type(diffusers_vae)}")
            logger.info(f"[GPU {self.gpu_id}] âœ… VAE component ready")
            
            # å°†ComfyUIçš„CLIPè½¬æ¢ä¸ºdiffusersæ ¼å¼
            logger.info(f"[GPU {self.gpu_id}] ğŸ“¥ Step 6: Converting ComfyUI CLIP to diffusers format...")
            logger.info(f"[GPU {self.gpu_id}] ComfyUI CLIP type: {type(comfyui_clip)}")
            
            # ComfyUIçš„CLIPå¯èƒ½åŒ…å«å¤šä¸ªtext encoder
            # å¯¹äºFLUXï¼Œæˆ‘ä»¬éœ€è¦CLIP-Lå’ŒT5-XXL
            text_encoder = None
            text_encoder_2 = None
            
            if hasattr(comfyui_clip, 'cond_stage_model'):
                # å°è¯•ä»ComfyUI CLIPä¸­æå–text encoder
                clip_model = comfyui_clip.cond_stage_model
                logger.info(f"[GPU {self.gpu_id}] Found cond_stage_model: {type(clip_model)}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å¤šä¸ªtext encoderï¼ˆFLUXéœ€è¦ä¸¤ä¸ªï¼‰
                if hasattr(clip_model, 'clip_l'):
                    text_encoder = clip_model.clip_l
                    logger.info(f"[GPU {self.gpu_id}] âœ… Found CLIP-L from ComfyUI")
                
                if hasattr(clip_model, 't5xxl'):
                    text_encoder_2 = clip_model.t5xxl
                    logger.info(f"[GPU {self.gpu_id}] âœ… Found T5-XXL from ComfyUI")
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•å…¶ä»–å±æ€§
                if text_encoder is None and hasattr(clip_model, 'transformer'):
                    text_encoder = clip_model.transformer
                    logger.info(f"[GPU {self.gpu_id}] âœ… Found transformer as text_encoder")
                
                logger.info(f"[GPU {self.gpu_id}] Available clip_model attributes: {[attr for attr in dir(clip_model) if not attr.startswith('_')]}")
            
            # å¦‚æœæ— æ³•ä»ComfyUIæå–å¿…è¦çš„text encodersï¼Œè¿”å›å¤±è´¥
            if text_encoder is None or text_encoder_2 is None:
                logger.error(f"[GPU {self.gpu_id}] Missing text encoders from ComfyUI CLIP")
                logger.error(f"[GPU {self.gpu_id}] CLIP-L: {'âœ…' if text_encoder else 'âŒ'}, T5-XXL: {'âœ…' if text_encoder_2 else 'âŒ'}")
                return False
            
            # åˆ›å»ºç®€å•çš„tokenizerï¼ˆä¸éœ€è¦ä¸‹è½½ï¼‰
            logger.info(f"[GPU {self.gpu_id}] ğŸ“¥ Step 7: Creating lightweight tokenizers...")
            try:
                # ğŸ”§ ä¿®å¤tokenizeråˆ›å»ºé—®é¢˜ - åˆ›å»ºæœ€ç®€å•çš„tokenizeråŒ…è£…å™¨
                logger.info(f"[GPU {self.gpu_id}] Creating minimal tokenizer wrappers...")
                
                class MinimalTokenizer:
                    def __init__(self, vocab_size=49408, max_length=77):
                        self.vocab_size = vocab_size
                        self.model_max_length = max_length
                        self.pad_token_id = 0
                        self.eos_token_id = 2
                        self.bos_token_id = 1
                        self.unk_token_id = 3
                    
                    def __call__(self, text, **kwargs):
                        # è¿”å›ä¸€ä¸ªåŸºæœ¬çš„ç¼–ç ç»“æœ
                        batch_size = 1 if isinstance(text, str) else len(text)
                        return {
                            'input_ids': torch.zeros((batch_size, self.model_max_length), dtype=torch.long),
                            'attention_mask': torch.ones((batch_size, self.model_max_length), dtype=torch.long)
                        }
                    
                    def encode(self, text, **kwargs):
                        return [0] * self.model_max_length
                    
                    def decode(self, token_ids, **kwargs):
                        return ""
                    
                    def batch_decode(self, sequences, **kwargs):
                        return [""] * len(sequences)
                
                # ä¸ºCLIPå’ŒT5åˆ›å»ºä¸åŒçš„tokenizer
                tokenizer = MinimalTokenizer(vocab_size=49408, max_length=77)  # CLIP tokenizer
                tokenizer_2 = MinimalTokenizer(vocab_size=32128, max_length=512)  # T5 tokenizer
                
                logger.info(f"[GPU {self.gpu_id}] âœ… Minimal tokenizer wrappers created successfully")
                
            except Exception as tokenizer_error:
                logger.error(f"[GPU {self.gpu_id}] Failed to create tokenizers: {tokenizer_error}")
                return False
            
            # åˆ›å»ºschedulerï¼ˆè½»é‡çº§ï¼‰
            logger.info(f"[GPU {self.gpu_id}] ğŸ“¥ Step 8: Creating scheduler...")
            try:
                from diffusers import FlowMatchEulerDiscreteScheduler
                scheduler = FlowMatchEulerDiscreteScheduler()
                logger.info(f"[GPU {self.gpu_id}] âœ… Scheduler created successfully")
            except Exception as scheduler_error:
                logger.error(f"[GPU {self.gpu_id}] Failed to create scheduler: {scheduler_error}")
                return False
            
            logger.info(f"[GPU {self.gpu_id}] âœ… All components ready for pipeline assembly")
            
            # ç»„è£…FluxPipeline
            logger.info(f"[GPU {self.gpu_id}] ğŸ“¥ Step 9: Assembling FluxPipeline...")
            try:
                pipeline = FluxPipeline(
                    transformer=transformer,
                    scheduler=scheduler,
                    vae=diffusers_vae,
                    text_encoder=text_encoder,
                    tokenizer=tokenizer,
                    text_encoder_2=text_encoder_2,
                    tokenizer_2=tokenizer_2,
                )
                
                logger.info(f"[GPU {self.gpu_id}] ğŸ‰ FluxPipeline created successfully using ComfyUI components!")
                
                # åˆ›å»ºxFuser wrapper
                logger.info(f"[GPU {self.gpu_id}] ğŸ“¥ Step 10: Creating xFuser wrapper...")
                from xfuser import xFuserFluxPipeline
                self.model_wrapper = xFuserFluxPipeline(pipeline, self.engine_config)
                
                logger.info(f"[GPU {self.gpu_id}] âœ… xFuser multi-GPU pipeline ready with ComfyUI components!")
                return True
                
            except Exception as pipeline_error:
                logger.error(f"[GPU {self.gpu_id}] Failed to create FluxPipeline: {pipeline_error}")
                logger.exception("Pipeline creation error:")
                return False
                
        except TimeoutError as e:
            logger.error(f"[GPU {self.gpu_id}] Pipeline creation timed out: {e}")
            return False
        except Exception as e:
            logger.error(f"[GPU {self.gpu_id}] Failed to create FluxPipeline from ComfyUI components: {e}")
            logger.exception("Component creation traceback:")
            return False
        finally:
            # æ¸…ç†è¶…æ—¶è®¾ç½®
            try:
                signal.alarm(0)  # å–æ¶ˆè¶…æ—¶
            except (AttributeError, OSError):
                pass
    
    def _cleanup_distributed(self):
        """æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ"""
        try:
            if torch.distributed.is_initialized():
                torch.distributed.destroy_process_group()
                logger.info(f"[GPU {self.gpu_id}] Process group destroyed")
            
            # æ¸…ç†ç¯å¢ƒå˜é‡
            for var in ['MASTER_ADDR', 'MASTER_PORT', 'WORLD_SIZE', 'RANK', 'LOCAL_RANK', 'NCCL_DEBUG']:
                if var in os.environ:
                    del os.environ[var]
            
            self.distributed_initialized = False
                    
        except Exception as e:
            logger.warning(f"Error during distributed cleanup: {e}")
    
    def run_inference(self, 
                     model_info: Dict,
                     conditioning_positive: Any,
                     conditioning_negative: Any,
                     latent_samples: torch.Tensor,
                     num_inference_steps: int = 20,
                     guidance_scale: float = 8.0,
                     seed: int = 42) -> Optional[torch.Tensor]:
        """è¿è¡Œæ¨ç† - ä½¿ç”¨xDiTçš„åŸå§‹æ–¹æ³•"""
        try:
            if not self.is_initialized:
                logger.error(f"Worker on GPU {self.gpu_id} not initialized")
                return None
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†å¸ƒå¼æ¨ç†
            if self.world_size > 1:
                # å¤šGPUæ¨¡å¼ï¼šç¡®ä¿åˆ†å¸ƒå¼å·²åˆå§‹åŒ–
                if not self.distributed_initialized:
                    logger.error(f"[GPU {self.gpu_id}] Distributed not initialized for multi-GPU inference")
                    return None
                
                # åˆ›å»ºpipelineï¼Œä¼ é€’æ­£ç¡®çš„æ¨¡å‹è·¯å¾„
                model_path = model_info.get('path', self.model_path)
                
                # ğŸ¯ ç®€åŒ–ï¼šç›´æ¥åˆ›å»ºxDiT pipeline
                if not self._create_xfuser_pipeline_if_needed(model_path):
                    logger.warning(f"âš ï¸ Pipeline creation failed on GPU {self.gpu_id}")
                    return None
            else:
                # å•GPUæ¨¡å¼ï¼šç›´æ¥è¿è¡Œ
                logger.info(f"[GPU {self.gpu_id}] Running single-GPU inference")
            
            logger.info(f"Running inference on GPU {self.gpu_id}: {num_inference_steps} steps")
            start_time = time.time()
            
            # è®¾ç½®éšæœºç§å­
            torch.manual_seed(seed)
            torch.cuda.manual_seed(seed)
            
            # ç¡®ä¿æ•°æ®åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            latent_samples = latent_samples.to(self.device)
            
            # å¤„ç†conditioning
            prompt_embeds = None
            pooled_prompt_embeds = None
            if conditioning_positive and len(conditioning_positive) > 0:
                if isinstance(conditioning_positive[0], torch.Tensor):
                    prompt_embeds = conditioning_positive[0].to(self.device)
                    if len(conditioning_positive) > 1:
                        pooled_prompt_embeds = conditioning_positive[1].to(self.device)
            
            negative_prompt_embeds = None
            negative_pooled_prompt_embeds = None
            if conditioning_negative and len(conditioning_negative) > 0:
                if isinstance(conditioning_negative[0], torch.Tensor):
                    negative_prompt_embeds = conditioning_negative[0].to(self.device)
                    if len(conditioning_negative) > 1:
                        negative_pooled_prompt_embeds = conditioning_negative[1].to(self.device)
            
            # å¦‚æœæ˜¯å•GPUæˆ–åˆ†å¸ƒå¼å¤±è´¥ï¼Œè¿”å›Noneè§¦å‘fallback
            if self.world_size == 1 or not self.distributed_initialized:
                logger.info(f"[GPU {self.gpu_id}] Returning None to trigger ComfyUI fallback")
                return None
            
            # ğŸ”§ æ£€æŸ¥æ¨¡å‹åŒ…è£…å™¨çŠ¶æ€
            if self.model_wrapper is None:
                logger.warning(f"[GPU {self.gpu_id}] Model wrapper not ready, triggering fallback")
                return None
            
            # åˆ›å»ºæ¨ç†é…ç½®
            model_path = model_info.get('path', self.model_path)
            xfuser_args = xFuserArgs(
                model=model_path,
                height=latent_samples.shape[2] * 8,
                width=latent_samples.shape[3] * 8,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                output_type="latent",
                strategy=self.strategy,
                device=self.device,
                dtype="bfloat16"
            )
            
            engine_config = xfuser_args.create_config()
            logger.info(f"[GPU {self.gpu_id}] Config created: steps={num_inference_steps}, CFG={guidance_scale}")
            
            # è¿è¡Œæ¨ç†
            try:
                with torch.inference_mode():
                    result = self.model_wrapper(
                        prompt_embeds=prompt_embeds,
                        pooled_prompt_embeds=pooled_prompt_embeds,
                        negative_prompt_embeds=negative_prompt_embeds,
                        negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,
                        height=latent_samples.shape[2] * 8,
                        width=latent_samples.shape[3] * 8,
                        num_inference_steps=num_inference_steps,
                        guidance_scale=guidance_scale,
                        generator=torch.Generator(device=self.device).manual_seed(seed),
                        output_type="latent",
                        latents=latent_samples,
                        engine_config=engine_config
                    )
                
                # æå–ç»“æœ
                if hasattr(result, 'images'):
                    result_latents = result.images
                elif hasattr(result, 'latents'):
                    result_latents = result.latents
                else:
                    result_latents = result
                
                # ç¡®ä¿ç»“æœåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                result_latents = result_latents.to(self.device)
                
                end_time = time.time()
                logger.info(f"âœ… Inference completed on GPU {self.gpu_id} in {end_time - start_time:.2f}s")
                
                return result_latents
                
            except Exception as e:
                logger.error(f"Inference failed on GPU {self.gpu_id}: {e}")
                logger.exception("Inference traceback:")
                return None
                
        except Exception as e:
            logger.error(f"Error during inference on GPU {self.gpu_id}: {e}")
            logger.exception("Full traceback:")
            return None
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            # æ¸…ç†æ¨¡å‹
            if self.model_wrapper is not None:
                del self.model_wrapper
                self.model_wrapper = None
            
            # æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ
            self._cleanup_distributed()
            
            # æ¸…ç†CUDAç¼“å­˜
            torch.cuda.empty_cache()
            
            self.is_initialized = False
            logger.info(f"âœ… Worker on GPU {self.gpu_id} cleaned up")
            
        except Exception as e:
            logger.error(f"Error during cleanup on GPU {self.gpu_id}: {e}")
            logger.exception("Cleanup traceback:")

    def load_model(self, model_path: str, model_type: str = "flux"):
        """åŠ è½½æ¨¡å‹ï¼Œæ”¯æŒsafetensorså’Œdiffusersæ ¼å¼"""
        try:
            logger.info(f"[GPU {self.gpu_id}] Loading model: {model_path}")
            
            # åˆå§‹åŒ–engine_configï¼ˆå¦‚æœè¿˜æ²¡æœ‰çš„è¯ï¼‰
            if not hasattr(self, 'engine_config'):
                from xfuser import xFuserArgs
                xfuser_args = xFuserArgs(
                    model=model_path,
                    height=1024,
                    width=1024,
                    num_inference_steps=20,
                    guidance_scale=3.5,
                    output_type="latent",
                    tensor_parallel_degree=self.world_size,
                    use_ray=True,
                    ray_world_size=self.world_size
                )
                self.engine_config = xfuser_args.create_config()
            
            # æ£€æŸ¥è·¯å¾„æ ¼å¼
            if model_path.endswith('.safetensors'):
                logger.info(f"[GPU {self.gpu_id}] Detected safetensors format")
                
                # ğŸ¯ é‡è¦ï¼šå¯¹äºsafetensorsï¼Œæˆ‘ä»¬ä½¿ç”¨å»¶è¿ŸåŠ è½½ç­–ç•¥
                # ä¸åœ¨è¿™é‡Œé¢„åŠ è½½ä»»ä½•ç»„ä»¶ï¼Œç­‰å¾…ComfyUIç»„ä»¶ä¼ é€’
                logger.info(f"[GPU {self.gpu_id}] ğŸ’¡ Using deferred loading strategy for safetensors")
                logger.info(f"[GPU {self.gpu_id}] ğŸ¯ Will use ComfyUI components when available")
                logger.info(f"[GPU {self.gpu_id}] âš¡ No downloads needed - using existing ComfyUI components!")
                
                # æ ‡è®°ä¸ºå»¶è¿ŸåŠ è½½æ¨¡å¼
                self.model_wrapper = "deferred_loading"
                logger.info(f"[GPU {self.gpu_id}] âœ… Deferred loading mode enabled for safetensors")
                return "deferred_loading"
                        
            elif os.path.isdir(model_path):
                # Diffusersæ ¼å¼ç›®å½• - ç›´æ¥ä½¿ç”¨åŸæœ‰é€»è¾‘
                logger.info(f"[GPU {self.gpu_id}] Detected diffusers format")
                
                from xfuser import xFuserFluxPipeline
                from diffusers import FluxPipeline
                import torch
                
                pipeline = FluxPipeline.from_pretrained(
                    model_path,
                    torch_dtype=torch.bfloat16,
                    device_map=None,
                    low_cpu_mem_usage=True
                )
                
                self.model_wrapper = xFuserFluxPipeline(pipeline, self.engine_config)
                logger.info(f"[GPU {self.gpu_id}] âœ… Diffusers model loaded successfully")
                
                return "success"
            else:
                logger.warning(f"[GPU {self.gpu_id}] Unknown model format: {model_path}")
                self.model_wrapper = "deferred_loading"
                return "deferred_loading"
                
        except Exception as e:
            logger.error(f"[GPU {self.gpu_id}] Model loading failed: {e}")
            logger.exception("Full traceback:")
            
            # æœ€ç»ˆfallback
            self.model_wrapper = "deferred_loading"
            return "deferred_loading"

# Non-Ray fallback worker for when Ray is not available
class XDiTWorkerFallback:
    """Fallback worker when Ray is not available"""
    
    def __init__(self, gpu_id: int, model_path: str, strategy: str = "Hybrid"):
        self.gpu_id = gpu_id
        self.model_path = model_path
        self.strategy = strategy
        self.model_wrapper = None
        self.is_initialized = False
        
        logger.info(f"Initializing fallback worker on GPU {gpu_id}")
    
    def initialize(self) -> bool:
        """Initialize the fallback worker"""
        try:
            # è®¾ç½®GPUè®¾å¤‡
            if torch.cuda.is_available():
                torch.cuda.set_device(self.gpu_id)
            
            logger.info(f"Fallback worker on GPU {self.gpu_id} ready")
            
            self.is_initialized = True
            logger.info(f"âœ… Fallback worker on GPU {self.gpu_id} initialized successfully")
            return True
            
        except Exception as e:
            logger.error(f"Failed to initialize fallback worker on GPU {self.gpu_id}: {e}")
            logger.exception("Full traceback:")
            return False
    
    def get_gpu_info(self) -> Dict[str, Any]:
        """Get GPU information"""
        try:
            if not torch.cuda.is_available():
                return {"gpu_id": self.gpu_id, "error": "CUDA not available"}
                
            props = torch.cuda.get_device_properties(self.gpu_id)
            memory_total = props.total_memory / (1024**3)
            memory_reserved = torch.cuda.memory_reserved(self.gpu_id) / (1024**3)
            memory_allocated = torch.cuda.memory_allocated(self.gpu_id) / (1024**3)
            
            return {
                "gpu_id": self.gpu_id,
                "name": props.name,
                "memory_total_gb": memory_total,
                "memory_reserved_gb": memory_reserved,
                "memory_allocated_gb": memory_allocated,
                "memory_free_gb": memory_total - memory_reserved,
                "compute_capability": f"{props.major}.{props.minor}",
                "is_initialized": self.is_initialized
            }
        except Exception as e:
            logger.error(f"Error getting GPU info for GPU {self.gpu_id}: {e}")
            return {"gpu_id": self.gpu_id, "error": str(e)}
    
    def wrap_model_for_xdit(self, model_state_dict: Dict, model_config: Dict) -> bool:
        """Wrap model with xDiT (fallback version)"""
        try:
            logger.info(f"Wrapping model with fallback method on GPU {self.gpu_id}")
            
            # åœ¨æ²¡æœ‰xDiTçš„æƒ…å†µä¸‹ï¼Œç®€å•åœ°å‡†å¤‡å¥½æ¥å—æ¨¡å‹
            self.model_wrapper = "fallback_placeholder"
            
            logger.info(f"âœ… Model ready on GPU {self.gpu_id} (fallback mode)")
            return True
            
        except Exception as e:
            logger.error(f"Failed to wrap model on GPU {self.gpu_id}: {e}")
            return False
    
    def run_inference(self, 
                     model_info: Dict,  # æ”¹ä¸ºè½»é‡çº§çš„æ¨¡å‹ä¿¡æ¯
                     conditioning_positive: Any,
                     conditioning_negative: Any,
                     latent_samples: torch.Tensor,
                     num_inference_steps: int = 20,
                     guidance_scale: float = 8.0,
                     seed: int = 42) -> Optional[torch.Tensor]:
        """Run inference (fallback version)"""
        try:
            if not self.is_initialized:
                logger.error(f"Fallback worker on GPU {self.gpu_id} not initialized")
                return None
            
            logger.info(f"Running fallback inference on GPU {self.gpu_id}")
            logger.info(f"Model info: {model_info.get('type', 'unknown')}, Latent shape: {latent_samples.shape}")
            start_time = time.time()
            
            # è®¾ç½®éšæœºç§å­
            torch.manual_seed(seed)
            if torch.cuda.is_available():
                torch.cuda.manual_seed(seed)
            
            # ğŸ”§ ä¼˜åŒ–ï¼šç¡®ä¿æ•°æ®åœ¨æ­£ç¡®çš„GPUä¸Š
            device = f"cuda:{self.gpu_id}" if torch.cuda.is_available() else "cpu"
            if latent_samples.device != torch.device(device):
                latent_samples = latent_samples.to(device)
            
            # ğŸ”§ ä¿®å¤Fluxæ¨¡å‹é€šé“æ•°é—®é¢˜
            # Fluxæ¨¡å‹ä½¿ç”¨16é€šé“latent spaceï¼Œå¦‚æœè¾“å…¥æ˜¯4é€šé“ï¼Œéœ€è¦è¿›è¡Œè½¬æ¢
            if latent_samples.shape[1] == 4 and model_info.get('type', '').lower() == 'flux':
                logger.info(f"Converting 4-channel to 16-channel latent for Flux model")
                # ç®€å•çš„é€šé“æ‰©å±•ç­–ç•¥ï¼ˆå®é™…çš„xDiTä¼šæœ‰æ›´å¤æ‚çš„å¤„ç†ï¼‰
                # æ–¹æ³•1ï¼šé‡å¤é€šé“ [1,4,H,W] -> [1,16,H,W]
                expanded_latents = latent_samples.repeat(1, 4, 1, 1)
                # æ–¹æ³•2ï¼šä¹Ÿå¯ä»¥ç”¨é›¶å¡«å……
                # expanded_latents = torch.cat([latent_samples, torch.zeros_like(latent_samples).repeat(1, 3, 1, 1)], dim=1)
                latent_samples = expanded_latents
                logger.info(f"Latent shape converted to: {latent_samples.shape}")
            
            # åœ¨fallbackæ¨¡å¼ä¸‹ï¼Œåªæ˜¯è¿”å›åŸå§‹latents
            # å®é™…ä½¿ç”¨ä¸­ä¼šå›é€€åˆ°ComfyUIçš„æ ‡å‡†é‡‡æ ·
            # result_latents = latent_samples.clone()
            
            # TODO: fallbackæ¨¡å¼ä¹Ÿè¿”å›Noneï¼Œè®©ç³»ç»Ÿä½¿ç”¨æ ‡å‡†ComfyUIé‡‡æ ·
            logger.info(f"âš ï¸ Fallback worker returning None to trigger standard ComfyUI sampling")
            return None
            
            end_time = time.time()
            logger.info(f"âœ… Fallback inference completed on GPU {self.gpu_id} in {end_time - start_time:.2f}s")
            
            # return result_latents
                
        except Exception as e:
            logger.error(f"Error during fallback inference on GPU {self.gpu_id}: {e}")
            logger.exception("Full traceback:")
            return None
    
    def cleanup(self):
        """Clean up resources"""
        try:
            if self.model_wrapper is not None:
                del self.model_wrapper
                self.model_wrapper = None
            
            self.is_initialized = False
            
            # Clear CUDA cache
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
            logger.info(f"âœ… Fallback worker on GPU {self.gpu_id} cleaned up")
            
        except Exception as e:
            logger.error(f"Error during cleanup on GPU {self.gpu_id}: {e}") 