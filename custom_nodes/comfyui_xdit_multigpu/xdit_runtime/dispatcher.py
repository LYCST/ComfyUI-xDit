"""
xDiT Dispatcher
==============

Worker scheduling strategies for multi-GPU inference.
"""

import time
import logging
import threading
import os
from typing import Dict, Any, Optional, List, Tuple
from enum import Enum
import torch

# Try to import Ray
try:
    import ray
    RAY_AVAILABLE = True
except ImportError:
    RAY_AVAILABLE = False

from .worker import XDiTWorker, XDiTWorkerFallback, find_free_port
from .ray_manager import initialize_ray, is_ray_available
from comfyui_model_wrapper import ComfyUIModelWrapper

logger = logging.getLogger(__name__)

class SchedulingStrategy(Enum):
    """Scheduling strategies"""
    ROUND_ROBIN = "round_robin"
    LEAST_LOADED = "least_loaded"
    WEIGHTED_ROUND_ROBIN = "weighted_round_robin"
    ADAPTIVE = "adaptive"

class XDiTDispatcher:
    """
    Dispatcher for managing multiple xDiT workers.
    Supports different scheduling strategies.
    """
    
    def __init__(self, gpu_devices: List[int], model_path: str, strategy: str = "Hybrid", 
                 scheduling_strategy: SchedulingStrategy = SchedulingStrategy.ROUND_ROBIN):
        self.gpu_devices = gpu_devices
        self.model_path = model_path
        self.strategy = strategy
        self.scheduling_strategy = scheduling_strategy
        self.workers = []
        self.current_worker_index = 0
        self.worker_loads = {}
        self.lock = threading.Lock()
        self.is_initialized = False
        self.model_wrapper = ComfyUIModelWrapper(model_path)  # ‰ΩøÁî®Ëá™ÂÆö‰πâÊ®°ÂûãÂä†ËΩΩÂô®
        self.pipeline = None
        
        # ÂàÜÂ∏ÉÂºèÈÖçÁΩÆ
        self.master_addr = "127.0.0.1"
        self.master_port = find_free_port()
        self.world_size = len(gpu_devices)
        
        logger.info(f"Initializing XDiT Dispatcher with {len(gpu_devices)} GPUs")
        logger.info(f"Scheduling strategy: {scheduling_strategy.value}")
        logger.info(f"Distributed config: {self.master_addr}:{self.master_port}, world_size={self.world_size}")

        # ÂàùÂßãÂåñ ComfyUI Ê®°ÂûãÂåÖË£ÖÂô®
        if not self.model_wrapper.load_components():
            logger.error(f"Failed to initialize model wrapper for {model_path}")
            return

        self.pipeline = self.model_wrapper.get_pipeline()  # Ëé∑ÂèñÁªÑÂêàÂêéÁöÑ pipeline

        logger.info(f"Pipeline ready: {self.pipeline}")
    
    # def initialize(self) -> bool:
    #     """Initialize all workers with coordinated distributed setup"""
    #     try:
    #         if RAY_AVAILABLE:
    #             # Initialize Ray if not already done
    #             if not is_ray_available():
    #                 success = initialize_ray()
    #                 if not success:
    #                     logger.error("Failed to initialize Ray")
    #                     return False
    #                 logger.info("Ray initialized successfully")
                
    #             # Create Ray workers with proper distributed parameters
    #             worker_init_futures = []
                
    #             for i, gpu_id in enumerate(self.gpu_devices):
    #                 worker = XDiTWorker.remote(
    #                     gpu_id=gpu_id, 
    #                     model_path=self.model_path, 
    #                     strategy=self.strategy,
    #                     master_addr=self.master_addr,
    #                     master_port=self.master_port,
    #                     world_size=self.world_size,
    #                     rank=i  # ‰ΩøÁî®Á¥¢Âºï‰Ωú‰∏∫rank
    #                 )
    #                 self.workers[gpu_id] = worker
    #                 self.worker_loads[gpu_id] = 0
                    
    #                 # Initialize worker (basic setup)
    #                 future = worker.initialize.remote()
    #                 worker_init_futures.append((gpu_id, future))
                
    #             # Wait for all workers to complete basic initialization
    #             logger.info("Waiting for all workers to complete basic initialization...")
    #             for gpu_id, future in worker_init_futures:
    #                 try:
    #                     success = ray.get(future, timeout=60)
    #                     if not success:
    #                         logger.error(f"Failed to initialize Ray worker on GPU {gpu_id}")
    #                         return False
    #                     logger.info(f"‚úÖ Ray worker initialized on GPU {gpu_id}")
    #                 except Exception as e:
    #                     logger.error(f"Worker initialization failed on GPU {gpu_id}: {e}")
    #                     return False
                
    #             # Now initialize distributed environment for multi-GPU
    #             if self.world_size > 1:
    #                 logger.info("Initializing distributed environment for multi-GPU...")
    #                 distributed_futures = []
                    
    #                 for gpu_id, worker in self.workers.items():
    #                     future = worker.initialize_distributed.remote()
    #                     distributed_futures.append((gpu_id, future))
                    
    #                 # Wait for distributed initialization
    #                 distributed_success = True
    #                 for gpu_id, future in distributed_futures:
    #                     try:
    #                         success = ray.get(future, timeout=300)  # 5ÂàÜÈíüË∂ÖÊó∂
    #                         if not success:
    #                             logger.error(f"Failed to initialize distributed on GPU {gpu_id}")
    #                             distributed_success = False
    #                         else:
    #                             logger.info(f"‚úÖ Distributed initialized on GPU {gpu_id}")
    #                     except Exception as e:
    #                         logger.error(f"Distributed initialization failed on GPU {gpu_id}: {e}")
    #                         distributed_success = False
                    
    #                 if not distributed_success:
    #                     logger.warning("Some workers failed distributed initialization, falling back to single-GPU mode")
    #                     # ‰∏çËøîÂõûFalseÔºåËÄåÊòØÁªßÁª≠ÔºåËÆ©Á≥ªÁªüÂõûÈÄÄÂà∞ÂçïGPU
    #             else:
    #                 logger.info("Single GPU mode, skipping distributed initialization")
                    
    #         else:
    #             # Use fallback workers
    #             logger.warning("Ray not available, using fallback workers")
    #             for gpu_id in self.gpu_devices:
    #                 worker = XDiTWorkerFallback(gpu_id, self.model_path, self.strategy)
    #                 self.workers[gpu_id] = worker
    #                 self.worker_loads[gpu_id] = 0
                    
    #                 # Initialize worker
    #                 success = worker.initialize()
    #                 if not success:
    #                     logger.error(f"Failed to initialize fallback worker on GPU {gpu_id}")
    #                     return False
                    
    #                 logger.info(f"‚úÖ Fallback worker initialized on GPU {gpu_id}")
            
    #         self.is_initialized = True
    #         logger.info(f"‚úÖ XDiT Dispatcher initialized with {len(self.workers)} workers")
    #         return True
            
    #     except Exception as e:
    #         logger.error(f"Failed to initialize XDiT Dispatcher: {e}")
    #         logger.exception("Dispatcher initialization traceback:")
    #         return False
    
    def initialize(self) -> bool:
        """ÊîπËøõÁöÑÂàùÂßãÂåñÊñπÊ≥ï"""
        try:
            logger.info("=" * 60)
            logger.info("üöÄ ÂºÄÂßãÂàùÂßãÂåñXDiT Dispatcher")
            logger.info(f"  ‚Ä¢ GPUËÆæÂ§á: {self.gpu_devices}")
            logger.info(f"  ‚Ä¢ Âπ∂Ë°åÁ≠ñÁï•: {self.strategy}")
            logger.info(f"  ‚Ä¢ Ë∞ÉÂ∫¶Á≠ñÁï•: {self.scheduling_strategy.value}")
            logger.info(f"  ‚Ä¢ Ê®°ÂûãË∑ØÂæÑ: {self.model_path}")
            logger.info(f"  ‚Ä¢ World size: {self.world_size}")
            logger.info("=" * 60)
            
            # Ê£ÄÊü•GPUÂèØÁî®ÊÄß
            import torch
            if not torch.cuda.is_available():
                logger.error("‚ùå CUDA‰∏çÂèØÁî®")
                return False
            
            available_gpus = torch.cuda.device_count()
            logger.info(f"üìä Ê£ÄÊµãÂà∞{available_gpus}‰∏™GPU")
            
            for gpu_id in self.gpu_devices:
                if gpu_id >= available_gpus:
                    logger.error(f"‚ùå GPU {gpu_id}‰∏çÂ≠òÂú®ÔºàÂè™Êúâ{available_gpus}‰∏™GPUÂèØÁî®Ôºâ")
                    return False
                
                # Ê£ÄÊü•GPUÂÜÖÂ≠ò
                try:
                    torch.cuda.set_device(gpu_id)
                    memory_total = torch.cuda.get_device_properties(gpu_id).total_memory / 1024**3
                    memory_free = (torch.cuda.get_device_properties(gpu_id).total_memory - torch.cuda.memory_allocated(gpu_id)) / 1024**3
                    logger.info(f"  ‚Ä¢ GPU {gpu_id}: {memory_free:.1f}GB ÂèØÁî® / {memory_total:.1f}GB ÊÄªËÆ°")
                except Exception as e:
                    logger.warning(f"  ‚Ä¢ GPU {gpu_id}: Êó†Ê≥ïËé∑ÂèñÂÜÖÂ≠ò‰ø°ÊÅØ - {e}")
            
            # Ê£ÄÊü•Ê®°ÂûãÊñá‰ª∂
            if not os.path.exists(self.model_path):
                logger.error(f"‚ùå Ê®°ÂûãÊñá‰ª∂‰∏çÂ≠òÂú®: {self.model_path}")
                return False
            
            model_size = os.path.getsize(self.model_path) / 1024**3
            logger.info(f"üìÅ Ê®°ÂûãÊñá‰ª∂: {model_size:.1f}GB")
            if not RAY_AVAILABLE:
                logger.warning("Ray‰∏çÂèØÁî®Ôºå‰ΩøÁî®fallbackÊ®°Âºè")
                return self._initialize_fallback()
            
            # 1. È¶ñÂÖàÂàùÂßãÂåñRayÔºàÂ¶ÇÊûúÈúÄË¶ÅÔºâ
            if not is_ray_available():
                logger.info("ÂàùÂßãÂåñRayÈõÜÁæ§...")
                # ‰∏∫Â§öGPU‰ºòÂåñRayÈÖçÁΩÆ
                success = initialize_ray(
                    num_gpus=len(self.gpu_devices),
                    object_store_memory_gb=min(64, len(self.gpu_devices) * 8),  # ÊØèGPU 8GB object store
                    dashboard_port=None  # Á¶ÅÁî®dashboardËäÇÁúÅÂÜÖÂ≠ò
                )
                if not success:
                    logger.error("‚ùå RayÂàùÂßãÂåñÂ§±Ë¥•")
                    return False
                logger.info("‚úÖ RayÂàùÂßãÂåñÊàêÂäü")
            else:
                logger.info("‚úÖ RayÂ∑≤ÁªèËøêË°å")

            # ÊòæÁ§∫RayÁä∂ÊÄÅ
            ray_info = get_ray_info()
            logger.info(f"üìä RayÁä∂ÊÄÅ: {ray_info}")

            # 2. ÂàÜÈò∂ÊÆµÂàõÂª∫workers
            logger.info(f"ÂàõÂª∫{len(self.gpu_devices)}‰∏™GPU workers...")
            
            # Èò∂ÊÆµ1: ÂàõÂª∫ÊâÄÊúâRay actors
            worker_actors = {}
            for i, gpu_id in enumerate(self.gpu_devices):
                try:
                    # ‰ΩøÁî®Âä®ÊÄÅÁ´ØÂè£ÈÅøÂÖçÂÜ≤Á™Å
                    master_port = find_free_port() if i == 0 else self.master_port
                    if i == 0:
                        self.master_port = master_port
                    
                    worker = XDiTWorker.remote(
                        gpu_id=gpu_id,
                        model_path=self.model_path,
                        strategy=self.strategy,
                        master_addr=self.master_addr,
                        master_port=self.master_port,
                        world_size=self.world_size,
                        rank=i
                    )
                    worker_actors[gpu_id] = worker
                    logger.info(f"‚úÖ Created worker actor for GPU {gpu_id}")
                    
                except Exception as e:
                    logger.error(f"‚ùå Failed to create worker for GPU {gpu_id}: {e}")
                    return False
            
            # Èò∂ÊÆµ2: Âü∫Á°ÄÂàùÂßãÂåñ
            logger.info("ÊâßË°åworkersÂü∫Á°ÄÂàùÂßãÂåñ...")
            init_futures = []
            for gpu_id, worker in worker_actors.items():
                future = worker.initialize.remote()
                init_futures.append((gpu_id, future))
            
            # Á≠âÂæÖÂü∫Á°ÄÂàùÂßãÂåñÂÆåÊàê
            for gpu_id, future in init_futures:
                try:
                    success = ray.get(future, timeout=60)
                    if not success:
                        logger.error(f"Worker {gpu_id} Âü∫Á°ÄÂàùÂßãÂåñÂ§±Ë¥•")
                        return False
                    logger.info(f"‚úÖ Worker {gpu_id} Âü∫Á°ÄÂàùÂßãÂåñÂÆåÊàê")
                except Exception as e:
                    logger.error(f"Worker {gpu_id} ÂàùÂßãÂåñÂºÇÂ∏∏: {e}")
                    return False
            
            # Èò∂ÊÆµ3: ÂàÜÂ∏ÉÂºèÁéØÂ¢ÉÂàùÂßãÂåñÔºà‰ªÖÂ§öGPUÔºâ
            if self.world_size > 1:
                logger.info("ÂàùÂßãÂåñÂàÜÂ∏ÉÂºèÁéØÂ¢É...")
                success = self._initialize_distributed(worker_actors)
                if not success:
                    logger.warning("ÂàÜÂ∏ÉÂºèÂàùÂßãÂåñÂ§±Ë¥•ÔºåÈôçÁ∫ß‰∏∫ÂçïGPUÊ®°Âºè")
                    # ‰∏çËøîÂõûFalseÔºåËÄåÊòØÁªßÁª≠‰ΩøÁî®ÂçïGPUÊ®°Âºè
                    self.world_size = 1
            
            # ‰øùÂ≠òworkers
            self.workers = worker_actors
            for gpu_id in self.gpu_devices:
                self.worker_loads[gpu_id] = 0
            
            self.is_initialized = True
            logger.info(f"‚úÖ DispatcherÂàùÂßãÂåñÂÆåÊàêÔºå{len(self.workers)}‰∏™workersÂ∞±Áª™")
            return True   
            
        except Exception as e:
            logger.error(f"DispatcherÂàùÂßãÂåñÂ§±Ë¥•: {e}")
            logger.exception("ÂàùÂßãÂåñÈîôËØØ:")
            return self._initialize_fallback()
    
    def _initialize_distributed(self, worker_actors) -> bool:
        """ÂàÜÂ∏ÉÂºèÁéØÂ¢ÉÂàùÂßãÂåñÔºåÂ∏¶ÈáçËØïÊú∫Âà∂"""
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                logger.info(f"ÂàÜÂ∏ÉÂºèÂàùÂßãÂåñÂ∞ùËØï {attempt + 1}/{max_retries}")
                
                # Á°Æ‰øùÊâÄÊúâworkers‰ΩøÁî®Áõ∏ÂêåÁöÑmasterÈÖçÁΩÆ
                logger.info(f"‰ΩøÁî®masterÈÖçÁΩÆ: {self.master_addr}:{self.master_port}")
                
                # ÂêØÂä®ÂàÜÂ∏ÉÂºèÂàùÂßãÂåñ
                dist_futures = []
                for gpu_id, worker in worker_actors.items():
                    future = worker.initialize_distributed.remote()
                    dist_futures.append((gpu_id, future))
                
                # Á≠âÂæÖÂàÜÂ∏ÉÂºèÂàùÂßãÂåñÔºåÂª∂ÈïøË∂ÖÊó∂Êó∂Èó¥
                timeout = 300 + attempt * 60  # ÈÄêÊ¨°Â¢ûÂä†Ë∂ÖÊó∂Êó∂Èó¥
                success_count = 0
                
                for gpu_id, future in dist_futures:
                    try:
                        success = ray.get(future, timeout=timeout)
                        if success:
                            success_count += 1
                            logger.info(f"‚úÖ GPU {gpu_id} ÂàÜÂ∏ÉÂºèÂàùÂßãÂåñÊàêÂäü")
                        else:
                            logger.error(f"‚ùå GPU {gpu_id} ÂàÜÂ∏ÉÂºèÂàùÂßãÂåñÂ§±Ë¥•")
                    except Exception as e:
                        logger.error(f"‚ùå GPU {gpu_id} ÂàÜÂ∏ÉÂºèÂàùÂßãÂåñÂºÇÂ∏∏: {e}")
                
                # Ê£ÄÊü•ÊàêÂäüÁéá
                if success_count == len(worker_actors):
                    logger.info("‚úÖ ÊâÄÊúâworkersÂàÜÂ∏ÉÂºèÂàùÂßãÂåñÊàêÂäü")
                    return True
                elif success_count >= len(worker_actors) // 2:
                    logger.warning(f"‚ö†Ô∏è ÈÉ®ÂàÜworkersÂàùÂßãÂåñÊàêÂäü ({success_count}/{len(worker_actors)})")
                    return True
                else:
                    logger.error(f"‚ùå Â§ßÈÉ®ÂàÜworkersÂàùÂßãÂåñÂ§±Ë¥• ({success_count}/{len(worker_actors)})")
                    if attempt < max_retries - 1:
                        logger.info("Á≠âÂæÖÈáçËØï...")
                        time.sleep(5)
                        continue
                    else:
                        return False
                        
            except Exception as e:
                logger.error(f"ÂàÜÂ∏ÉÂºèÂàùÂßãÂåñÂ∞ùËØï{attempt + 1}Â§±Ë¥•: {e}")
                if attempt == max_retries - 1:
                    return False
                time.sleep(5)
        
        return False
    
    def _initialize_fallback(self) -> bool:
        """FallbackÂàùÂßãÂåñÊ®°Âºè"""
        try:
            logger.info("‰ΩøÁî®fallbackÊ®°ÂºèÂàùÂßãÂåñ")
            
            # Âè™‰ΩøÁî®Á¨¨‰∏Ä‰∏™GPU
            primary_gpu = self.gpu_devices[0] if self.gpu_devices else 0
            
            worker = XDiTWorkerFallback(primary_gpu, self.model_path, self.strategy)
            success = worker.initialize()
            
            if success:
                self.workers = {primary_gpu: worker}
                self.worker_loads = {primary_gpu: 0}
                self.is_initialized = True
                self.world_size = 1  # Âº∫Âà∂ÂçïGPUÊ®°Âºè
                
                logger.info(f"‚úÖ FallbackÊ®°ÂºèÂàùÂßãÂåñÊàêÂäüÔºå‰ΩøÁî®GPU {primary_gpu}")
                return True
            else:
                logger.error("‚ùå FallbackÊ®°ÂºèÂàùÂßãÂåñ‰πüÂ§±Ë¥•")
                return False
                
        except Exception as e:
            logger.error(f"FallbackÂàùÂßãÂåñÂ§±Ë¥•: {e}")
            return False
    
    def run_inference(self, model_info, conditioning_positive, conditioning_negative, 
                     latent_samples, num_inference_steps=20, guidance_scale=8.0, seed=42, 
                     comfyui_vae=None, comfyui_clip=None) -> Optional[torch.Tensor]:
        """ÊîπËøõÁöÑÊé®ÁêÜÊñπÊ≥ï"""
        try:
            if not self.is_initialized or not self.workers:
                logger.error("DispatcherÊú™ÂàùÂßãÂåñ")
                return None
            
            # Êõ¥Êñ∞model_info‰ª•ÂåÖÂê´ComfyUIÁªÑ‰ª∂
            enhanced_model_info = model_info.copy()
            enhanced_model_info.update({
                'vae': comfyui_vae,
                'clip': comfyui_clip,
                'comfyui_mode': True
            })
            
            logger.info(f"üéØ ËøêË°åÊé®ÁêÜ: {num_inference_steps}Ê≠•, CFG={guidance_scale}")
            logger.info(f"  ‚Ä¢ Workers: {len(self.workers)}")
            logger.info(f"  ‚Ä¢ VAE: {'‚úÖ' if comfyui_vae else '‚ùå'}")
            logger.info(f"  ‚Ä¢ CLIP: {'‚úÖ' if comfyui_clip else '‚ùå'}")
            
            # ÈÄâÊã©worker
            worker = self.get_next_worker()
            if worker is None:
                logger.error("Ê≤°ÊúâÂèØÁî®ÁöÑworker")
                return None
            
            # ÊâßË°åÊé®ÁêÜ
            if RAY_AVAILABLE and not isinstance(worker, XDiTWorkerFallback):
                # RayÊ®°Âºè
                future = worker.run_inference.remote(
                    model_info=enhanced_model_info,
                    conditioning_positive=conditioning_positive,
                    conditioning_negative=conditioning_negative,
                    latent_samples=latent_samples,
                    num_inference_steps=num_inference_steps,
                    guidance_scale=guidance_scale,
                    seed=seed
                )
                
                # Á≠âÂæÖÁªìÊûúÔºåÂ∏¶Ë∂ÖÊó∂
                timeout = min(600, num_inference_steps * 15)  # ÊúÄÂ§ö10ÂàÜÈíü
                try:
                    result = ray.get(future, timeout=timeout)
                    if result is not None:
                        logger.info("‚úÖ RayÊé®ÁêÜÂÆåÊàê")
                        return result
                    else:
                        logger.warning("‚ö†Ô∏è RayÊé®ÁêÜËøîÂõûNone")
                        return None
                except ray.exceptions.GetTimeoutError:
                    logger.error(f"‚è∞ RayÊé®ÁêÜË∂ÖÊó∂ ({timeout}s)")
                    return None
            else:
                # FallbackÊ®°Âºè
                result = worker.run_inference(
                    model_info=enhanced_model_info,
                    conditioning_positive=conditioning_positive,
                    conditioning_negative=conditioning_negative,
                    latent_samples=latent_samples,
                    num_inference_steps=num_inference_steps,
                    guidance_scale=guidance_scale,
                    seed=seed
                )
                
                if result is not None:
                    logger.info("‚úÖ FallbackÊé®ÁêÜÂÆåÊàê")
                    return result
                else:
                    logger.warning("‚ö†Ô∏è FallbackÊé®ÁêÜËøîÂõûNone")
                    return None
                    
        except Exception as e:
            logger.error(f"Êé®ÁêÜÊâßË°åÂ§±Ë¥•: {e}")
            logger.exception("Êé®ÁêÜÈîôËØØ:")
            return None 

    def initialize(self):
        """Initialize workers"""
        for gpu_id in self.gpu_devices:
            # ÂàùÂßãÂåñÊØè‰∏™ workerÔºå‰º†ÂÖ•ÂÆåÊï¥ÁöÑ pipeline
            worker = XDiTWorker(gpu_id, self.pipeline)
            self.workers.append(worker)
        logger.info(f"Initialized {len(self.workers)} workers")
        
    def get_next_worker(self) -> Optional[Any]:
        """Get next worker based on scheduling strategy"""
        if not self.is_initialized or not self.workers:
            return None
        
        with self.lock:
            if self.scheduling_strategy == SchedulingStrategy.ROUND_ROBIN:
                return self._round_robin_schedule()
            elif self.scheduling_strategy == SchedulingStrategy.LEAST_LOADED:
                return self._least_loaded_schedule()
            elif self.scheduling_strategy == SchedulingStrategy.WEIGHTED_ROUND_ROBIN:
                return self._weighted_round_robin_schedule()
            elif self.scheduling_strategy == SchedulingStrategy.ADAPTIVE:
                return self._adaptive_schedule()
            else:
                # Default to round robin
                return self._round_robin_schedule()
    
    def _round_robin_schedule(self) -> Optional[Any]:
        """Round robin scheduling"""
        worker_ids = list(self.workers.keys())
        if not worker_ids:
            return None
        
        worker_id = worker_ids[self.current_worker_index % len(worker_ids)]
        self.current_worker_index += 1
        
        worker = self.workers[worker_id]
        self.worker_loads[worker_id] += 1
        
        logger.debug(f"Round robin: assigned to GPU {worker_id}")
        return worker
    
    def _least_loaded_schedule(self) -> Optional[Any]:
        """Least loaded scheduling"""
        if not self.worker_loads:
            return None
        
        # Find worker with minimum load
        min_load = min(self.worker_loads.values())
        candidates = [gpu_id for gpu_id, load in self.worker_loads.items() if load == min_load]
        
        # If multiple candidates, choose the first one
        worker_id = candidates[0]
        worker = self.workers[worker_id]
        self.worker_loads[worker_id] += 1
        
        logger.debug(f"Least loaded: assigned to GPU {worker_id} (load: {min_load})")
        return worker
    
    def _weighted_round_robin_schedule(self) -> Optional[Any]:
        """Weighted round robin scheduling based on GPU memory"""
        worker_ids = list(self.workers.keys())
        if not worker_ids:
            return None
        
        # Get GPU memory info to determine weights
        weights = []
        for gpu_id in worker_ids:
            try:
                if RAY_AVAILABLE:
                    gpu_info = ray.get(self.workers[gpu_id].get_gpu_info.remote())
                else:
                    gpu_info = self.workers[gpu_id].get_gpu_info()
                
                # Weight based on available memory
                memory_weight = gpu_info.get('memory_free_gb', 0) / gpu_info.get('memory_total_gb', 1)
                weights.append(memory_weight)
            except Exception as e:
                logger.warning(f"Error getting GPU info for {gpu_id}: {e}")
                weights.append(1.0)  # Default weight
        
        # Normalize weights
        total_weight = sum(weights)
        if total_weight > 0:
            weights = [w / total_weight for w in weights]
        else:
            weights = [1.0 / len(worker_ids)] * len(worker_ids)
        
        # Choose worker based on weights
        import random
        worker_id = random.choices(worker_ids, weights=weights)[0]
        worker = self.workers[worker_id]
        self.worker_loads[worker_id] += 1
        
        logger.debug(f"Weighted round robin: assigned to GPU {worker_id}")
        return worker
    
    def _adaptive_schedule(self) -> Optional[Any]:
        """Adaptive scheduling based on current load and performance"""
        if not self.worker_loads:
            return None
        
        # Get current GPU info for all workers
        gpu_infos = {}
        for gpu_id, worker in self.workers.items():
            try:
                if RAY_AVAILABLE:
                    gpu_info = ray.get(worker.get_gpu_info.remote())
                else:
                    gpu_info = worker.get_gpu_info()
                gpu_infos[gpu_id] = gpu_info
            except Exception as e:
                logger.warning(f"Error getting GPU info for {gpu_id}: {e}")
                continue
        
        # Calculate adaptive score for each worker
        scores = {}
        for gpu_id, gpu_info in gpu_infos.items():
            if 'error' in gpu_info:
                continue
            
            # Score based on multiple factors
            memory_score = gpu_info.get('memory_free_gb', 0) / gpu_info.get('memory_total_gb', 1)
            load_score = 1.0 / (self.worker_loads.get(gpu_id, 0) + 1)  # Lower load = higher score
            utilization_score = 1.0 - (gpu_info.get('memory_allocated_gb', 0) / gpu_info.get('memory_total_gb', 1))
            
            # Combined score
            total_score = memory_score * 0.4 + load_score * 0.3 + utilization_score * 0.3
            scores[gpu_id] = total_score
        
        if not scores:
            return self._round_robin_schedule()
        
        # Choose worker with highest score
        best_gpu_id = max(scores.keys(), key=lambda x: scores[x])
        worker = self.workers[best_gpu_id]
        self.worker_loads[best_gpu_id] += 1
        
        logger.debug(f"Adaptive: assigned to GPU {best_gpu_id} (score: {scores[best_gpu_id]:.3f})")
        return worker
    
    def _validate_model_path(self, model_path: str) -> bool:
        """È™åËØÅÊ®°ÂûãË∑ØÂæÑÊòØÂê¶ÊúâÊïà"""
        try:
            if model_path.endswith('.safetensors'):
                return os.path.exists(model_path)
            elif os.path.isdir(model_path):
                model_index_path = os.path.join(model_path, "model_index.json")
                return os.path.exists(model_index_path)
            else:
                return False
        except Exception as e:
            logger.error(f"Error validating model path {model_path}: {e}")
            return False

    def load_model_distributed(self, model_path: str, model_type: str = "flux"):
        """ÂàÜÂ∏ÉÂºèÂä†ËΩΩÊ®°Âûã"""
        try:
            logger.info(f"üöÄ Starting distributed model loading: {model_path}")
            
            # È™åËØÅÊ®°ÂûãË∑ØÂæÑ
            if not self._validate_model_path(model_path):
                raise ValueError(f"Invalid model path: {model_path}")
            
            # Ê£ÄÊü•Ê®°ÂûãÊ†ºÂºè
            if model_path.endswith('.safetensors'):
                logger.info("üí° Safetensors format detected - using ComfyUI component reuse strategy")
                logger.info("‚ö° No downloads needed! Will use ComfyUI loaded VAE/CLIP components")
                logger.info("üéØ This should complete in seconds, not minutes")
            else:
                logger.info("üì¶ Diffusers format detected - loading complete pipeline")
            
            # ‰ΩøÁî®Êñ∞ÁöÑload_modelÊñπÊ≥ï
            futures = []
            for worker in self.workers.values():
                future = worker.load_model.remote(model_path, model_type)
                futures.append(future)
            
            logger.info("‚è≥ Initializing workers with intelligent component reuse...")
            
            # Á≠âÂæÖÊâÄÊúâworkerÂÆåÊàêÂä†ËΩΩ - ÂØπ‰∫ésafetensorsÂ∫îËØ•ÂæàÂø´
            timeout = 300 if model_path.endswith('.safetensors') else 1800  # safetensors: 5ÂàÜÈíü, diffusers: 30ÂàÜÈíü
            results = ray.get(futures, timeout=timeout)
            
            # ÂàÜÊûêÁªìÊûú
            success_count = sum(1 for r in results if r == "success")
            deferred_count = sum(1 for r in results if r == "deferred_loading")
            
            logger.info(f"üìä Loading results: {success_count} success, {deferred_count} deferred")
            
            if success_count > 0:
                logger.info("‚úÖ Multi-GPU acceleration enabled!")
                self.model_loaded = True
                return "multi_gpu_success"
            elif deferred_count > 0:
                logger.info("‚úÖ Workers ready for ComfyUI component integration")
                self.model_loaded = True
                return "fallback_to_comfyui"
            else:
                raise Exception("All workers failed to load model")
                
        except Exception as e:
            logger.error(f"‚ùå Distributed model loading failed: {e}")
            logger.exception("Full traceback:")
            return "failed"

    # def run_inference(self, 
    #                 model_state_dict: Dict,
    #                 conditioning_positive: Any,
    #                 conditioning_negative: Any,
    #                 latent_samples: torch.Tensor,
    #                 num_inference_steps: int = 20,
    #                 guidance_scale: float = 8.0,
    #                 seed: int = 42,
    #                 comfyui_vae: Any = None,
    #                 comfyui_clip: Any = None) -> Optional[torch.Tensor]:
    #     """Run inference using the dispatcher with ComfyUI model integration"""
    #     try:
    #         if not self.is_initialized:
    #             logger.error("Dispatcher not initialized")
    #             return None
            
    #         # üîß ÂÖ≥ÈîÆ‰øÆÂ§çÔºöÈ¶ñÂÖàÂ∞ùËØïÂàÜÂ∏ÉÂºèÂä†ËΩΩÊ®°Âûã
    #         if not hasattr(self, 'model_loaded') or not self.model_loaded:
    #             logger.info("üîÑ Loading model distributed...")
    #             load_result = self.load_model_distributed(self.model_path)
    #             if load_result == "failed":
    #                 logger.error("‚ùå Model loading failed completely")
    #                 return None
    #             elif load_result == "fallback_to_comfyui":
    #                 logger.info("üéØ Workers ready for ComfyUI component integration - proceeding with multi-GPU inference")
            
    #         # Get next available worker
    #         worker = self.get_next_worker()
    #         if worker is None:
    #             logger.error("No available workers")
    #             return None
                
    #         # üîß ‰øÆÂ§çÊ®°ÂûãË∑ØÂæÑÂ§ÑÁêÜÔºöÁõ¥Êé•‰ΩøÁî®safetensorsÊñá‰ª∂ËøõË°åxDiTÊé®ÁêÜ
    #         effective_model_path = self.model_path
            
    #         # üéØ ÊûÑÂª∫ÂåÖÂê´ComfyUIÁªÑ‰ª∂ÁöÑmodel_info
    #         model_info = {
    #             'path': effective_model_path,
    #             'type': 'flux',  # ÂÅáËÆæÊòØFLUXÊ®°Âûã
    #             'vae': comfyui_vae,
    #             'clip': comfyui_clip
    #         }
            
    #         logger.info(f"üéØ Passing ComfyUI components to worker:")
    #         logger.info(f"  ‚Ä¢ VAE: {'‚úÖ Available' if comfyui_vae is not None else '‚ùå Missing'}")
    #         logger.info(f"  ‚Ä¢ CLIP: {'‚úÖ Available' if comfyui_clip is not None else '‚ùå Missing'}")
            
    #         # È™åËØÅÊ®°ÂûãË∑ØÂæÑ
    #         if not os.path.exists(effective_model_path):
    #             logger.error(f"Model file not found: {effective_model_path}")
    #             return None
            
    #         logger.info(f"Running xDiT inference with {len(self.workers)} workers")
    #         logger.info(f"Model: {model_info['path']}")
    #         logger.info(f"Steps: {num_inference_steps}, CFG: {guidance_scale}")
            
    #         # üîß Â¢ûÂä†Ë∂ÖÊó∂Êó∂Èó¥ÂíåÊ∑ªÂä†ËøõÂ∫¶ÁõëÊéß
    #         max_retries = 3
    #         timeout_seconds = 300  # 5ÂàÜÈíüË∂ÖÊó∂
            
    #         for attempt in range(max_retries):
    #             try:
    #                 logger.info(f"üîÑ Attempt {attempt + 1}/{max_retries} - Running xDiT inference...")
                    
    #                 # üîß Run inference with model_info instead of model_state_dict
    #                 if RAY_AVAILABLE:
    #                     # ÂàõÂª∫Êé®ÁêÜ‰ªªÂä°
    #                     future = worker.run_inference.remote(
    #                         model_info=model_info,  # ‰º†ÈÄíÂåÖÂê´ComfyUIÁªÑ‰ª∂ÁöÑmodel_info
    #                         conditioning_positive=conditioning_positive,
    #                         conditioning_negative=conditioning_negative,
    #                         latent_samples=latent_samples,
    #                         num_inference_steps=num_inference_steps,
    #                         guidance_scale=guidance_scale,
    #                         seed=seed
    #                     )
                        
    #                     # ‰ΩøÁî®Êõ¥Êô∫ËÉΩÁöÑÁ≠âÂæÖÁ≠ñÁï•
    #                     logger.info(f"‚è≥ Waiting for worker response (timeout: {timeout_seconds}s)...")
    #                     start_time = time.time()
    #                     check_interval = 10  # ÊØè10ÁßíÊ£ÄÊü•‰∏ÄÊ¨°
                        
    #                     while True:
    #                         try:
    #                             # Â∞ùËØïËé∑ÂèñÁªìÊûúÔºàÈùûÈòªÂ°ûÔºâ
    #                             result = ray.get(future, timeout=check_interval)
    #                             break  # ÊàêÂäüËé∑ÂèñÁªìÊûú
    #                         except ray.exceptions.GetTimeoutError:
    #                             elapsed = time.time() - start_time
    #                             if elapsed > timeout_seconds:
    #                                 logger.error(f"‚è∞ Timeout after {elapsed:.1f}s")
    #                                 raise TimeoutError(f"Inference timeout after {elapsed:.1f}s")
    #                             else:
    #                                 logger.info(f"‚è≥ Still processing... ({elapsed:.1f}s elapsed)")
    #                                 # Ê£ÄÊü•workerÁä∂ÊÄÅ
    #                                 try:
    #                                     gpu_info = ray.get(worker.get_gpu_info.remote(), timeout=1)
    #                                     logger.info(f"Worker GPU memory: {gpu_info.get('memory_allocated_gb', 0):.1f}GB allocated")
    #                                 except:
    #                                     pass
    #                 else:
    #                     result = worker.run_inference(
    #                         model_info=model_info,  # ‰º†ÈÄíÂåÖÂê´ComfyUIÁªÑ‰ª∂ÁöÑmodel_info
    #                         conditioning_positive=conditioning_positive,
    #                         conditioning_negative=conditioning_negative,
    #                         latent_samples=latent_samples,
    #                         num_inference_steps=num_inference_steps,
    #                         guidance_scale=guidance_scale,
    #                         seed=seed
    #                     )
                    
    #                 # Ê£ÄÊü•ÁªìÊûú
    #                 if result is not None:
    #                     logger.info(f"‚úÖ xDiT inference completed successfully on attempt {attempt + 1}")
                        
    #                     # Update worker load
    #                     worker_id = None
    #                     for gpu_id, w in self.workers.items():
    #                         if w == worker:
    #                             worker_id = gpu_id
    #                             break
                        
    #                     if worker_id is not None:
    #                         self.worker_loads[worker_id] = max(0, self.worker_loads[worker_id] - 1)
                        
    #                     return result
    #                 else:
    #                     logger.warning(f"‚ö†Ô∏è xDiT inference returned None on attempt {attempt + 1}")
    #                     if attempt < max_retries - 1:
    #                         logger.info(f"üîÑ Retrying with different worker...")
    #                         # Â∞ùËØï‰∏ã‰∏Ä‰∏™worker
    #                         worker = self.get_next_worker()
    #                         if worker is None:
    #                             logger.error("No more available workers")
    #                             break
    #                     else:
    #                         logger.error("‚ùå All attempts failed - xDiT inference returned None")
    #                         break
                            
    #             except (ray.exceptions.GetTimeoutError, TimeoutError):
    #                 logger.error(f"‚è∞ Timeout on attempt {attempt + 1}")
    #                 if attempt < max_retries - 1:
    #                     logger.info(f"üîÑ Retrying with different worker...")
    #                     # Â∞ùËØï‰∏ã‰∏Ä‰∏™worker
    #                     worker = self.get_next_worker()
    #                     if worker is None:
    #                         logger.error("No more available workers")
    #                         break
    #                 else:
    #                     logger.error("‚ùå All attempts timed out")
    #                     break
                        
    #             except Exception as e:
    #                 logger.error(f"‚ùå Error on attempt {attempt + 1}: {e}")
    #                 logger.exception("Inference error traceback:")
    #                 if attempt < max_retries - 1:
    #                     logger.info(f"üîÑ Retrying with different worker...")
    #                     # Â∞ùËØï‰∏ã‰∏Ä‰∏™worker
    #                     worker = self.get_next_worker()
    #                     if worker is None:
    #                         logger.error("No more available workers")
    #                         break
    #                 else:
    #                     logger.error("‚ùå All attempts failed")
    #                     break
            
    #         # Â¶ÇÊûúÊâÄÊúâÂ∞ùËØïÈÉΩÂ§±Ë¥•‰∫ÜÔºåËß¶Âèëfallback
    #         logger.warning("‚ö†Ô∏è xDiT multi-GPU failed, falling back to single-GPU")
    #         return None
            
    #     except Exception as e:
    #         logger.error(f"Error during inference: {e}")
    #         logger.exception("Full traceback:")
    #         return None

    def get_status(self) -> Dict[str, Any]:
        """Get dispatcher status"""
        status = {
            "is_initialized": self.is_initialized,
            "num_workers": len(self.workers),
            "scheduling_strategy": self.scheduling_strategy.value,
            "worker_loads": self.worker_loads.copy(),
            "gpu_devices": self.gpu_devices
        }
        
        # Get detailed GPU info
        gpu_infos = {}
        for gpu_id, worker in self.workers.items():
            try:
                if RAY_AVAILABLE:
                    gpu_info = ray.get(worker.get_gpu_info.remote())
                else:
                    gpu_info = worker.get_gpu_info()
                gpu_infos[gpu_id] = gpu_info
            except Exception as e:
                gpu_infos[gpu_id] = {"error": str(e)}
        
        status["gpu_infos"] = gpu_infos
        return status
    
    def cleanup(self):
        """Clean up all workers"""
        try:
            logger.info("Cleaning up XDiT Dispatcher")
            
            for gpu_id, worker in self.workers.items():
                try:
                    if RAY_AVAILABLE:
                        ray.get(worker.cleanup.remote())
                    else:
                        worker.cleanup()
                    logger.info(f"‚úÖ Worker on GPU {gpu_id} cleaned up")
                except Exception as e:
                    logger.error(f"Error cleaning up worker on GPU {gpu_id}: {e}")
            
            self.workers.clear()
            self.worker_loads.clear()
            self.is_initialized = False
            
            # Shutdown Ray if we initialized it
            if RAY_AVAILABLE and ray.is_initialized():
                ray.shutdown()
                logger.info("Ray shutdown")
            
            logger.info("‚úÖ XDiT Dispatcher cleaned up")
            
        except Exception as e:
            logger.error(f"Error during dispatcher cleanup: {e}") 