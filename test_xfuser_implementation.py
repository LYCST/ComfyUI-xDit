#!/usr/bin/env python3
"""
æµ‹è¯•xfuser FLUXå¤šGPUæ¨ç†å®ç°
=============================
éªŒè¯xfuseræ˜¯å¦æ­£ç¡®é›†æˆå¹¶èƒ½è¿›è¡ŒFLUXæ¨¡å‹çš„å¤šGPUæ¨ç†
"""

import sys
import torch
import logging
import os
import time

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_xfuser_import():
    """æµ‹è¯•xfuserå¯¼å…¥"""
    logger.info("1ï¸âƒ£ Testing xfuser import...")
    
    try:
        import xfuser
        logger.info(f"âœ… xfuser imported successfully, version: {getattr(xfuser, '__version__', 'unknown')}")
        
        from xfuser import xFuserArgs
        logger.info("âœ… xFuserArgs imported")
        
        from xfuser import xFuserFluxPipeline
        logger.info("âœ… xFuserFluxPipeline imported")
        
        from xfuser.core.distributed import initialize_runtime_state
        logger.info("âœ… initialize_runtime_state imported")
        
        return True
        
    except ImportError as e:
        logger.error(f"âŒ xfuser import failed: {e}")
        logger.info("ğŸ’¡ Try: pip install xfuser")
        return False
    except Exception as e:
        logger.error(f"âŒ Unexpected error during import: {e}")
        return False

def test_flux_model_availability():
    """æµ‹è¯•FLUXæ¨¡å‹å¯ç”¨æ€§"""
    logger.info("2ï¸âƒ£ Testing FLUX model availability...")
    
    # æ£€æŸ¥å¸¸è§çš„FLUXæ¨¡å‹è·¯å¾„
    flux_paths = [
        "/home/shuzuan/prj/ComfyUI-xDit/models/unet/flux1-dev.safetensors",
        "/home/shuzuan/prj/ComfyUI-xDit/models/checkpoints/flux1-dev.safetensors",
        "black-forest-labs/FLUX.1-dev",
        "black-forest-labs/FLUX.1-schnell"
    ]
    
    for path in flux_paths:
        if os.path.exists(path):
            logger.info(f"âœ… Found FLUX model at: {path}")
            return path
        elif not path.startswith("/"):
            logger.info(f"ğŸ” Will try to download from HuggingFace: {path}")
            return path
    
    logger.warning("âš ï¸ No local FLUX model found, will try HuggingFace download")
    return "black-forest-labs/FLUX.1-schnell"

def test_gpu_availability():
    """æµ‹è¯•GPUå¯ç”¨æ€§"""
    logger.info("3ï¸âƒ£ Testing GPU availability...")
    
    if not torch.cuda.is_available():
        logger.error("âŒ CUDA not available")
        return False
    
    gpu_count = torch.cuda.device_count()
    logger.info(f"âœ… Found {gpu_count} GPUs")
    
    for i in range(gpu_count):
        props = torch.cuda.get_device_properties(i)
        memory_gb = props.total_memory / (1024**3)
        logger.info(f"   GPU {i}: {props.name} ({memory_gb:.1f} GB)")
    
    return gpu_count >= 2  # è‡³å°‘éœ€è¦2ä¸ªGPUè¿›è¡Œå¤šGPUæµ‹è¯•

def test_xfuser_args_creation():
    """æµ‹è¯•xFuserArgsåˆ›å»º"""
    logger.info("4ï¸âƒ£ Testing xFuserArgs creation...")
    
    try:
        from xfuser import xFuserArgs
        
        # åˆ›å»ºxfuserå‚æ•°ï¼ˆç”¨äº8GPUé…ç½®ï¼‰
        xfuser_args = xFuserArgs(
            model="black-forest-labs/FLUX.1-schnell",
            height=1024,
            width=1024,
            ulysses_degree=2,  # åºåˆ—å¹¶è¡Œåº¦
            pipefusion_parallel_degree=2,  # ç®¡é“å¹¶è¡Œåº¦
            use_cfg_parallel=False,  # FLUXä¸ä½¿ç”¨CFG
            use_parallel_vae=False,
            seed=42,
            output_type="latent",
            warmup_steps=0,
            trust_remote_code=True,
            use_torch_compile=False,
            num_inference_steps=20,
        )
        
        logger.info("âœ… xFuserArgs created successfully")
        logger.info(f"   Model: {xfuser_args.model}")
        logger.info(f"   Ulysses degree: {xfuser_args.ulysses_degree}")
        logger.info(f"   PipeFusion degree: {xfuser_args.pipefusion_parallel_degree}")
        
        return xfuser_args
        
    except Exception as e:
        logger.error(f"âŒ xFuserArgs creation failed: {e}")
        logger.exception("Full traceback:")
        return None

def test_distributed_initialization(xfuser_args):
    """æµ‹è¯•åˆ†å¸ƒå¼åˆå§‹åŒ–"""
    logger.info("5ï¸âƒ£ Testing distributed initialization...")
    
    try:
        from xfuser.core.distributed import initialize_runtime_state
        
        # åˆå§‹åŒ–åˆ†å¸ƒå¼çŠ¶æ€
        initialize_runtime_state(xfuser_args)
        logger.info("âœ… Distributed runtime state initialized")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ Distributed initialization failed: {e}")
        logger.exception("Full traceback:")
        return False

def test_flux_pipeline_creation(model_path):
    """æµ‹è¯•FLUX pipelineåˆ›å»º"""
    logger.info("6ï¸âƒ£ Testing FLUX pipeline creation...")
    
    try:
        from xfuser import xFuserFluxPipeline, xFuserArgs
        
        # è®¾ç½®å•GPUåˆ†å¸ƒå¼ç¯å¢ƒå˜é‡
        os.environ['RANK'] = '0'
        os.environ['WORLD_SIZE'] = '1'
        os.environ['LOCAL_RANK'] = '0'
        os.environ['MASTER_ADDR'] = 'localhost'
        os.environ['MASTER_PORT'] = '12355'
        
        # åˆ›å»ºxfuserå‚æ•°
        xfuser_args = xFuserArgs(
            model=model_path,
            height=512,  # æµ‹è¯•ç”¨è¾ƒå°å°ºå¯¸
            width=512,
            num_inference_steps=4,
            seed=42,
            output_type="latent",
            # è®¾ç½®ä¸ºå•GPUé…ç½®
            ulysses_degree=1,
            pipefusion_parallel_degree=1,
            use_cfg_parallel=False,
        )
        
        # ä½¿ç”¨xFuserArgsåˆ›å»ºé…ç½®
        engine_config = xfuser_args.create_config()
        logger.info("âœ… EngineConfig created from xFuserArgs")
        
        # åˆ›å»ºFLUX pipeline
        pipeline = xFuserFluxPipeline.from_pretrained(
            model_path,
            engine_config=engine_config,
            torch_dtype=torch.float16,
        )
        
        logger.info("âœ… xFuserFluxPipeline created successfully")
        logger.info(f"   Device: {getattr(pipeline, 'device', 'unknown')}")
        
        return pipeline
        
    except Exception as e:
        logger.error(f"âŒ FLUX pipeline creation failed: {e}")
        logger.exception("Full traceback:")
        return None

def test_simple_inference(pipeline):
    """æµ‹è¯•ç®€å•æ¨ç†"""
    logger.info("7ï¸âƒ£ Testing simple inference...")
    
    try:
        # ç®€å•çš„æ¨ç†æµ‹è¯•
        start_time = time.time()
        
        result = pipeline(
            prompt="a cute cat",
            height=512,  # ä½¿ç”¨è¾ƒå°å°ºå¯¸è¿›è¡Œæµ‹è¯•
            width=512,
            num_inference_steps=4,  # ä½¿ç”¨è¾ƒå°‘æ­¥æ•°è¿›è¡Œæµ‹è¯•
            guidance_scale=1.0,  # FLUX.1-schnell ä½¿ç”¨1.0
            output_type="latent",
        )
        
        end_time = time.time()
        
        logger.info(f"âœ… Simple inference completed in {end_time - start_time:.2f}s")
        logger.info(f"   Output type: {type(result)}")
        
        if hasattr(result, 'images'):
            logger.info(f"   Images shape: {result.images.shape}")
        elif hasattr(result, 'latents'):
            logger.info(f"   Latents shape: {result.latents.shape}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ Simple inference failed: {e}")
        logger.exception("Full traceback:")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("ğŸ§ª xfuser FLUXå¤šGPUæ¨ç†æµ‹è¯•")
    logger.info("=" * 60)
    
    # æµ‹è¯•åºåˆ—
    tests = [
        ("Import Test", test_xfuser_import),
        ("GPU Availability", test_gpu_availability),
        ("FLUX Model", test_flux_model_availability),
    ]
    
    results = {}
    
    # åŸºç¡€æµ‹è¯•
    for test_name, test_func in tests:
        try:
            if test_name == "FLUX Model":
                result = test_func()
                model_path = result
                results[test_name] = bool(result)
            else:
                result = test_func()
                results[test_name] = result
        except Exception as e:
            logger.error(f"Test {test_name} failed with exception: {e}")
            results[test_name] = False
    
    # å¦‚æœåŸºç¡€æµ‹è¯•é€šè¿‡ï¼Œç»§ç»­è¿›é˜¶æµ‹è¯•
    if all(results.values()):
        logger.info("\nğŸš€ Basic tests passed, proceeding with advanced tests...")
        
        # åˆ›å»ºxfuserå‚æ•°
        xfuser_args = test_xfuser_args_creation()
        if xfuser_args:
            results["xFuserArgs"] = True
            
            # åˆå§‹åŒ–åˆ†å¸ƒå¼ï¼ˆå¯é€‰ï¼Œå•GPUæµ‹è¯•å¯èƒ½ä¸éœ€è¦ï¼‰
            # distributed_ok = test_distributed_initialization(xfuser_args)
            # results["Distributed Init"] = distributed_ok
            
            # åˆ›å»ºpipeline
            pipeline = test_flux_pipeline_creation(model_path)
            if pipeline:
                results["Pipeline Creation"] = True
                
                # ç®€å•æ¨ç†æµ‹è¯•
                inference_ok = test_simple_inference(pipeline)
                results["Simple Inference"] = inference_ok
            else:
                results["Pipeline Creation"] = False
        else:
            results["xFuserArgs"] = False
    
    # è¾“å‡ºæµ‹è¯•ç»“æœ
    logger.info("\nğŸ“Š Test Results:")
    logger.info("=" * 60)
    
    all_passed = True
    for test_name, passed in results.items():
        status = "âœ… PASS" if passed else "âŒ FAIL"
        logger.info(f"   {test_name}: {status}")
        if not passed:
            all_passed = False
    
    logger.info("=" * 60)
    if all_passed:
        logger.info("ğŸ‰ ALL TESTS PASSED! xfuser is ready for FLUX multi-GPU inference")
        logger.info("ğŸ’¡ You can now test the ComfyUI integration")
    else:
        logger.error("âŒ Some tests failed. Please check the setup")
        logger.info("ğŸ’¡ Recommendations:")
        if not results.get("Import Test", False):
            logger.info("   â€¢ Install xfuser: pip install xfuser")
        if not results.get("GPU Availability", False):
            logger.info("   â€¢ Check CUDA installation and GPU drivers")
        if not results.get("FLUX Model", False):
            logger.info("   â€¢ Download FLUX model or check model paths")

if __name__ == "__main__":
    main() 