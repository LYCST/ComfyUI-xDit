#!/usr/bin/env python3
"""
TACO-DiT ä½¿ç”¨ç¤ºä¾‹

å±•ç¤ºå¦‚ä½•åœ¨ComfyUIä¸­ä½¿ç”¨TACO-DiTè¿›è¡Œå¤šGPUå¹¶è¡Œæ¨ç†
"""

import sys
import os
import logging
import torch

# æ·»åŠ ComfyUIè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def example_1_auto_config():
    """ç¤ºä¾‹1: è‡ªåŠ¨é…ç½®æ¨¡å¼"""
    logger.info("="*60)
    logger.info("ç¤ºä¾‹1: è‡ªåŠ¨é…ç½®æ¨¡å¼")
    logger.info("="*60)
    
    from comfy.taco_dit import TACODiTConfigManager
    
    # åˆ›å»ºé…ç½®ç®¡ç†å™¨ï¼Œè‡ªåŠ¨æ£€æµ‹æœ€ä¼˜é…ç½®
    config_manager = TACODiTConfigManager()
    
    # è·å–è‡ªåŠ¨æ£€æµ‹çš„é…ç½®
    config = config_manager.config
    
    logger.info(f"è‡ªåŠ¨æ£€æµ‹çš„é…ç½®:")
    logger.info(f"  - å¯ç”¨çŠ¶æ€: {config.enabled}")
    logger.info(f"  - åºåˆ—å¹¶è¡Œ: {config.sequence_parallel} (åº¦: {config.sequence_parallel_degree})")
    logger.info(f"  - æµæ°´çº¿å¹¶è¡Œ: {config.pipeline_parallel} (åº¦: {config.pipeline_parallel_degree})")
    logger.info(f"  - å¼ é‡å¹¶è¡Œ: {config.tensor_parallel} (åº¦: {config.tensor_parallel_degree})")
    logger.info(f"  - CFGå¹¶è¡Œ: {config.cfg_parallel}")
    logger.info(f"  - Flash Attention: {config.use_flash_attention}")
    
    # è·å–å¹¶è¡Œä¿¡æ¯å­—ç¬¦ä¸²
    parallel_info = config_manager.get_parallel_info()
    logger.info(f"å¹¶è¡Œé…ç½®: {parallel_info}")
    
    return config

def example_2_manual_config():
    """ç¤ºä¾‹2: æ‰‹åŠ¨é…ç½®æ¨¡å¼"""
    logger.info("="*60)
    logger.info("ç¤ºä¾‹2: æ‰‹åŠ¨é…ç½®æ¨¡å¼")
    logger.info("="*60)
    
    from comfy.taco_dit import TACODiTConfig
    
    # æ‰‹åŠ¨åˆ›å»ºé…ç½®
    config = TACODiTConfig(
        enabled=True,
        sequence_parallel=True,
        sequence_parallel_degree=4,  # ä½¿ç”¨4ä¸ªGPUè¿›è¡Œåºåˆ—å¹¶è¡Œ
        pipeline_parallel=True,
        pipeline_parallel_degree=2,  # ä½¿ç”¨2ä¸ªGPUè¿›è¡Œæµæ°´çº¿å¹¶è¡Œ
        cfg_parallel=True,
        use_flash_attention=True,
        use_cache=True
    )
    
    logger.info(f"æ‰‹åŠ¨é…ç½®:")
    logger.info(f"  - æ€»å¹¶è¡Œåº¦: {config.sequence_parallel_degree * config.pipeline_parallel_degree}")
    logger.info(f"  - åºåˆ—å¹¶è¡Œ: {config.sequence_parallel_degree}")
    logger.info(f"  - æµæ°´çº¿å¹¶è¡Œ: {config.pipeline_parallel_degree}")
    
    return config

def example_3_distributed_setup():
    """ç¤ºä¾‹3: åˆ†å¸ƒå¼ç¯å¢ƒè®¾ç½®"""
    logger.info("="*60)
    logger.info("ç¤ºä¾‹3: åˆ†å¸ƒå¼ç¯å¢ƒè®¾ç½®")
    logger.info("="*60)
    
    from comfy.taco_dit import TACODiTDistributedManager, TACODiTConfig
    
    # åˆ›å»ºé…ç½®
    config = TACODiTConfig(enabled=True, auto_detect=False)
    
    # åˆ›å»ºåˆ†å¸ƒå¼ç®¡ç†å™¨
    dist_manager = TACODiTDistributedManager()
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
    success = dist_manager.initialize(config)
    logger.info(f"åˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ–: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
    
    if success:
        # è·å–åˆ†å¸ƒå¼ä¿¡æ¯
        world_info = dist_manager.get_world_info()
        logger.info(f"åˆ†å¸ƒå¼ä¿¡æ¯: {world_info}")
        
        # è·å–å½“å‰è®¾å¤‡
        device = dist_manager.get_device()
        logger.info(f"å½“å‰è®¾å¤‡: {device}")
    
    return dist_manager

def example_4_execution_engine():
    """ç¤ºä¾‹4: æ‰§è¡Œå¼•æ“ä½¿ç”¨"""
    logger.info("="*60)
    logger.info("ç¤ºä¾‹4: æ‰§è¡Œå¼•æ“ä½¿ç”¨")
    logger.info("="*60)
    
    from comfy.taco_dit import TACODiTExecutionEngine, TACODiTConfig
    
    # åˆ›å»ºé…ç½®
    config = TACODiTConfig(
        enabled=True,
        sequence_parallel=True,
        sequence_parallel_degree=2,
        cfg_parallel=True
    )
    
    # åˆ›å»ºæ‰§è¡Œå¼•æ“
    engine = TACODiTExecutionEngine(config)
    
    # è·å–æ‰§è¡Œä¿¡æ¯
    exec_info = engine.get_execution_info()
    logger.info(f"æ‰§è¡Œå¼•æ“ä¿¡æ¯:")
    logger.info(f"  - å¹¶è¡Œé…ç½®: {exec_info['parallel_config']}")
    logger.info(f"  - xDitå¯ç”¨: {exec_info['xdit_available']}")
    logger.info(f"  - åˆ†å¸ƒå¼ä¿¡æ¯: {exec_info['distributed_info']}")
    
    return engine

def example_5_model_wrapper():
    """ç¤ºä¾‹5: æ¨¡å‹åŒ…è£…å™¨ä½¿ç”¨"""
    logger.info("="*60)
    logger.info("ç¤ºä¾‹5: æ¨¡å‹åŒ…è£…å™¨ä½¿ç”¨")
    logger.info("="*60)
    
    from comfy.taco_dit import TACODiTModelPatcher, TACODiTConfig
    
    # åˆ›å»ºé…ç½®
    config = TACODiTConfig(
        enabled=True,
        sequence_parallel=True,
        sequence_parallel_degree=2
    )
    
    # åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿæ¨¡å‹ï¼ˆå®é™…ä½¿ç”¨æ—¶ä¼šæ˜¯çœŸå®çš„DiTæ¨¡å‹ï¼‰
    class MockModel:
        def __init__(self):
            self.name = "MockDiTModel"
        
        def forward(self, **kwargs):
            return torch.randn(1, 3, 1024, 1024)  # æ¨¡æ‹Ÿè¾“å‡º
    
    # åˆ›å»ºæ¨¡å‹åŒ…è£…å™¨
    model = MockModel()
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    
    model_patcher = TACODiTModelPatcher(
        model=model,
        load_device=device,
        offload_device='cpu',
        parallel_config=config
    )
    
    # è·å–æ¨¡å‹ä¿¡æ¯
    model_info = model_patcher.get_model_info()
    logger.info(f"æ¨¡å‹åŒ…è£…å™¨ä¿¡æ¯:")
    logger.info(f"  - æ¨¡å‹ç±»å‹: {model_info['model_type']}")
    logger.info(f"  - å¹¶è¡Œå¯ç”¨: {model_info['parallel_enabled']}")
    logger.info(f"  - xDitåŒ…è£…å™¨: {model_info['xdit_wrapper']}")
    
    return model_patcher

def example_6_performance_monitoring():
    """ç¤ºä¾‹6: æ€§èƒ½ç›‘æ§"""
    logger.info("="*60)
    logger.info("ç¤ºä¾‹6: æ€§èƒ½ç›‘æ§")
    logger.info("="*60)
    
    from comfy.taco_dit import TACODiTExecutionEngine, TACODiTConfig
    import time
    
    # åˆ›å»ºé…ç½®
    config = TACODiTConfig(enabled=True, sequence_parallel=True, sequence_parallel_degree=2)
    
    # åˆ›å»ºæ‰§è¡Œå¼•æ“
    engine = TACODiTExecutionEngine(config)
    
    # æ¨¡æ‹Ÿå¤šæ¬¡æ‰§è¡Œ
    for i in range(3):
        logger.info(f"æ‰§è¡Œ {i+1}/3...")
        
        # æ¨¡æ‹Ÿè¾“å…¥æ•°æ®
        input_data = {
            'batch_size': 1,
            'sequence_length': 256,
            'height': 1024,
            'width': 1024,
            'x': torch.randn(1, 3, 1024, 1024)
        }
        
        # æ¨¡æ‹Ÿæ¨¡å‹
        class MockModel:
            def forward(self, **kwargs):
                time.sleep(0.1)  # æ¨¡æ‹Ÿæ¨ç†æ—¶é—´
                return torch.randn(1, 3, 1024, 1024)
        
        model = MockModel()
        
        # æ‰§è¡Œæ¨ç†
        start_time = time.time()
        output = engine.execute(model, input_data)
        execution_time = time.time() - start_time
        
        logger.info(f"  æ‰§è¡Œæ—¶é—´: {execution_time:.3f}s")
    
    # è·å–æ‰§è¡Œç»Ÿè®¡
    exec_info = engine.get_execution_info()
    stats = exec_info['execution_stats']
    
    logger.info(f"æ‰§è¡Œç»Ÿè®¡:")
    logger.info(f"  - æ€»æ‰§è¡Œæ¬¡æ•°: {stats['total_executions']}")
    logger.info(f"  - å¹³å‡æ‰§è¡Œæ—¶é—´: {stats['avg_time']:.3f}s")
    logger.info(f"  - å¹¶è¡Œæ‰§è¡Œæ¬¡æ•°: {stats['parallel_executions']}")
    logger.info(f"  - å›é€€æ‰§è¡Œæ¬¡æ•°: {stats['fallback_executions']}")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ TACO-DiT ä½¿ç”¨ç¤ºä¾‹")
    logger.info("="*60)
    
    try:
        # è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
        config1 = example_1_auto_config()
        config2 = example_2_manual_config()
        dist_manager = example_3_distributed_setup()
        engine = example_4_execution_engine()
        model_patcher = example_5_model_wrapper()
        example_6_performance_monitoring()
        
        logger.info("="*60)
        logger.info("ğŸ‰ æ‰€æœ‰ç¤ºä¾‹è¿è¡ŒæˆåŠŸï¼")
        logger.info("="*60)
        logger.info("TACO-DiT å·²å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥å¼€å§‹ä½¿ç”¨å¤šGPUå¹¶è¡Œæ¨ç†äº†ï¼")
        
        # æä¾›ä½¿ç”¨å»ºè®®
        logger.info("\nğŸ“‹ ä½¿ç”¨å»ºè®®:")
        logger.info("1. å¯¹äº6å¼ RTX 4090ï¼Œå»ºè®®ä½¿ç”¨åºåˆ—å¹¶è¡Œåº¦4 + æµæ°´çº¿å¹¶è¡Œåº¦2")
        logger.info("2. å¯ç”¨Flash Attentionä»¥è·å¾—æœ€ä½³æ€§èƒ½")
        logger.info("3. ä½¿ç”¨CFGå¹¶è¡Œæ¥åŠ é€Ÿåˆ†ç±»å™¨å¼•å¯¼")
        logger.info("4. ç›‘æ§GPUå†…å­˜ä½¿ç”¨æƒ…å†µï¼Œé¿å…OOM")
        
    except Exception as e:
        logger.error(f"âŒ ç¤ºä¾‹è¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 