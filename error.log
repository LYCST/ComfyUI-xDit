Checkpoint files will always be loaded safely.
Total VRAM 24210 MB, total RAM 257688 MB
pytorch version: 2.5.1+cu124
Set vram state to: NORMAL_VRAM
Device: cuda:0 NVIDIA GeForce RTX 4090 : cudaMallocAsync
Using pytorch attention
Python version: 3.11.13 (main, Jun  5 2025, 13:12:00) [GCC 11.2.0]
ComfyUI version: 0.3.41
ComfyUI frontend version: 1.22.2
[Prompt Server] web root: /home/shuzuan/miniconda3/envs/comfyui-xdit/lib/python3.11/site-packages/comfyui_frontend_package/static
✅ xDiT framework loaded successfully
✅ Ray framework loaded successfully
🚀 ComfyUI xDiT Multi-GPU Plugin v1.0.0 loaded successfully!
   📋 Drop-in replacements for standard ComfyUI nodes:
      • XDiTCheckpointLoader -> CheckpointLoaderSimple
      • XDiTUNetLoader -> UNetLoader
      • XDiTVAELoader -> VAELoader
      • XDiTCLIPLoader -> CLIPLoader
      • XDiTDualCLIPLoader -> DualCLIPLoader
      • XDiTKSampler -> KSampler
   ✅ Multi-GPU acceleration enabled
   📊 Available parallel strategies: PipeFusion, USP, Hybrid, Tensor, CFG
   🎯 Ray-based distributed computing enabled
   📈 Available scheduling strategies: round_robin, least_loaded, weighted_round_robin, adaptive
   💡 Usage: Simply replace standard nodes with xDiT versions for automatic multi-GPU acceleration

Import times for custom nodes:
   0.0 seconds: /home/shuzuan/prj/ComfyUI-xDit/custom_nodes/websocket_image_save.py
   0.7 seconds: /home/shuzuan/prj/ComfyUI-xDit/custom_nodes/comfyui_xdit_multigpu

Context impl SQLiteImpl.
Will assume non-transactional DDL.
No target revision found.
Starting server

To see the GUI go to: http://0.0.0.0:12411
got prompt
Using pytorch attention in VAE
Using pytorch attention in VAE
VAE load device: cuda:0, offload device: cpu, dtype: torch.bfloat16
CLIP/text encoder model load device: cuda:0, offload device: cpu, current: cpu, dtype: torch.float16
clip missing: ['text_projection.weight']
The load_unet function has been deprecated and will be removed please switch to: load_diffusion_model
model weight dtype torch.bfloat16, manual cast: None
model_type FLUX
Initializing xDiT multi-GPU acceleration for UNet flux/flux1-dev.safetensors on GPUs: [0, 1, 2, 3, 4, 5, 6, 7]
Initializing XDiT Dispatcher with 8 GPUs
Scheduling strategy: round_robin
Distributed config: 127.0.0.1:41445, world_size=8
Ray configuration for 8 GPUs:
  Object store memory: 64GB
  Total system memory: 201GB
Initializing Ray with config: {'num_cpus': 96, 'num_gpus': 8, 'object_store_memory': 68719476736, 'dashboard_port': 8265, 'dashboard_host': '0.0.0.0', 'ignore_reinit_error': True, 'local_mode': False, 'log_to_driver': True, 'logging_level': 20, '_temp_dir': '/home/shuzuan/prj/ComfyUI-xDit/ray_temp', 'include_dashboard': False}
2025-06-26 17:21:58,333	WARNING worker.py:1578 -- SIGTERM handler is not set because current thread is not the main thread.
2025-06-26 17:22:02,783	INFO worker.py:1917 -- Started a local Ray instance.
Ray initialized successfully
Available resources: {'GPU': 8.0, 'memory': 162470166528.0, 'object_store_memory': 68719476736.0, 'CPU': 96.0, 'node:__internal_head__': 1.0, 'node:192.168.10.101': 1.0, 'accelerator_type:G': 1.0}
Available GPUs: [0, 1, 2, 3, 4, 5, 6, 7]
Ray initialized successfully
Waiting for all workers to complete basic initialization...
✅ Ray worker initialized on GPU 0
✅ Ray worker initialized on GPU 1
✅ Ray worker initialized on GPU 2
✅ Ray worker initialized on GPU 3
✅ Ray worker initialized on GPU 4
✅ Ray worker initialized on GPU 5
✅ Ray worker initialized on GPU 6
✅ Ray worker initialized on GPU 7
Initializing distributed environment for multi-GPU...
✅ Distributed initialized on GPU 0
✅ Distributed initialized on GPU 1
✅ Distributed initialized on GPU 2
✅ Distributed initialized on GPU 3
✅ Distributed initialized on GPU 4
✅ Distributed initialized on GPU 5
✅ Distributed initialized on GPU 6
✅ Distributed initialized on GPU 7
✅ XDiT Dispatcher initialized with 8 workers
✅ xDiT multi-GPU acceleration enabled for UNet with 8 workers
   Scheduling strategy: round_robin
Requested to load FluxClipModel_
[36m(XDiTWorker pid=2901988)[0m I0626 17:22:09.102000 2901988 site-packages/torch/distributed/distributed_c10d.py:415] Using backend config: {'cuda': 'nccl'}
loaded completely 22589.425 9319.23095703125 True
Attempting xDiT multi-GPU acceleration: 20 steps, CFG=1.0
🎯 ComfyUI components available:
  • VAE: ✅ Available
  • CLIP: ✅ Available
🔄 Loading model distributed...
🚀 Starting distributed model loading: /home/shuzuan/prj/ComfyUI-xDit/models/unet/flux/flux1-dev.safetensors
💡 Safetensors format detected - using ComfyUI component reuse strategy
⚡ No downloads needed! Will use ComfyUI loaded VAE/CLIP components
🎯 This should complete in seconds, not minutes
⏳ Initializing workers with intelligent component reuse...
📊 Loading results: 0 success, 8 deferred
✅ Workers ready for ComfyUI component integration
🎯 Workers ready for ComfyUI component integration - proceeding with multi-GPU inference
🎯 Passing ComfyUI components to worker:
  • VAE: ✅ Available
  • CLIP: ✅ Available
Using safetensors file for xDiT: /home/shuzuan/prj/ComfyUI-xDit/models/unet/flux/flux1-dev.safetensors
✅ Safetensors file verified: /home/shuzuan/prj/ComfyUI-xDit/models/unet/flux/flux1-dev.safetensors
Running xDiT inference with 8 workers
Model: /home/shuzuan/prj/ComfyUI-xDit/models/unet/flux/flux1-dev.safetensors
Steps: 20, CFG: 1.0
🔄 Attempt 1/3 - Running xDiT inference...
