Checkpoint files will always be loaded safely.
Total VRAM 24210 MB, total RAM 257688 MB
pytorch version: 2.5.1+cu124
Set vram state to: NORMAL_VRAM
Device: cuda:0 NVIDIA GeForce RTX 4090 : cudaMallocAsync
Using pytorch attention
Python version: 3.11.13 (main, Jun  5 2025, 13:12:00) [GCC 11.2.0]
ComfyUI version: 0.3.41
ComfyUI frontend version: 1.22.2
[Prompt Server] web root: /home/shuzuan/miniconda3/envs/comfyui-xdit/lib/python3.11/site-packages/comfyui_frontend_package/static
âœ… xDiT framework loaded successfully
âœ… Ray framework loaded successfully
ğŸš€ ComfyUI xDiT Multi-GPU Plugin v1.0.0 loaded successfully!
   ğŸ“‹ Drop-in replacements for standard ComfyUI nodes:
      â€¢ XDiTCheckpointLoader -> CheckpointLoaderSimple
      â€¢ XDiTUNetLoader -> UNetLoader
      â€¢ XDiTVAELoader -> VAELoader
      â€¢ XDiTCLIPLoader -> CLIPLoader
      â€¢ XDiTDualCLIPLoader -> DualCLIPLoader
      â€¢ XDiTKSampler -> KSampler
   âœ… Multi-GPU acceleration enabled
   ğŸ“Š Available parallel strategies: PipeFusion, USP, Hybrid, Tensor, CFG
   ğŸ¯ Ray-based distributed computing enabled
   ğŸ“ˆ Available scheduling strategies: round_robin, least_loaded, weighted_round_robin, adaptive
   ğŸ’¡ Usage: Simply replace standard nodes with xDiT versions for automatic multi-GPU acceleration

Import times for custom nodes:
   0.0 seconds: /home/shuzuan/prj/ComfyUI-xDit/custom_nodes/websocket_image_save.py
   0.7 seconds: /home/shuzuan/prj/ComfyUI-xDit/custom_nodes/comfyui_xdit_multigpu

Context impl SQLiteImpl.
Will assume non-transactional DDL.
No target revision found.
Starting server

To see the GUI go to: http://0.0.0.0:12411
got prompt
Using pytorch attention in VAE
Using pytorch attention in VAE
VAE load device: cuda:0, offload device: cpu, dtype: torch.bfloat16
CLIP/text encoder model load device: cuda:0, offload device: cpu, current: cpu, dtype: torch.float16
clip missing: ['text_projection.weight']
The load_unet function has been deprecated and will be removed please switch to: load_diffusion_model
model weight dtype torch.bfloat16, manual cast: None
model_type FLUX
Initializing xDiT multi-GPU acceleration for UNet flux/flux1-dev.safetensors on GPUs: [0, 1, 2, 3, 4, 5, 6, 7]
Initializing XDiT Dispatcher with 8 GPUs
Scheduling strategy: round_robin
Distributed config: 127.0.0.1:41445, world_size=8
Ray configuration for 8 GPUs:
  Object store memory: 64GB
  Total system memory: 201GB
Initializing Ray with config: {'num_cpus': 96, 'num_gpus': 8, 'object_store_memory': 68719476736, 'dashboard_port': 8265, 'dashboard_host': '0.0.0.0', 'ignore_reinit_error': True, 'local_mode': False, 'log_to_driver': True, 'logging_level': 20, '_temp_dir': '/home/shuzuan/prj/ComfyUI-xDit/ray_temp', 'include_dashboard': False}
2025-06-26 17:21:58,333	WARNING worker.py:1578 -- SIGTERM handler is not set because current thread is not the main thread.
2025-06-26 17:22:02,783	INFO worker.py:1917 -- Started a local Ray instance.
Ray initialized successfully
Available resources: {'GPU': 8.0, 'memory': 162470166528.0, 'object_store_memory': 68719476736.0, 'CPU': 96.0, 'node:__internal_head__': 1.0, 'node:192.168.10.101': 1.0, 'accelerator_type:G': 1.0}
Available GPUs: [0, 1, 2, 3, 4, 5, 6, 7]
Ray initialized successfully
Waiting for all workers to complete basic initialization...
âœ… Ray worker initialized on GPU 0
âœ… Ray worker initialized on GPU 1
âœ… Ray worker initialized on GPU 2
âœ… Ray worker initialized on GPU 3
âœ… Ray worker initialized on GPU 4
âœ… Ray worker initialized on GPU 5
âœ… Ray worker initialized on GPU 6
âœ… Ray worker initialized on GPU 7
Initializing distributed environment for multi-GPU...
âœ… Distributed initialized on GPU 0
âœ… Distributed initialized on GPU 1
âœ… Distributed initialized on GPU 2
âœ… Distributed initialized on GPU 3
âœ… Distributed initialized on GPU 4
âœ… Distributed initialized on GPU 5
âœ… Distributed initialized on GPU 6
âœ… Distributed initialized on GPU 7
âœ… XDiT Dispatcher initialized with 8 workers
âœ… xDiT multi-GPU acceleration enabled for UNet with 8 workers
   Scheduling strategy: round_robin
Requested to load FluxClipModel_
[36m(XDiTWorker pid=2901988)[0m I0626 17:22:09.102000 2901988 site-packages/torch/distributed/distributed_c10d.py:415] Using backend config: {'cuda': 'nccl'}
loaded completely 22589.425 9319.23095703125 True
Attempting xDiT multi-GPU acceleration: 20 steps, CFG=1.0
ğŸ¯ ComfyUI components available:
  â€¢ VAE: âœ… Available
  â€¢ CLIP: âœ… Available
ğŸ”„ Loading model distributed...
ğŸš€ Starting distributed model loading: /home/shuzuan/prj/ComfyUI-xDit/models/unet/flux/flux1-dev.safetensors
ğŸ’¡ Safetensors format detected - using ComfyUI component reuse strategy
âš¡ No downloads needed! Will use ComfyUI loaded VAE/CLIP components
ğŸ¯ This should complete in seconds, not minutes
â³ Initializing workers with intelligent component reuse...
ğŸ“Š Loading results: 0 success, 8 deferred
âœ… Workers ready for ComfyUI component integration
ğŸ¯ Workers ready for ComfyUI component integration - proceeding with multi-GPU inference
ğŸ¯ Passing ComfyUI components to worker:
  â€¢ VAE: âœ… Available
  â€¢ CLIP: âœ… Available
Using safetensors file for xDiT: /home/shuzuan/prj/ComfyUI-xDit/models/unet/flux/flux1-dev.safetensors
âœ… Safetensors file verified: /home/shuzuan/prj/ComfyUI-xDit/models/unet/flux/flux1-dev.safetensors
Running xDiT inference with 8 workers
Model: /home/shuzuan/prj/ComfyUI-xDit/models/unet/flux/flux1-dev.safetensors
Steps: 20, CFG: 1.0
ğŸ”„ Attempt 1/3 - Running xDiT inference...
