[2025-06-25 17:17:55,172 I 1412746 1412746] (raylet) main.cc:226: Setting cluster ID to: 59e9e893e7d6b771b04ea1c24ba00a733a2943f18b23308bfe3c793c
[2025-06-25 17:17:55,179 I 1412746 1412746] (raylet) main.cc:341: Raylet is not set to kill unknown children.
[2025-06-25 17:17:55,179 I 1412746 1412746] (raylet) io_service_pool.cc:36: IOServicePool is running with 1 io_service.
[2025-06-25 17:17:55,179 I 1412746 1412746] (raylet) main.cc:469: Setting node ID node_id=c3065f0a91075821625dad700db11cc1befa3100fb68c9da57a85f4b
[2025-06-25 17:17:55,180 I 1412746 1412746] (raylet) store_runner.cc:50: Allowing the Plasma store to use up to 68.7195GB of memory.
[2025-06-25 17:17:55,180 I 1412746 1412746] (raylet) store_runner.cc:66: Starting object store with directory /dev/shm, fallback /home/shuzuan/prj/ComfyUI-xDit/ray_temp/session_2025-06-25_17-17-52_471385_1411652, and huge page support disabled
[2025-06-25 17:17:55,180 I 1412746 1412784] (raylet) dlmalloc.cc:324: Setting dlmalloc config: plasma_directory=/dev/shm, fallback_directory=/home/shuzuan/prj/ComfyUI-xDit/ray_temp/session_2025-06-25_17-17-52_471385_1411652, hugepage_enabled=0, fallback_enabled=1
[2025-06-25 17:17:55,180 I 1412746 1412784] (raylet) dlmalloc.cc:153: create_and_mmap_buffer(68719476744, /dev/shm/plasmaXXXXXX)
[2025-06-25 17:17:55,180 I 1412746 1412784] (raylet) store.cc:576: Plasma store debug dump: 
Current usage: 0 / 68.7195 GB
- num bytes created total: 0
0 pending objects of total size 0MB
- objects spillable: 0
- bytes spillable: 0
- objects unsealed: 0
- bytes unsealed: 0
- objects in use: 0
- bytes in use: 0
- objects evictable: 0
- bytes evictable: 0

- objects created by worker: 0
- bytes created by worker: 0
- objects restored: 0
- bytes restored: 0
- objects received: 0
- bytes received: 0
- objects errored: 0
- bytes errored: 0

[2025-06-25 17:17:55,182 I 1412746 1412746] (raylet) grpc_server.cc:141: ObjectManager server started, listening on port 35181.
[2025-06-25 17:17:55,185 I 1412746 1412746] (raylet) worker_killing_policy.cc:107: Running GroupByOwner policy.
[2025-06-25 17:17:55,185 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:17:55,185 I 1412746 1412746] (raylet) memory_monitor.cc:48: MemoryMonitor initialized with usage threshold at 256695140352 bytes (0.95 system memory), total system memory bytes: 270205427712
[2025-06-25 17:17:55,185 I 1412746 1412746] (raylet) node_manager.cc:227: Initializing NodeManager node_id=c3065f0a91075821625dad700db11cc1befa3100fb68c9da57a85f4b
[2025-06-25 17:17:55,185 I 1412746 1412746] (raylet) grpc_server.cc:141: NodeManager server started, listening on port 39541.
[2025-06-25 17:17:55,193 I 1412746 1412830] (raylet) agent_manager.cc:80: Monitor agent process with name dashboard_agent
[2025-06-25 17:17:55,193 I 1412746 1412832] (raylet) agent_manager.cc:80: Monitor agent process with name runtime_env_agent
[2025-06-25 17:17:55,193 I 1412746 1412746] (raylet) event.cc:500: Ray Event initialized for RAYLET
[2025-06-25 17:17:55,193 I 1412746 1412746] (raylet) event.cc:331: Set ray event level to warning
[2025-06-25 17:17:55,195 I 1412746 1412746] (raylet) raylet.cc:260: Raylet of id, c3065f0a91075821625dad700db11cc1befa3100fb68c9da57a85f4b started. Raylet consists of node_manager and object_manager. node_manager address: 192.168.10.101:39541 object_manager address: 192.168.10.101:35181 hostname: vm-ubuntu1
[2025-06-25 17:17:55,197 I 1412746 1412746] (raylet) node_manager.cc:450: [state-dump] NodeManager:
[state-dump] Node ID: c3065f0a91075821625dad700db11cc1befa3100fb68c9da57a85f4b
[state-dump] Node name: 192.168.10.101
[state-dump] InitialConfigResources: {CPU: 96, object_store_memory: 6.87195e+10, accelerator_type:G: 1, GPU: 8, node:192.168.10.101: 1, memory: 1.72942e+11, node:__internal_head__: 1}
[state-dump] ClusterTaskManager:
[state-dump] ========== Node: c3065f0a91075821625dad700db11cc1befa3100fb68c9da57a85f4b =================
[state-dump] Infeasible queue length: 0
[state-dump] Schedule queue length: 0
[state-dump] Dispatch queue length: 0
[state-dump] num_waiting_for_resource: 0
[state-dump] num_waiting_for_plasma_memory: 0
[state-dump] num_waiting_for_remote_node_resources: 0
[state-dump] num_worker_not_started_by_job_config_not_exist: 0
[state-dump] num_worker_not_started_by_registration_timeout: 0
[state-dump] num_tasks_waiting_for_workers: 0
[state-dump] num_cancelled_tasks: 0
[state-dump] cluster_resource_scheduler state: 
[state-dump] Local id: 3089504843952496296 Local resources: {"total":{node:__internal_head__: [10000], CPU: [960000], memory: [1729419591680000], object_store_memory: [687194767360000], node:192.168.10.101: [10000], accelerator_type:G: [10000], GPU: [10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000]}}, "available": {node:192.168.10.101: [10000], accelerator_type:G: [10000], GPU: [10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000], CPU: [960000], node:__internal_head__: [10000], memory: [1729419591680000], object_store_memory: [687194767360000]}}, "labels":{"ray.io/node_id":"c3065f0a91075821625dad700db11cc1befa3100fb68c9da57a85f4b",} is_draining: 0 is_idle: 1 Cluster resources (at most 20 nodes are shown): node id: 3089504843952496296{"total":{memory: 1729419591680000, node:__internal_head__: 10000, accelerator_type:G: 10000, GPU: 80000, object_store_memory: 687194767360000, CPU: 960000, node:192.168.10.101: 10000}}, "available": {object_store_memory: 687194767360000, node:__internal_head__: 10000, node:192.168.10.101: 10000, accelerator_type:G: 10000, GPU: 80000, CPU: 960000, memory: 1729419591680000}}, "labels":{"ray.io/node_id":"c3065f0a91075821625dad700db11cc1befa3100fb68c9da57a85f4b",}, "is_draining": 0, "draining_deadline_timestamp_ms": -1} { "placement group locations": [], "node to bundles": []}
[state-dump] Waiting tasks size: 0
[state-dump] Number of executing tasks: 0
[state-dump] Number of pinned task arguments: 0
[state-dump] Number of total spilled tasks: 0
[state-dump] Number of spilled waiting tasks: 0
[state-dump] Number of spilled unschedulable tasks: 0
[state-dump] Resource usage {
[state-dump] }
[state-dump] Backlog Size per scheduling descriptor :{workerId: num backlogs}:
[state-dump] 
[state-dump] Running tasks by scheduling class:
[state-dump] ==================================================
[state-dump] 
[state-dump] ClusterResources:
[state-dump] LocalObjectManager:
[state-dump] - num pinned objects: 0
[state-dump] - pinned objects size: 0
[state-dump] - num objects pending restore: 0
[state-dump] - num objects pending spill: 0
[state-dump] - num bytes pending spill: 0
[state-dump] - num bytes currently spilled: 0
[state-dump] - cumulative spill requests: 0
[state-dump] - cumulative restore requests: 0
[state-dump] - spilled objects pending delete: 0
[state-dump] 
[state-dump] ObjectManager:
[state-dump] - num local objects: 0
[state-dump] - num unfulfilled push requests: 0
[state-dump] - num object pull requests: 0
[state-dump] - num chunks received total: 0
[state-dump] - num chunks received failed (all): 0
[state-dump] - num chunks received failed / cancelled: 0
[state-dump] - num chunks received failed / plasma error: 0
[state-dump] Event stats:
[state-dump] Global stats: 0 total (0 active)
[state-dump] Queueing time: mean = -nan s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] Execution time:  mean = -nan s, total = 0.000 s
[state-dump] Event stats:
[state-dump] PushManager:
[state-dump] - num pushes in flight: 0
[state-dump] - num chunks in flight: 0
[state-dump] - num chunks remaining: 0
[state-dump] - max chunks allowed: 409
[state-dump] OwnershipBasedObjectDirectory:
[state-dump] - num listeners: 0
[state-dump] - cumulative location updates: 0
[state-dump] - num location updates per second: 0.000
[state-dump] - num location lookups per second: 0.000
[state-dump] - num locations added per second: 0.000
[state-dump] - num locations removed per second: 0.000
[state-dump] BufferPool:
[state-dump] - create buffer state map size: 0
[state-dump] PullManager:
[state-dump] - num bytes available for pulled objects: 68719476736
[state-dump] - num bytes being pulled (all): 0
[state-dump] - num bytes being pulled / pinned: 0
[state-dump] - get request bundles: BundlePullRequestQueue{0 total, 0 active, 0 inactive, 0 unpullable}
[state-dump] - wait request bundles: BundlePullRequestQueue{0 total, 0 active, 0 inactive, 0 unpullable}
[state-dump] - task request bundles: BundlePullRequestQueue{0 total, 0 active, 0 inactive, 0 unpullable}
[state-dump] - first get request bundle: N/A
[state-dump] - first wait request bundle: N/A
[state-dump] - first task request bundle: N/A
[state-dump] - num objects queued: 0
[state-dump] - num objects actively pulled (all): 0
[state-dump] - num objects actively pulled / pinned: 0
[state-dump] - num bundles being pulled: 0
[state-dump] - num pull retries: 0
[state-dump] - max timeout seconds: 0
[state-dump] - max timeout request is already processed. No entry.
[state-dump] 
[state-dump] WorkerPool:
[state-dump] - registered jobs: 0
[state-dump] - process_failed_job_config_missing: 0
[state-dump] - process_failed_rate_limited: 0
[state-dump] - process_failed_pending_registration: 0
[state-dump] - process_failed_runtime_env_setup_failed: 0
[state-dump] - num PYTHON workers: 0
[state-dump] - num PYTHON drivers: 0
[state-dump] - num PYTHON pending start requests: 0
[state-dump] - num PYTHON pending registration requests: 0
[state-dump] - num object spill callbacks queued: 0
[state-dump] - num object restore queued: 0
[state-dump] - num util functions queued: 0
[state-dump] - num JAVA workers: 0
[state-dump] - num JAVA drivers: 0
[state-dump] - num JAVA pending start requests: 0
[state-dump] - num JAVA pending registration requests: 0
[state-dump] - num object spill callbacks queued: 0
[state-dump] - num object restore queued: 0
[state-dump] - num util functions queued: 0
[state-dump] - num idle workers: 0
[state-dump] TaskDependencyManager:
[state-dump] - task deps map size: 0
[state-dump] - get req map size: 0
[state-dump] - wait req map size: 0
[state-dump] - local objects map size: 0
[state-dump] WaitManager:
[state-dump] - num active wait requests: 0
[state-dump] Subscriber:
[state-dump] Channel WORKER_OBJECT_LOCATIONS_CHANNEL
[state-dump] - cumulative subscribe requests: 0
[state-dump] - cumulative unsubscribe requests: 0
[state-dump] - active subscribed publishers: 0
[state-dump] - cumulative published messages: 0
[state-dump] - cumulative processed messages: 0
[state-dump] Channel WORKER_REF_REMOVED_CHANNEL
[state-dump] - cumulative subscribe requests: 0
[state-dump] - cumulative unsubscribe requests: 0
[state-dump] - active subscribed publishers: 0
[state-dump] - cumulative published messages: 0
[state-dump] - cumulative processed messages: 0
[state-dump] Channel WORKER_OBJECT_EVICTION
[state-dump] - cumulative subscribe requests: 0
[state-dump] - cumulative unsubscribe requests: 0
[state-dump] - active subscribed publishers: 0
[state-dump] - cumulative published messages: 0
[state-dump] - cumulative processed messages: 0
[state-dump] num async plasma notifications: 0
[state-dump] Event stats:
[state-dump] Global stats: 29 total (14 active)
[state-dump] Queueing time: mean = 1.393 ms, max = 8.780 ms, min = 22.270 us, total = 40.398 ms
[state-dump] Execution time:  mean = 686.584 us, total = 19.911 ms
[state-dump] Event stats:
[state-dump] 	PeriodicalRunner.RunFnPeriodically - 12 total (2 active, 1 running), Execution time: mean = 131.662 us, total = 1.580 ms, Queueing time: mean = 3.361 ms, max = 8.780 ms, min = 28.507 us, total = 40.337 ms
[state-dump] 	ray::rpc::InternalKVGcsService.grpc_client.GetInternalConfig.OnReplyReceived - 1 total (0 active), Execution time: mean = 15.242 ms, total = 15.242 ms, Queueing time: mean = 39.430 us, max = 39.430 us, min = 39.430 us, total = 39.430 us
[state-dump] 	NodeManager.deadline_timer.flush_free_objects - 1 total (1 active), Execution time: mean = 0.000 s, total = 0.000 s, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	NodeManager.GCTaskFailureReason - 1 total (1 active), Execution time: mean = 0.000 s, total = 0.000 s, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	ray::rpc::InternalKVGcsService.grpc_client.GetInternalConfig - 1 total (0 active), Execution time: mean = 1.084 ms, total = 1.084 ms, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	ray::rpc::InternalPubSubGcsService.grpc_client.GcsSubscriberCommandBatch.OnReplyReceived - 1 total (1 active), Execution time: mean = 0.000 s, total = 0.000 s, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	NodeManager.ScheduleAndDispatchTasks - 1 total (1 active), Execution time: mean = 0.000 s, total = 0.000 s, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	ray::rpc::InternalPubSubGcsService.grpc_client.GcsSubscriberPoll - 1 total (1 active), Execution time: mean = 0.000 s, total = 0.000 s, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	ray::rpc::InternalPubSubGcsService.grpc_client.GcsSubscriberCommandBatch - 1 total (0 active), Execution time: mean = 656.919 us, total = 656.919 us, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	NodeManager.deadline_timer.spill_objects_when_over_threshold - 1 total (1 active), Execution time: mean = 0.000 s, total = 0.000 s, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	ClusterResourceManager.ResetRemoteNodeView - 1 total (1 active), Execution time: mean = 0.000 s, total = 0.000 s, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	RayletWorkerPool.deadline_timer.kill_idle_workers - 1 total (1 active), Execution time: mean = 0.000 s, total = 0.000 s, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	NodeManager.deadline_timer.debug_state_dump - 1 total (1 active), Execution time: mean = 0.000 s, total = 0.000 s, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	MemoryMonitor.CheckIsMemoryUsageAboveThreshold - 1 total (1 active), Execution time: mean = 0.000 s, total = 0.000 s, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	NodeManager.CheckForUnexpectedWorkerDisconnects - 1 total (1 active), Execution time: mean = 0.000 s, total = 0.000 s, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	NodeManager.deadline_timer.record_metrics - 1 total (1 active), Execution time: mean = 0.000 s, total = 0.000 s, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	ray::rpc::NodeInfoGcsService.grpc_client.RegisterNode - 1 total (0 active), Execution time: mean = 1.113 ms, total = 1.113 ms, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	ray::rpc::NodeInfoGcsService.grpc_client.RegisterNode.OnReplyReceived - 1 total (0 active), Execution time: mean = 234.981 us, total = 234.981 us, Queueing time: mean = 22.270 us, max = 22.270 us, min = 22.270 us, total = 22.270 us
[state-dump] DebugString() time ms: 0
[state-dump] 
[state-dump] 
[2025-06-25 17:17:55,197 I 1412746 1412746] (raylet) accessor.cc:768: Received notification for node, IsAlive = 1 node_id=c3065f0a91075821625dad700db11cc1befa3100fb68c9da57a85f4b
[2025-06-25 17:17:55,655 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412942, the token is 0
[2025-06-25 17:17:55,657 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412943, the token is 1
[2025-06-25 17:17:55,660 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412944, the token is 2
[2025-06-25 17:17:55,662 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412945, the token is 3
[2025-06-25 17:17:55,665 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412946, the token is 4
[2025-06-25 17:17:55,667 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412947, the token is 5
[2025-06-25 17:17:55,669 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412949, the token is 6
[2025-06-25 17:17:55,671 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412950, the token is 7
[2025-06-25 17:17:55,673 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412953, the token is 8
[2025-06-25 17:17:55,675 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412954, the token is 9
[2025-06-25 17:17:55,677 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412955, the token is 10
[2025-06-25 17:17:55,680 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412956, the token is 11
[2025-06-25 17:17:55,682 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412957, the token is 12
[2025-06-25 17:17:55,685 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412958, the token is 13
[2025-06-25 17:17:55,688 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412959, the token is 14
[2025-06-25 17:17:55,690 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412960, the token is 15
[2025-06-25 17:17:55,693 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412961, the token is 16
[2025-06-25 17:17:55,695 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412962, the token is 17
[2025-06-25 17:17:55,698 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412963, the token is 18
[2025-06-25 17:17:55,700 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412964, the token is 19
[2025-06-25 17:17:55,703 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412965, the token is 20
[2025-06-25 17:17:55,705 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412967, the token is 21
[2025-06-25 17:17:55,707 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412968, the token is 22
[2025-06-25 17:17:55,710 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412969, the token is 23
[2025-06-25 17:17:55,712 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412970, the token is 24
[2025-06-25 17:17:55,714 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412971, the token is 25
[2025-06-25 17:17:55,717 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412972, the token is 26
[2025-06-25 17:17:55,719 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412973, the token is 27
[2025-06-25 17:17:55,722 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412974, the token is 28
[2025-06-25 17:17:55,724 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412975, the token is 29
[2025-06-25 17:17:55,727 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412976, the token is 30
[2025-06-25 17:17:55,730 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412977, the token is 31
[2025-06-25 17:17:55,732 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412978, the token is 32
[2025-06-25 17:17:55,735 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412979, the token is 33
[2025-06-25 17:17:55,737 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412980, the token is 34
[2025-06-25 17:17:55,740 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412981, the token is 35
[2025-06-25 17:17:55,743 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412983, the token is 36
[2025-06-25 17:17:55,746 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412984, the token is 37
[2025-06-25 17:17:55,749 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412986, the token is 38
[2025-06-25 17:17:55,751 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412987, the token is 39
[2025-06-25 17:17:55,754 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412988, the token is 40
[2025-06-25 17:17:55,757 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412990, the token is 41
[2025-06-25 17:17:55,760 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412993, the token is 42
[2025-06-25 17:17:55,763 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412994, the token is 43
[2025-06-25 17:17:55,766 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412995, the token is 44
[2025-06-25 17:17:55,769 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412996, the token is 45
[2025-06-25 17:17:55,772 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412998, the token is 46
[2025-06-25 17:17:55,776 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1412999, the token is 47
[2025-06-25 17:17:55,779 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413000, the token is 48
[2025-06-25 17:17:55,782 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413001, the token is 49
[2025-06-25 17:17:55,785 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413003, the token is 50
[2025-06-25 17:17:55,788 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413004, the token is 51
[2025-06-25 17:17:55,791 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413005, the token is 52
[2025-06-25 17:17:55,794 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413006, the token is 53
[2025-06-25 17:17:55,797 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413007, the token is 54
[2025-06-25 17:17:55,800 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413008, the token is 55
[2025-06-25 17:17:55,803 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413009, the token is 56
[2025-06-25 17:17:55,806 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413010, the token is 57
[2025-06-25 17:17:55,809 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413011, the token is 58
[2025-06-25 17:17:55,812 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413012, the token is 59
[2025-06-25 17:17:55,815 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413013, the token is 60
[2025-06-25 17:17:55,818 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413014, the token is 61
[2025-06-25 17:17:55,821 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413015, the token is 62
[2025-06-25 17:17:55,825 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413016, the token is 63
[2025-06-25 17:17:55,828 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413017, the token is 64
[2025-06-25 17:17:55,831 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413018, the token is 65
[2025-06-25 17:17:55,834 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413019, the token is 66
[2025-06-25 17:17:55,838 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413020, the token is 67
[2025-06-25 17:17:55,841 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413021, the token is 68
[2025-06-25 17:17:55,844 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413022, the token is 69
[2025-06-25 17:17:55,848 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413023, the token is 70
[2025-06-25 17:17:55,851 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413024, the token is 71
[2025-06-25 17:17:55,855 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413025, the token is 72
[2025-06-25 17:17:55,859 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413026, the token is 73
[2025-06-25 17:17:55,862 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413027, the token is 74
[2025-06-25 17:17:55,866 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413028, the token is 75
[2025-06-25 17:17:55,869 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413029, the token is 76
[2025-06-25 17:17:55,873 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413030, the token is 77
[2025-06-25 17:17:55,876 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413031, the token is 78
[2025-06-25 17:17:55,880 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413032, the token is 79
[2025-06-25 17:17:55,884 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413033, the token is 80
[2025-06-25 17:17:55,887 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413034, the token is 81
[2025-06-25 17:17:55,911 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413037, the token is 82
[2025-06-25 17:17:55,914 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413038, the token is 83
[2025-06-25 17:17:55,919 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413039, the token is 84
[2025-06-25 17:17:55,923 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413040, the token is 85
[2025-06-25 17:17:55,927 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413041, the token is 86
[2025-06-25 17:17:55,931 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413042, the token is 87
[2025-06-25 17:17:55,935 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413043, the token is 88
[2025-06-25 17:17:55,939 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413044, the token is 89
[2025-06-25 17:17:55,943 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413045, the token is 90
[2025-06-25 17:17:55,948 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413046, the token is 91
[2025-06-25 17:17:55,952 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413048, the token is 92
[2025-06-25 17:17:55,956 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413049, the token is 93
[2025-06-25 17:17:55,960 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413050, the token is 94
[2025-06-25 17:17:55,965 I 1412746 1412746] (raylet) worker_pool.cc:527: Started worker process with pid 1413051, the token is 95
[2025-06-25 17:17:56,072 I 1412746 1412784] (raylet) object_store.cc:38: Object store current usage 8e-09 / 68.7195 GB.
[2025-06-25 17:17:56,629 I 1412746 1412746] (raylet) worker_pool.cc:724: Job 01000000 already started in worker pool.
[2025-06-25 17:18:00,218 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:18:05,192 W 1412746 1412778] (raylet) metric_exporter.cc:105: [1] Export metrics to agent failed: RpcError: RPC Error message: failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:64651: Failed to connect to remote host: Connection refused; RPC Error details:  rpc_code: 14. This won't affect Ray, but you can lose metrics from the cluster.
[2025-06-25 17:18:05,222 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:18:10,226 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:18:15,228 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:18:20,231 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:18:25,235 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:18:30,239 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:18:35,242 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:18:40,245 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:18:45,248 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:18:50,252 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:18:55,180 I 1412746 1412784] (raylet) store.cc:576: Plasma store debug dump: 
Current usage: 0 / 68.7195 GB
- num bytes created total: 776
0 pending objects of total size 0MB
- objects spillable: 0
- bytes spillable: 0
- objects unsealed: 0
- bytes unsealed: 0
- objects in use: 0
- bytes in use: 0
- objects evictable: 0
- bytes evictable: 0

- objects created by worker: 0
- bytes created by worker: 0
- objects restored: 0
- bytes restored: 0
- objects received: 0
- bytes received: 0
- objects errored: 0
- bytes errored: 0

[2025-06-25 17:18:55,197 I 1412746 1412746] (raylet) node_manager.cc:450: [state-dump] NodeManager:
[state-dump] Node ID: c3065f0a91075821625dad700db11cc1befa3100fb68c9da57a85f4b
[state-dump] Node name: 192.168.10.101
[state-dump] InitialConfigResources: {CPU: 96, object_store_memory: 6.87195e+10, accelerator_type:G: 1, GPU: 8, node:192.168.10.101: 1, memory: 1.72942e+11, node:__internal_head__: 1}
[state-dump] ClusterTaskManager:
[state-dump] ========== Node: c3065f0a91075821625dad700db11cc1befa3100fb68c9da57a85f4b =================
[state-dump] Infeasible queue length: 0
[state-dump] Schedule queue length: 0
[state-dump] Dispatch queue length: 0
[state-dump] num_waiting_for_resource: 0
[state-dump] num_waiting_for_plasma_memory: 0
[state-dump] num_waiting_for_remote_node_resources: 0
[state-dump] num_worker_not_started_by_job_config_not_exist: 0
[state-dump] num_worker_not_started_by_registration_timeout: 0
[state-dump] num_tasks_waiting_for_workers: 0
[state-dump] num_cancelled_tasks: 0
[state-dump] cluster_resource_scheduler state: 
[state-dump] Local id: 3089504843952496296 Local resources: {"total":{node:__internal_head__: [10000], CPU: [960000], memory: [1729419591680000], object_store_memory: [687194767360000], node:192.168.10.101: [10000], accelerator_type:G: [10000], GPU: [10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000]}}, "available": {node:192.168.10.101: [10000], accelerator_type:G: [10000], GPU: [0, 0, 0, 0, 0, 0, 0, 0], CPU: [880000], node:__internal_head__: [10000], memory: [1729419591680000], object_store_memory: [687194767360000]}}, "labels":{"ray.io/node_id":"c3065f0a91075821625dad700db11cc1befa3100fb68c9da57a85f4b",} is_draining: 0 is_idle: 0 Cluster resources (at most 20 nodes are shown): node id: 3089504843952496296{"total":{accelerator_type:G: 10000, GPU: 80000, CPU: 960000, object_store_memory: 687194767360000, node:192.168.10.101: 10000, memory: 1729419591680000, node:__internal_head__: 10000}}, "available": {memory: 1729419591680000, node:__internal_head__: 10000, accelerator_type:G: 10000, object_store_memory: 687194767360000, CPU: 880000, node:192.168.10.101: 10000}}, "labels":{"ray.io/node_id":"c3065f0a91075821625dad700db11cc1befa3100fb68c9da57a85f4b",}, "is_draining": 0, "draining_deadline_timestamp_ms": -1} { "placement group locations": [], "node to bundles": []}
[state-dump] Waiting tasks size: 0
[state-dump] Number of executing tasks: 0
[state-dump] Number of pinned task arguments: 0
[state-dump] Number of total spilled tasks: 0
[state-dump] Number of spilled waiting tasks: 0
[state-dump] Number of spilled unschedulable tasks: 0
[state-dump] Resource usage {
[state-dump]     - (language=PYTHON actor_or_task=XDiTWorker.__init__ pid=1412943 worker_id=3d251100376b2a11709a5f9e62446b6489df8d9c136c7110db6f66ec): {CPU: 1, GPU: 1}
[state-dump]     - (language=PYTHON actor_or_task=XDiTWorker.__init__ pid=1412945 worker_id=99106b7c6a0e4c9f67770b0a311d40ea69a3c2a3a8073779b2171f56): {CPU: 1, GPU: 1}
[state-dump]     - (language=PYTHON actor_or_task=XDiTWorker.__init__ pid=1412955 worker_id=90af73dab71f69691c7b9ae8e3f7551ed2f44584052fbdb01fe5b34b): {CPU: 1, GPU: 1}
[state-dump]     - (language=PYTHON actor_or_task=XDiTWorker.__init__ pid=1412958 worker_id=47434baa29d4649191f8ef66612fe3b5b79786a91dae93615d8a7f74): {GPU: 1, CPU: 1}
[state-dump]     - (language=PYTHON actor_or_task=XDiTWorker.__init__ pid=1412949 worker_id=3797b72263f2acfea34665de8bb2a29d883adbf97b03e7348f868e1c): {CPU: 1, GPU: 1}
[state-dump]     - (language=PYTHON actor_or_task=XDiTWorker.__init__ pid=1412965 worker_id=4502209fa338fc7cbdb9afc95d1539c25f3fe23ca5ccd054a6ecc677): {GPU: 1, CPU: 1}
[state-dump]     - (language=PYTHON actor_or_task=XDiTWorker.__init__ pid=1412960 worker_id=80f2de78ef35c57f741af61456e5d3a45466df25c003c93d5f59315f): {CPU: 1, GPU: 1}
[state-dump]     - (language=PYTHON actor_or_task=XDiTWorker.__init__ pid=1412957 worker_id=8dca9a155c99a3e84b38827c052f68988a74457d6aa8d84bfd9bc5dc): {GPU: 1, CPU: 1}
[state-dump] }
[state-dump] Backlog Size per scheduling descriptor :{workerId: num backlogs}:
[state-dump] 
[state-dump] Running tasks by scheduling class:
[state-dump] ==================================================
[state-dump] 
[state-dump] ClusterResources:
[state-dump] LocalObjectManager:
[state-dump] - num pinned objects: 0
[state-dump] - pinned objects size: 0
[state-dump] - num objects pending restore: 0
[state-dump] - num objects pending spill: 0
[state-dump] - num bytes pending spill: 0
[state-dump] - num bytes currently spilled: 0
[state-dump] - cumulative spill requests: 0
[state-dump] - cumulative restore requests: 0
[state-dump] - spilled objects pending delete: 0
[state-dump] 
[state-dump] ObjectManager:
[state-dump] - num local objects: 0
[state-dump] - num unfulfilled push requests: 0
[state-dump] - num object pull requests: 0
[state-dump] - num chunks received total: 0
[state-dump] - num chunks received failed (all): 0
[state-dump] - num chunks received failed / cancelled: 0
[state-dump] - num chunks received failed / plasma error: 0
[state-dump] Event stats:
[state-dump] Global stats: 0 total (0 active)
[state-dump] Queueing time: mean = -nan s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] Execution time:  mean = -nan s, total = 0.000 s
[state-dump] Event stats:
[state-dump] PushManager:
[state-dump] - num pushes in flight: 0
[state-dump] - num chunks in flight: 0
[state-dump] - num chunks remaining: 0
[state-dump] - max chunks allowed: 409
[state-dump] OwnershipBasedObjectDirectory:
[state-dump] - num listeners: 0
[state-dump] - cumulative location updates: 0
[state-dump] - num location updates per second: 0.000
[state-dump] - num location lookups per second: 0.000
[state-dump] - num locations added per second: 0.000
[state-dump] - num locations removed per second: 0.000
[state-dump] BufferPool:
[state-dump] - create buffer state map size: 0
[state-dump] PullManager:
[state-dump] - num bytes available for pulled objects: 68719476736
[state-dump] - num bytes being pulled (all): 0
[state-dump] - num bytes being pulled / pinned: 0
[state-dump] - get request bundles: BundlePullRequestQueue{0 total, 0 active, 0 inactive, 0 unpullable}
[state-dump] - wait request bundles: BundlePullRequestQueue{0 total, 0 active, 0 inactive, 0 unpullable}
[state-dump] - task request bundles: BundlePullRequestQueue{0 total, 0 active, 0 inactive, 0 unpullable}
[state-dump] - first get request bundle: N/A
[state-dump] - first wait request bundle: N/A
[state-dump] - first task request bundle: N/A
[state-dump] - num objects queued: 0
[state-dump] - num objects actively pulled (all): 0
[state-dump] - num objects actively pulled / pinned: 0
[state-dump] - num bundles being pulled: 0
[state-dump] - num pull retries: 0
[state-dump] - max timeout seconds: 0
[state-dump] - max timeout request is already processed. No entry.
[state-dump] 
[state-dump] WorkerPool:
[state-dump] - registered jobs: 1
[state-dump] - process_failed_job_config_missing: 0
[state-dump] - process_failed_rate_limited: 0
[state-dump] - process_failed_pending_registration: 0
[state-dump] - process_failed_runtime_env_setup_failed: 0
[state-dump] - num PYTHON workers: 96
[state-dump] - num PYTHON drivers: 1
[state-dump] - num PYTHON pending start requests: 0
[state-dump] - num PYTHON pending registration requests: 0
[state-dump] - num object spill callbacks queued: 0
[state-dump] - num object restore queued: 0
[state-dump] - num util functions queued: 0
[state-dump] - num JAVA workers: 0
[state-dump] - num JAVA drivers: 0
[state-dump] - num JAVA pending start requests: 0
[state-dump] - num JAVA pending registration requests: 0
[state-dump] - num object spill callbacks queued: 0
[state-dump] - num object restore queued: 0
[state-dump] - num util functions queued: 0
[state-dump] - num idle workers: 88
[state-dump] TaskDependencyManager:
[state-dump] - task deps map size: 0
[state-dump] - get req map size: 0
[state-dump] - wait req map size: 0
[state-dump] - local objects map size: 0
[state-dump] WaitManager:
[state-dump] - num active wait requests: 0
[state-dump] Subscriber:
[state-dump] Channel WORKER_OBJECT_LOCATIONS_CHANNEL
[state-dump] - cumulative subscribe requests: 0
[state-dump] - cumulative unsubscribe requests: 0
[state-dump] - active subscribed publishers: 0
[state-dump] - cumulative published messages: 0
[state-dump] - cumulative processed messages: 0
[state-dump] Channel WORKER_REF_REMOVED_CHANNEL
[state-dump] - cumulative subscribe requests: 0
[state-dump] - cumulative unsubscribe requests: 0
[state-dump] - active subscribed publishers: 0
[state-dump] - cumulative published messages: 0
[state-dump] - cumulative processed messages: 0
[state-dump] Channel WORKER_OBJECT_EVICTION
[state-dump] - cumulative subscribe requests: 0
[state-dump] - cumulative unsubscribe requests: 0
[state-dump] - active subscribed publishers: 0
[state-dump] - cumulative published messages: 0
[state-dump] - cumulative processed messages: 0
[state-dump] num async plasma notifications: 0
[state-dump] Event stats:
[state-dump] Global stats: 15564 total (112 active)
[state-dump] Queueing time: mean = 7.246 ms, max = 10.510 s, min = -0.000 s, total = 112.770 s
[state-dump] Execution time:  mean = 193.795 us, total = 3.016 s
[state-dump] Event stats:
[state-dump] 	NodeManagerService.grpc_server.ReportWorkerBacklog - 5745 total (0 active), Execution time: mean = 176.217 us, total = 1.012 s, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	NodeManagerService.grpc_server.ReportWorkerBacklog.HandleRequestImpl - 5745 total (0 active), Execution time: mean = 15.579 us, total = 89.500 ms, Queueing time: mean = 26.927 us, max = 1.617 ms, min = 2.511 us, total = 154.693 ms
[state-dump] 	NodeManager.CheckGC - 598 total (1 active), Execution time: mean = 1.650 us, total = 986.499 us, Queueing time: mean = 496.733 us, max = 267.454 ms, min = 7.695 us, total = 297.046 ms
[state-dump] 	RaySyncer.OnDemandBroadcasting - 598 total (1 active), Execution time: mean = 6.721 us, total = 4.019 ms, Queueing time: mean = 491.934 us, max = 267.446 ms, min = -0.000 s, total = 294.177 ms
[state-dump] 	ObjectManager.UpdateAvailableMemory - 597 total (0 active), Execution time: mean = 2.329 us, total = 1.391 ms, Queueing time: mean = 31.162 us, max = 3.267 ms, min = 3.427 us, total = 18.604 ms
[state-dump] 	ClientConnection.async_read.ProcessMessageHeader - 436 total (97 active), Execution time: mean = 3.814 us, total = 1.663 ms, Queueing time: mean = 255.646 ms, max = 10.510 s, min = 10.155 us, total = 111.462 s
[state-dump] 	ClientConnection.async_read.ProcessMessage - 339 total (0 active), Execution time: mean = 1.009 ms, total = 341.898 ms, Queueing time: mean = 9.524 us, max = 211.620 us, min = 1.917 us, total = 3.229 ms
[state-dump] 	RayletWorkerPool.deadline_timer.kill_idle_workers - 300 total (1 active), Execution time: mean = 14.283 us, total = 4.285 ms, Queueing time: mean = 616.902 us, max = 171.556 ms, min = -0.000 s, total = 185.071 ms
[state-dump] 	MemoryMonitor.CheckIsMemoryUsageAboveThreshold - 239 total (1 active), Execution time: mean = 134.950 us, total = 32.253 ms, Queueing time: mean = 1.168 ms, max = 271.296 ms, min = -0.000 s, total = 279.194 ms
[state-dump] 	ClientConnection.async_write.DoAsyncWrites - 98 total (0 active), Execution time: mean = 687.888 ns, total = 67.413 us, Queueing time: mean = 24.592 us, max = 141.185 us, min = 8.594 us, total = 2.410 ms
[state-dump] 	ObjectManager.ObjectDeleted - 97 total (0 active), Execution time: mean = 11.465 us, total = 1.112 ms, Queueing time: mean = 47.495 us, max = 498.503 us, min = 13.070 us, total = 4.607 ms
[state-dump] 	NodeManagerService.grpc_server.GetSystemConfig - 97 total (0 active), Execution time: mean = 315.658 us, total = 30.619 ms, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	ObjectManager.ObjectAdded - 97 total (0 active), Execution time: mean = 8.183 us, total = 793.773 us, Queueing time: mean = 33.141 us, max = 254.797 us, min = 6.663 us, total = 3.215 ms
[state-dump] 	NodeManagerService.grpc_server.GetSystemConfig.HandleRequestImpl - 97 total (0 active), Execution time: mean = 21.951 us, total = 2.129 ms, Queueing time: mean = 28.149 us, max = 186.585 us, min = 4.908 us, total = 2.730 ms
[state-dump] 	NodeManager.ScheduleAndDispatchTasks - 61 total (1 active), Execution time: mean = 7.333 us, total = 447.333 us, Queueing time: mean = 25.303 us, max = 75.419 us, min = 8.871 us, total = 1.543 ms
[state-dump] 	NodeManagerService.grpc_server.GetResourceLoad.HandleRequestImpl - 60 total (0 active), Execution time: mean = 65.548 us, total = 3.933 ms, Queueing time: mean = 24.553 us, max = 61.602 us, min = 8.511 us, total = 1.473 ms
[state-dump] 	NodeManager.CheckForUnexpectedWorkerDisconnects - 60 total (1 active), Execution time: mean = 64.922 us, total = 3.895 ms, Queueing time: mean = 20.027 us, max = 67.613 us, min = -0.000 s, total = 1.202 ms
[state-dump] 	NodeManagerService.grpc_server.GetResourceLoad - 60 total (0 active), Execution time: mean = 237.485 us, total = 14.249 ms, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	NodeManager.deadline_timer.flush_free_objects - 60 total (1 active), Execution time: mean = 3.505 us, total = 210.275 us, Queueing time: mean = 85.853 us, max = 1.287 ms, min = 8.912 us, total = 5.151 ms
[state-dump] 	NodeManager.deadline_timer.spill_objects_when_over_threshold - 60 total (1 active), Execution time: mean = 1.814 us, total = 108.832 us, Queueing time: mean = 87.267 us, max = 1.284 ms, min = 6.446 us, total = 5.236 ms
[state-dump] 	ClusterResourceManager.ResetRemoteNodeView - 21 total (1 active), Execution time: mean = 2.714 us, total = 56.990 us, Queueing time: mean = 21.403 us, max = 49.245 us, min = 9.618 us, total = 449.459 us
[state-dump] 	PeriodicalRunner.RunFnPeriodically - 14 total (0 active), Execution time: mean = 153.356 us, total = 2.147 ms, Queueing time: mean = 3.139 ms, max = 8.780 ms, min = 28.507 us, total = 43.949 ms
[state-dump] 	NodeManager.deadline_timer.record_metrics - 12 total (1 active), Execution time: mean = 256.723 us, total = 3.081 ms, Queueing time: mean = 179.597 us, max = 1.057 ms, min = 16.299 us, total = 2.155 ms
[state-dump] 	RaySyncer.BroadcastMessage - 9 total (0 active), Execution time: mean = 107.004 us, total = 963.036 us, Queueing time: mean = 202.333 ns, max = 476.000 ns, min = 67.000 ns, total = 1.821 us
[state-dump] 	 - 9 total (0 active), Execution time: mean = 455.222 ns, total = 4.097 us, Queueing time: mean = 36.859 us, max = 104.566 us, min = 17.093 us, total = 331.732 us
[state-dump] 	NodeManagerService.grpc_server.RequestWorkerLease.HandleRequestImpl - 8 total (0 active), Execution time: mean = 91.695 us, total = 733.562 us, Queueing time: mean = 19.210 us, max = 24.229 us, min = 14.724 us, total = 153.682 us
[state-dump] 	WorkerPool.PopWorkerCallback - 8 total (0 active), Execution time: mean = 19.403 us, total = 155.223 us, Queueing time: mean = 5.593 us, max = 6.692 us, min = 4.868 us, total = 44.743 us
[state-dump] 	NodeManagerService.grpc_server.RequestWorkerLease - 8 total (0 active), Execution time: mean = 261.558 us, total = 2.092 ms, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	NodeManager.deadline_timer.debug_state_dump - 6 total (1 active), Execution time: mean = 821.316 us, total = 4.928 ms, Queueing time: mean = 22.750 us, max = 48.567 us, min = 10.295 us, total = 136.498 us
[state-dump] 	ray::rpc::InternalPubSubGcsService.grpc_client.GcsSubscriberPoll - 2 total (1 active), Execution time: mean = 717.121 ms, total = 1.434 s, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	ray::rpc::InternalPubSubGcsService.grpc_client.GcsSubscriberCommandBatch - 2 total (0 active), Execution time: mean = 597.795 us, total = 1.196 ms, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	ray::rpc::InternalPubSubGcsService.grpc_client.GcsSubscriberCommandBatch.OnReplyReceived - 2 total (0 active), Execution time: mean = 105.510 us, total = 211.019 us, Queueing time: mean = 759.244 us, max = 1.502 ms, min = 16.763 us, total = 1.518 ms
[state-dump] 	RaySyncerRegister - 2 total (0 active), Execution time: mean = 1.485 us, total = 2.970 us, Queueing time: mean = 160.000 ns, max = 208.000 ns, min = 112.000 ns, total = 320.000 ns
[state-dump] 	ray::rpc::NodeInfoGcsService.grpc_client.RegisterNode.OnReplyReceived - 1 total (0 active), Execution time: mean = 234.981 us, total = 234.981 us, Queueing time: mean = 22.270 us, max = 22.270 us, min = 22.270 us, total = 22.270 us
[state-dump] 	ray::rpc::JobInfoGcsService.grpc_client.AddJob.OnReplyReceived - 1 total (0 active), Execution time: mean = 28.433 us, total = 28.433 us, Queueing time: mean = 155.737 us, max = 155.737 us, min = 155.737 us, total = 155.737 us
[state-dump] 	NodeManager.GcsCheckAlive - 1 total (1 active), Execution time: mean = 0.000 s, total = 0.000 s, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	ray::rpc::NodeInfoGcsService.grpc_client.CheckAlive.OnReplyReceived - 1 total (0 active), Execution time: mean = 14.754 us, total = 14.754 us, Queueing time: mean = 22.319 us, max = 22.319 us, min = 22.319 us, total = 22.319 us
[state-dump] 	ray::rpc::NodeInfoGcsService.grpc_client.RegisterNode - 1 total (0 active), Execution time: mean = 1.113 ms, total = 1.113 ms, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	ray::rpc::JobInfoGcsService.grpc_client.GetAllJobInfo - 1 total (0 active), Execution time: mean = 500.844 us, total = 500.844 us, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	ray::rpc::NodeInfoGcsService.grpc_client.GetAllNodeInfo.OnReplyReceived - 1 total (0 active), Execution time: mean = 120.718 us, total = 120.718 us, Queueing time: mean = 17.721 us, max = 17.721 us, min = 17.721 us, total = 17.721 us
[state-dump] 	ray::rpc::JobInfoGcsService.grpc_client.AddJob - 1 total (0 active), Execution time: mean = 660.368 us, total = 660.368 us, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	ray::rpc::JobInfoGcsService.grpc_client.GetAllJobInfo.OnReplyReceived - 1 total (0 active), Execution time: mean = 13.212 us, total = 13.212 us, Queueing time: mean = 16.939 us, max = 16.939 us, min = 16.939 us, total = 16.939 us
[state-dump] 	ray::rpc::NodeInfoGcsService.grpc_client.GetAllNodeInfo - 1 total (0 active), Execution time: mean = 710.941 us, total = 710.941 us, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	ray::rpc::NodeInfoGcsService.grpc_client.CheckAlive - 1 total (0 active), Execution time: mean = 575.760 us, total = 575.760 us, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	ray::rpc::InternalKVGcsService.grpc_client.GetInternalConfig - 1 total (0 active), Execution time: mean = 1.084 ms, total = 1.084 ms, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	ray::rpc::InternalPubSubGcsService.grpc_client.GcsSubscriberPoll.OnReplyReceived - 1 total (0 active), Execution time: mean = 151.102 us, total = 151.102 us, Queueing time: mean = 17.047 us, max = 17.047 us, min = 17.047 us, total = 17.047 us
[state-dump] 	Subscriber.HandlePublishedMessage_GCS_JOB_CHANNEL - 1 total (0 active), Execution time: mean = 42.544 us, total = 42.544 us, Queueing time: mean = 140.637 us, max = 140.637 us, min = 140.637 us, total = 140.637 us
[state-dump] 	ray::rpc::InternalKVGcsService.grpc_client.GetInternalConfig.OnReplyReceived - 1 total (0 active), Execution time: mean = 15.242 ms, total = 15.242 ms, Queueing time: mean = 39.430 us, max = 39.430 us, min = 39.430 us, total = 39.430 us
[state-dump] 	NodeManager.deadline_timer.print_event_loop_stats - 1 total (1 active, 1 running), Execution time: mean = 0.000 s, total = 0.000 s, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	NodeManager.GCTaskFailureReason - 1 total (1 active), Execution time: mean = 0.000 s, total = 0.000 s, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] DebugString() time ms: 1
[state-dump] 
[state-dump] 
[2025-06-25 17:18:55,255 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:19:00,258 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:19:05,262 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:19:10,266 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:19:15,269 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:19:20,273 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:19:25,277 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:19:30,280 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:19:35,283 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:19:40,287 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:19:45,290 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:19:50,294 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:19:55,180 I 1412746 1412784] (raylet) store.cc:576: Plasma store debug dump: 
Current usage: 0 / 68.7195 GB
- num bytes created total: 776
0 pending objects of total size 0MB
- objects spillable: 0
- bytes spillable: 0
- objects unsealed: 0
- bytes unsealed: 0
- objects in use: 0
- bytes in use: 0
- objects evictable: 0
- bytes evictable: 0

- objects created by worker: 0
- bytes created by worker: 0
- objects restored: 0
- bytes restored: 0
- objects received: 0
- bytes received: 0
- objects errored: 0
- bytes errored: 0

[2025-06-25 17:19:55,198 I 1412746 1412746] (raylet) node_manager.cc:450: [state-dump] NodeManager:
[state-dump] Node ID: c3065f0a91075821625dad700db11cc1befa3100fb68c9da57a85f4b
[state-dump] Node name: 192.168.10.101
[state-dump] InitialConfigResources: {CPU: 96, object_store_memory: 6.87195e+10, accelerator_type:G: 1, GPU: 8, node:192.168.10.101: 1, memory: 1.72942e+11, node:__internal_head__: 1}
[state-dump] ClusterTaskManager:
[state-dump] ========== Node: c3065f0a91075821625dad700db11cc1befa3100fb68c9da57a85f4b =================
[state-dump] Infeasible queue length: 0
[state-dump] Schedule queue length: 0
[state-dump] Dispatch queue length: 0
[state-dump] num_waiting_for_resource: 0
[state-dump] num_waiting_for_plasma_memory: 0
[state-dump] num_waiting_for_remote_node_resources: 0
[state-dump] num_worker_not_started_by_job_config_not_exist: 0
[state-dump] num_worker_not_started_by_registration_timeout: 0
[state-dump] num_tasks_waiting_for_workers: 0
[state-dump] num_cancelled_tasks: 0
[state-dump] cluster_resource_scheduler state: 
[state-dump] Local id: 3089504843952496296 Local resources: {"total":{node:__internal_head__: [10000], CPU: [960000], memory: [1729419591680000], object_store_memory: [687194767360000], node:192.168.10.101: [10000], accelerator_type:G: [10000], GPU: [10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000]}}, "available": {node:192.168.10.101: [10000], accelerator_type:G: [10000], GPU: [0, 0, 0, 0, 0, 0, 0, 0], CPU: [880000], node:__internal_head__: [10000], memory: [1729419591680000], object_store_memory: [687194767360000]}}, "labels":{"ray.io/node_id":"c3065f0a91075821625dad700db11cc1befa3100fb68c9da57a85f4b",} is_draining: 0 is_idle: 0 Cluster resources (at most 20 nodes are shown): node id: 3089504843952496296{"total":{accelerator_type:G: 10000, GPU: 80000, CPU: 960000, object_store_memory: 687194767360000, node:192.168.10.101: 10000, memory: 1729419591680000, node:__internal_head__: 10000}}, "available": {memory: 1729419591680000, node:__internal_head__: 10000, accelerator_type:G: 10000, object_store_memory: 687194767360000, CPU: 880000, node:192.168.10.101: 10000}}, "labels":{"ray.io/node_id":"c3065f0a91075821625dad700db11cc1befa3100fb68c9da57a85f4b",}, "is_draining": 0, "draining_deadline_timestamp_ms": -1} { "placement group locations": [], "node to bundles": []}
[state-dump] Waiting tasks size: 0
[state-dump] Number of executing tasks: 0
[state-dump] Number of pinned task arguments: 0
[state-dump] Number of total spilled tasks: 0
[state-dump] Number of spilled waiting tasks: 0
[state-dump] Number of spilled unschedulable tasks: 0
[state-dump] Resource usage {
[state-dump]     - (language=PYTHON actor_or_task=XDiTWorker.__init__ pid=1412943 worker_id=3d251100376b2a11709a5f9e62446b6489df8d9c136c7110db6f66ec): {CPU: 1, GPU: 1}
[state-dump]     - (language=PYTHON actor_or_task=XDiTWorker.__init__ pid=1412945 worker_id=99106b7c6a0e4c9f67770b0a311d40ea69a3c2a3a8073779b2171f56): {CPU: 1, GPU: 1}
[state-dump]     - (language=PYTHON actor_or_task=XDiTWorker.__init__ pid=1412955 worker_id=90af73dab71f69691c7b9ae8e3f7551ed2f44584052fbdb01fe5b34b): {CPU: 1, GPU: 1}
[state-dump]     - (language=PYTHON actor_or_task=XDiTWorker.__init__ pid=1412958 worker_id=47434baa29d4649191f8ef66612fe3b5b79786a91dae93615d8a7f74): {GPU: 1, CPU: 1}
[state-dump]     - (language=PYTHON actor_or_task=XDiTWorker.__init__ pid=1412949 worker_id=3797b72263f2acfea34665de8bb2a29d883adbf97b03e7348f868e1c): {CPU: 1, GPU: 1}
[state-dump]     - (language=PYTHON actor_or_task=XDiTWorker.__init__ pid=1412965 worker_id=4502209fa338fc7cbdb9afc95d1539c25f3fe23ca5ccd054a6ecc677): {GPU: 1, CPU: 1}
[state-dump]     - (language=PYTHON actor_or_task=XDiTWorker.__init__ pid=1412960 worker_id=80f2de78ef35c57f741af61456e5d3a45466df25c003c93d5f59315f): {CPU: 1, GPU: 1}
[state-dump]     - (language=PYTHON actor_or_task=XDiTWorker.__init__ pid=1412957 worker_id=8dca9a155c99a3e84b38827c052f68988a74457d6aa8d84bfd9bc5dc): {GPU: 1, CPU: 1}
[state-dump] }
[state-dump] Backlog Size per scheduling descriptor :{workerId: num backlogs}:
[state-dump] 
[state-dump] Running tasks by scheduling class:
[state-dump] ==================================================
[state-dump] 
[state-dump] ClusterResources:
[state-dump] LocalObjectManager:
[state-dump] - num pinned objects: 0
[state-dump] - pinned objects size: 0
[state-dump] - num objects pending restore: 0
[state-dump] - num objects pending spill: 0
[state-dump] - num bytes pending spill: 0
[state-dump] - num bytes currently spilled: 0
[state-dump] - cumulative spill requests: 0
[state-dump] - cumulative restore requests: 0
[state-dump] - spilled objects pending delete: 0
[state-dump] 
[state-dump] ObjectManager:
[state-dump] - num local objects: 0
[state-dump] - num unfulfilled push requests: 0
[state-dump] - num object pull requests: 0
[state-dump] - num chunks received total: 0
[state-dump] - num chunks received failed (all): 0
[state-dump] - num chunks received failed / cancelled: 0
[state-dump] - num chunks received failed / plasma error: 0
[state-dump] Event stats:
[state-dump] Global stats: 0 total (0 active)
[state-dump] Queueing time: mean = -nan s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] Execution time:  mean = -nan s, total = 0.000 s
[state-dump] Event stats:
[state-dump] PushManager:
[state-dump] - num pushes in flight: 0
[state-dump] - num chunks in flight: 0
[state-dump] - num chunks remaining: 0
[state-dump] - max chunks allowed: 409
[state-dump] OwnershipBasedObjectDirectory:
[state-dump] - num listeners: 0
[state-dump] - cumulative location updates: 0
[state-dump] - num location updates per second: 0.000
[state-dump] - num location lookups per second: 0.000
[state-dump] - num locations added per second: 0.000
[state-dump] - num locations removed per second: 0.000
[state-dump] BufferPool:
[state-dump] - create buffer state map size: 0
[state-dump] PullManager:
[state-dump] - num bytes available for pulled objects: 68719476736
[state-dump] - num bytes being pulled (all): 0
[state-dump] - num bytes being pulled / pinned: 0
[state-dump] - get request bundles: BundlePullRequestQueue{0 total, 0 active, 0 inactive, 0 unpullable}
[state-dump] - wait request bundles: BundlePullRequestQueue{0 total, 0 active, 0 inactive, 0 unpullable}
[state-dump] - task request bundles: BundlePullRequestQueue{0 total, 0 active, 0 inactive, 0 unpullable}
[state-dump] - first get request bundle: N/A
[state-dump] - first wait request bundle: N/A
[state-dump] - first task request bundle: N/A
[state-dump] - num objects queued: 0
[state-dump] - num objects actively pulled (all): 0
[state-dump] - num objects actively pulled / pinned: 0
[state-dump] - num bundles being pulled: 0
[state-dump] - num pull retries: 0
[state-dump] - max timeout seconds: 0
[state-dump] - max timeout request is already processed. No entry.
[state-dump] 
[state-dump] WorkerPool:
[state-dump] - registered jobs: 1
[state-dump] - process_failed_job_config_missing: 0
[state-dump] - process_failed_rate_limited: 0
[state-dump] - process_failed_pending_registration: 0
[state-dump] - process_failed_runtime_env_setup_failed: 0
[state-dump] - num PYTHON workers: 96
[state-dump] - num PYTHON drivers: 1
[state-dump] - num PYTHON pending start requests: 0
[state-dump] - num PYTHON pending registration requests: 0
[state-dump] - num object spill callbacks queued: 0
[state-dump] - num object restore queued: 0
[state-dump] - num util functions queued: 0
[state-dump] - num JAVA workers: 0
[state-dump] - num JAVA drivers: 0
[state-dump] - num JAVA pending start requests: 0
[state-dump] - num JAVA pending registration requests: 0
[state-dump] - num object spill callbacks queued: 0
[state-dump] - num object restore queued: 0
[state-dump] - num util functions queued: 0
[state-dump] - num idle workers: 88
[state-dump] TaskDependencyManager:
[state-dump] - task deps map size: 0
[state-dump] - get req map size: 0
[state-dump] - wait req map size: 0
[state-dump] - local objects map size: 0
[state-dump] WaitManager:
[state-dump] - num active wait requests: 0
[state-dump] Subscriber:
[state-dump] Channel WORKER_OBJECT_LOCATIONS_CHANNEL
[state-dump] - cumulative subscribe requests: 0
[state-dump] - cumulative unsubscribe requests: 0
[state-dump] - active subscribed publishers: 0
[state-dump] - cumulative published messages: 0
[state-dump] - cumulative processed messages: 0
[state-dump] Channel WORKER_REF_REMOVED_CHANNEL
[state-dump] - cumulative subscribe requests: 0
[state-dump] - cumulative unsubscribe requests: 0
[state-dump] - active subscribed publishers: 0
[state-dump] - cumulative published messages: 0
[state-dump] - cumulative processed messages: 0
[state-dump] Channel WORKER_OBJECT_EVICTION
[state-dump] - cumulative subscribe requests: 0
[state-dump] - cumulative unsubscribe requests: 0
[state-dump] - active subscribed publishers: 0
[state-dump] - cumulative published messages: 0
[state-dump] - cumulative processed messages: 0
[state-dump] num async plasma notifications: 0
[state-dump] Event stats:
[state-dump] Global stats: 29941 total (112 active)
[state-dump] Queueing time: mean = 3.775 ms, max = 10.510 s, min = -0.000 s, total = 113.038 s
[state-dump] Execution time:  mean = 140.310 us, total = 4.201 s
[state-dump] Event stats:
[state-dump] 	NodeManagerService.grpc_server.ReportWorkerBacklog - 11564 total (0 active), Execution time: mean = 175.783 us, total = 2.033 s, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	NodeManagerService.grpc_server.ReportWorkerBacklog.HandleRequestImpl - 11564 total (0 active), Execution time: mean = 15.534 us, total = 179.638 ms, Queueing time: mean = 27.540 us, max = 1.617 ms, min = 2.511 us, total = 318.477 ms
[state-dump] 	RaySyncer.OnDemandBroadcasting - 1197 total (1 active), Execution time: mean = 5.397 us, total = 6.460 ms, Queueing time: mean = 266.401 us, max = 267.446 ms, min = -0.000 s, total = 318.882 ms
[state-dump] 	NodeManager.CheckGC - 1197 total (1 active), Execution time: mean = 1.562 us, total = 1.869 ms, Queueing time: mean = 269.987 us, max = 267.454 ms, min = 4.102 us, total = 323.174 ms
[state-dump] 	ObjectManager.UpdateAvailableMemory - 1196 total (0 active), Execution time: mean = 2.273 us, total = 2.719 ms, Queueing time: mean = 28.896 us, max = 3.267 ms, min = 1.507 us, total = 34.560 ms
[state-dump] 	RayletWorkerPool.deadline_timer.kill_idle_workers - 600 total (1 active), Execution time: mean = 13.313 us, total = 7.988 ms, Queueing time: mean = 325.611 us, max = 171.556 ms, min = -0.000 s, total = 195.367 ms
[state-dump] 	MemoryMonitor.CheckIsMemoryUsageAboveThreshold - 479 total (1 active), Execution time: mean = 137.646 us, total = 65.932 ms, Queueing time: mean = 600.554 us, max = 271.296 ms, min = -0.000 s, total = 287.665 ms
[state-dump] 	ClientConnection.async_read.ProcessMessageHeader - 436 total (97 active), Execution time: mean = 3.814 us, total = 1.663 ms, Queueing time: mean = 255.646 ms, max = 10.510 s, min = 10.155 us, total = 111.462 s
[state-dump] 	ClientConnection.async_read.ProcessMessage - 339 total (0 active), Execution time: mean = 1.009 ms, total = 341.898 ms, Queueing time: mean = 9.524 us, max = 211.620 us, min = 1.917 us, total = 3.229 ms
[state-dump] 	NodeManager.ScheduleAndDispatchTasks - 121 total (1 active), Execution time: mean = 6.878 us, total = 832.272 us, Queueing time: mean = 24.690 us, max = 75.419 us, min = 8.871 us, total = 2.987 ms
[state-dump] 	NodeManager.CheckForUnexpectedWorkerDisconnects - 120 total (1 active), Execution time: mean = 67.376 us, total = 8.085 ms, Queueing time: mean = 24.217 us, max = 67.613 us, min = -0.000 s, total = 2.906 ms
[state-dump] 	NodeManager.deadline_timer.spill_objects_when_over_threshold - 120 total (1 active), Execution time: mean = 1.696 us, total = 203.477 us, Queueing time: mean = 88.961 us, max = 1.284 ms, min = 6.446 us, total = 10.675 ms
[state-dump] 	NodeManagerService.grpc_server.GetResourceLoad.HandleRequestImpl - 120 total (0 active), Execution time: mean = 59.161 us, total = 7.099 ms, Queueing time: mean = 25.192 us, max = 67.591 us, min = 8.511 us, total = 3.023 ms
[state-dump] 	NodeManagerService.grpc_server.GetResourceLoad - 120 total (0 active), Execution time: mean = 231.575 us, total = 27.789 ms, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	NodeManager.deadline_timer.flush_free_objects - 120 total (1 active), Execution time: mean = 3.209 us, total = 385.054 us, Queueing time: mean = 87.702 us, max = 1.287 ms, min = 8.912 us, total = 10.524 ms
[state-dump] 	ClientConnection.async_write.DoAsyncWrites - 98 total (0 active), Execution time: mean = 687.888 ns, total = 67.413 us, Queueing time: mean = 24.592 us, max = 141.185 us, min = 8.594 us, total = 2.410 ms
[state-dump] 	ObjectManager.ObjectAdded - 97 total (0 active), Execution time: mean = 8.183 us, total = 793.773 us, Queueing time: mean = 33.141 us, max = 254.797 us, min = 6.663 us, total = 3.215 ms
[state-dump] 	NodeManagerService.grpc_server.GetSystemConfig - 97 total (0 active), Execution time: mean = 315.658 us, total = 30.619 ms, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	NodeManagerService.grpc_server.GetSystemConfig.HandleRequestImpl - 97 total (0 active), Execution time: mean = 21.951 us, total = 2.129 ms, Queueing time: mean = 28.149 us, max = 186.585 us, min = 4.908 us, total = 2.730 ms
[state-dump] 	ObjectManager.ObjectDeleted - 97 total (0 active), Execution time: mean = 11.465 us, total = 1.112 ms, Queueing time: mean = 47.495 us, max = 498.503 us, min = 13.070 us, total = 4.607 ms
[state-dump] 	ClusterResourceManager.ResetRemoteNodeView - 41 total (1 active), Execution time: mean = 2.892 us, total = 118.557 us, Queueing time: mean = 22.701 us, max = 64.109 us, min = 9.618 us, total = 930.726 us
[state-dump] 	NodeManager.deadline_timer.record_metrics - 24 total (1 active), Execution time: mean = 316.823 us, total = 7.604 ms, Queueing time: mean = 129.175 us, max = 1.057 ms, min = 12.800 us, total = 3.100 ms
[state-dump] 	PeriodicalRunner.RunFnPeriodically - 14 total (0 active), Execution time: mean = 153.356 us, total = 2.147 ms, Queueing time: mean = 3.139 ms, max = 8.780 ms, min = 28.507 us, total = 43.949 ms
[state-dump] 	NodeManager.deadline_timer.debug_state_dump - 12 total (1 active), Execution time: mean = 806.863 us, total = 9.682 ms, Queueing time: mean = 26.292 us, max = 79.652 us, min = 10.295 us, total = 315.500 us
[state-dump] 	RaySyncer.BroadcastMessage - 9 total (0 active), Execution time: mean = 107.004 us, total = 963.036 us, Queueing time: mean = 202.333 ns, max = 476.000 ns, min = 67.000 ns, total = 1.821 us
[state-dump] 	 - 9 total (0 active), Execution time: mean = 455.222 ns, total = 4.097 us, Queueing time: mean = 36.859 us, max = 104.566 us, min = 17.093 us, total = 331.732 us
[state-dump] 	NodeManagerService.grpc_server.RequestWorkerLease - 8 total (0 active), Execution time: mean = 261.558 us, total = 2.092 ms, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	NodeManagerService.grpc_server.RequestWorkerLease.HandleRequestImpl - 8 total (0 active), Execution time: mean = 91.695 us, total = 733.562 us, Queueing time: mean = 19.210 us, max = 24.229 us, min = 14.724 us, total = 153.682 us
[state-dump] 	WorkerPool.PopWorkerCallback - 8 total (0 active), Execution time: mean = 19.403 us, total = 155.223 us, Queueing time: mean = 5.593 us, max = 6.692 us, min = 4.868 us, total = 44.743 us
[state-dump] 	NodeManager.deadline_timer.print_event_loop_stats - 2 total (1 active, 1 running), Execution time: mean = 410.007 us, total = 820.015 us, Queueing time: mean = 7.580 us, max = 15.160 us, min = 15.160 us, total = 15.160 us
[state-dump] 	ray::rpc::InternalPubSubGcsService.grpc_client.GcsSubscriberPoll - 2 total (1 active), Execution time: mean = 717.121 ms, total = 1.434 s, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	ray::rpc::InternalPubSubGcsService.grpc_client.GcsSubscriberCommandBatch - 2 total (0 active), Execution time: mean = 597.795 us, total = 1.196 ms, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	ray::rpc::NodeInfoGcsService.grpc_client.CheckAlive - 2 total (0 active), Execution time: mean = 491.010 us, total = 982.021 us, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	NodeManager.GcsCheckAlive - 2 total (1 active), Execution time: mean = 53.962 us, total = 107.923 us, Queueing time: mean = 376.165 us, max = 752.330 us, min = 752.330 us, total = 752.330 us
[state-dump] 	ray::rpc::NodeInfoGcsService.grpc_client.CheckAlive.OnReplyReceived - 2 total (0 active), Execution time: mean = 17.894 us, total = 35.788 us, Queueing time: mean = 23.901 us, max = 25.482 us, min = 22.319 us, total = 47.801 us
[state-dump] 	ray::rpc::InternalPubSubGcsService.grpc_client.GcsSubscriberCommandBatch.OnReplyReceived - 2 total (0 active), Execution time: mean = 105.510 us, total = 211.019 us, Queueing time: mean = 759.244 us, max = 1.502 ms, min = 16.763 us, total = 1.518 ms
[state-dump] 	RaySyncerRegister - 2 total (0 active), Execution time: mean = 1.485 us, total = 2.970 us, Queueing time: mean = 160.000 ns, max = 208.000 ns, min = 112.000 ns, total = 320.000 ns
[state-dump] 	ray::rpc::JobInfoGcsService.grpc_client.AddJob.OnReplyReceived - 1 total (0 active), Execution time: mean = 28.433 us, total = 28.433 us, Queueing time: mean = 155.737 us, max = 155.737 us, min = 155.737 us, total = 155.737 us
[state-dump] 	ray::rpc::NodeInfoGcsService.grpc_client.RegisterNode - 1 total (0 active), Execution time: mean = 1.113 ms, total = 1.113 ms, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	ray::rpc::JobInfoGcsService.grpc_client.GetAllJobInfo - 1 total (0 active), Execution time: mean = 500.844 us, total = 500.844 us, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	ray::rpc::NodeInfoGcsService.grpc_client.GetAllNodeInfo.OnReplyReceived - 1 total (0 active), Execution time: mean = 120.718 us, total = 120.718 us, Queueing time: mean = 17.721 us, max = 17.721 us, min = 17.721 us, total = 17.721 us
[state-dump] 	ray::rpc::NodeInfoGcsService.grpc_client.RegisterNode.OnReplyReceived - 1 total (0 active), Execution time: mean = 234.981 us, total = 234.981 us, Queueing time: mean = 22.270 us, max = 22.270 us, min = 22.270 us, total = 22.270 us
[state-dump] 	ray::rpc::JobInfoGcsService.grpc_client.GetAllJobInfo.OnReplyReceived - 1 total (0 active), Execution time: mean = 13.212 us, total = 13.212 us, Queueing time: mean = 16.939 us, max = 16.939 us, min = 16.939 us, total = 16.939 us
[state-dump] 	ray::rpc::JobInfoGcsService.grpc_client.AddJob - 1 total (0 active), Execution time: mean = 660.368 us, total = 660.368 us, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	ray::rpc::InternalKVGcsService.grpc_client.GetInternalConfig - 1 total (0 active), Execution time: mean = 1.084 ms, total = 1.084 ms, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	ray::rpc::NodeInfoGcsService.grpc_client.GetAllNodeInfo - 1 total (0 active), Execution time: mean = 710.941 us, total = 710.941 us, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	ray::rpc::InternalPubSubGcsService.grpc_client.GcsSubscriberPoll.OnReplyReceived - 1 total (0 active), Execution time: mean = 151.102 us, total = 151.102 us, Queueing time: mean = 17.047 us, max = 17.047 us, min = 17.047 us, total = 17.047 us
[state-dump] 	Subscriber.HandlePublishedMessage_GCS_JOB_CHANNEL - 1 total (0 active), Execution time: mean = 42.544 us, total = 42.544 us, Queueing time: mean = 140.637 us, max = 140.637 us, min = 140.637 us, total = 140.637 us
[state-dump] 	NodeManager.GCTaskFailureReason - 1 total (1 active), Execution time: mean = 0.000 s, total = 0.000 s, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	ray::rpc::InternalKVGcsService.grpc_client.GetInternalConfig.OnReplyReceived - 1 total (0 active), Execution time: mean = 15.242 ms, total = 15.242 ms, Queueing time: mean = 39.430 us, max = 39.430 us, min = 39.430 us, total = 39.430 us
[state-dump] DebugString() time ms: 0
[state-dump] 
[state-dump] 
[2025-06-25 17:19:55,297 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:20:00,300 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:20:05,303 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:20:10,306 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:20:15,309 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:20:20,312 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:20:25,315 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:20:30,319 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:20:35,322 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:20:40,326 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:20:45,330 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:20:50,333 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:20:55,180 I 1412746 1412784] (raylet) store.cc:576: Plasma store debug dump: 
Current usage: 0 / 68.7195 GB
- num bytes created total: 776
0 pending objects of total size 0MB
- objects spillable: 0
- bytes spillable: 0
- objects unsealed: 0
- bytes unsealed: 0
- objects in use: 0
- bytes in use: 0
- objects evictable: 0
- bytes evictable: 0

- objects created by worker: 0
- bytes created by worker: 0
- objects restored: 0
- bytes restored: 0
- objects received: 0
- bytes received: 0
- objects errored: 0
- bytes errored: 0

[2025-06-25 17:20:55,199 I 1412746 1412746] (raylet) node_manager.cc:450: [state-dump] NodeManager:
[state-dump] Node ID: c3065f0a91075821625dad700db11cc1befa3100fb68c9da57a85f4b
[state-dump] Node name: 192.168.10.101
[state-dump] InitialConfigResources: {CPU: 96, object_store_memory: 6.87195e+10, accelerator_type:G: 1, GPU: 8, node:192.168.10.101: 1, memory: 1.72942e+11, node:__internal_head__: 1}
[state-dump] ClusterTaskManager:
[state-dump] ========== Node: c3065f0a91075821625dad700db11cc1befa3100fb68c9da57a85f4b =================
[state-dump] Infeasible queue length: 0
[state-dump] Schedule queue length: 0
[state-dump] Dispatch queue length: 0
[state-dump] num_waiting_for_resource: 0
[state-dump] num_waiting_for_plasma_memory: 0
[state-dump] num_waiting_for_remote_node_resources: 0
[state-dump] num_worker_not_started_by_job_config_not_exist: 0
[state-dump] num_worker_not_started_by_registration_timeout: 0
[state-dump] num_tasks_waiting_for_workers: 0
[state-dump] num_cancelled_tasks: 0
[state-dump] cluster_resource_scheduler state: 
[state-dump] Local id: 3089504843952496296 Local resources: {"total":{node:__internal_head__: [10000], CPU: [960000], memory: [1729419591680000], object_store_memory: [687194767360000], node:192.168.10.101: [10000], accelerator_type:G: [10000], GPU: [10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000]}}, "available": {node:192.168.10.101: [10000], accelerator_type:G: [10000], GPU: [0, 0, 0, 0, 0, 0, 0, 0], CPU: [880000], node:__internal_head__: [10000], memory: [1729419591680000], object_store_memory: [687194767360000]}}, "labels":{"ray.io/node_id":"c3065f0a91075821625dad700db11cc1befa3100fb68c9da57a85f4b",} is_draining: 0 is_idle: 0 Cluster resources (at most 20 nodes are shown): node id: 3089504843952496296{"total":{accelerator_type:G: 10000, GPU: 80000, CPU: 960000, object_store_memory: 687194767360000, node:192.168.10.101: 10000, memory: 1729419591680000, node:__internal_head__: 10000}}, "available": {memory: 1729419591680000, node:__internal_head__: 10000, accelerator_type:G: 10000, object_store_memory: 687194767360000, CPU: 880000, node:192.168.10.101: 10000}}, "labels":{"ray.io/node_id":"c3065f0a91075821625dad700db11cc1befa3100fb68c9da57a85f4b",}, "is_draining": 0, "draining_deadline_timestamp_ms": -1} { "placement group locations": [], "node to bundles": []}
[state-dump] Waiting tasks size: 0
[state-dump] Number of executing tasks: 0
[state-dump] Number of pinned task arguments: 0
[state-dump] Number of total spilled tasks: 0
[state-dump] Number of spilled waiting tasks: 0
[state-dump] Number of spilled unschedulable tasks: 0
[state-dump] Resource usage {
[state-dump]     - (language=PYTHON actor_or_task=XDiTWorker.__init__ pid=1412943 worker_id=3d251100376b2a11709a5f9e62446b6489df8d9c136c7110db6f66ec): {CPU: 1, GPU: 1}
[state-dump]     - (language=PYTHON actor_or_task=XDiTWorker.__init__ pid=1412945 worker_id=99106b7c6a0e4c9f67770b0a311d40ea69a3c2a3a8073779b2171f56): {CPU: 1, GPU: 1}
[state-dump]     - (language=PYTHON actor_or_task=XDiTWorker.__init__ pid=1412955 worker_id=90af73dab71f69691c7b9ae8e3f7551ed2f44584052fbdb01fe5b34b): {CPU: 1, GPU: 1}
[state-dump]     - (language=PYTHON actor_or_task=XDiTWorker.__init__ pid=1412958 worker_id=47434baa29d4649191f8ef66612fe3b5b79786a91dae93615d8a7f74): {GPU: 1, CPU: 1}
[state-dump]     - (language=PYTHON actor_or_task=XDiTWorker.__init__ pid=1412949 worker_id=3797b72263f2acfea34665de8bb2a29d883adbf97b03e7348f868e1c): {CPU: 1, GPU: 1}
[state-dump]     - (language=PYTHON actor_or_task=XDiTWorker.__init__ pid=1412965 worker_id=4502209fa338fc7cbdb9afc95d1539c25f3fe23ca5ccd054a6ecc677): {GPU: 1, CPU: 1}
[state-dump]     - (language=PYTHON actor_or_task=XDiTWorker.__init__ pid=1412960 worker_id=80f2de78ef35c57f741af61456e5d3a45466df25c003c93d5f59315f): {CPU: 1, GPU: 1}
[state-dump]     - (language=PYTHON actor_or_task=XDiTWorker.__init__ pid=1412957 worker_id=8dca9a155c99a3e84b38827c052f68988a74457d6aa8d84bfd9bc5dc): {GPU: 1, CPU: 1}
[state-dump] }
[state-dump] Backlog Size per scheduling descriptor :{workerId: num backlogs}:
[state-dump] 
[state-dump] Running tasks by scheduling class:
[state-dump] ==================================================
[state-dump] 
[state-dump] ClusterResources:
[state-dump] LocalObjectManager:
[state-dump] - num pinned objects: 0
[state-dump] - pinned objects size: 0
[state-dump] - num objects pending restore: 0
[state-dump] - num objects pending spill: 0
[state-dump] - num bytes pending spill: 0
[state-dump] - num bytes currently spilled: 0
[state-dump] - cumulative spill requests: 0
[state-dump] - cumulative restore requests: 0
[state-dump] - spilled objects pending delete: 0
[state-dump] 
[state-dump] ObjectManager:
[state-dump] - num local objects: 0
[state-dump] - num unfulfilled push requests: 0
[state-dump] - num object pull requests: 0
[state-dump] - num chunks received total: 0
[state-dump] - num chunks received failed (all): 0
[state-dump] - num chunks received failed / cancelled: 0
[state-dump] - num chunks received failed / plasma error: 0
[state-dump] Event stats:
[state-dump] Global stats: 0 total (0 active)
[state-dump] Queueing time: mean = -nan s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] Execution time:  mean = -nan s, total = 0.000 s
[state-dump] Event stats:
[state-dump] PushManager:
[state-dump] - num pushes in flight: 0
[state-dump] - num chunks in flight: 0
[state-dump] - num chunks remaining: 0
[state-dump] - max chunks allowed: 409
[state-dump] OwnershipBasedObjectDirectory:
[state-dump] - num listeners: 0
[state-dump] - cumulative location updates: 0
[state-dump] - num location updates per second: 0.000
[state-dump] - num location lookups per second: 0.000
[state-dump] - num locations added per second: 0.000
[state-dump] - num locations removed per second: 0.000
[state-dump] BufferPool:
[state-dump] - create buffer state map size: 0
[state-dump] PullManager:
[state-dump] - num bytes available for pulled objects: 68719476736
[state-dump] - num bytes being pulled (all): 0
[state-dump] - num bytes being pulled / pinned: 0
[state-dump] - get request bundles: BundlePullRequestQueue{0 total, 0 active, 0 inactive, 0 unpullable}
[state-dump] - wait request bundles: BundlePullRequestQueue{0 total, 0 active, 0 inactive, 0 unpullable}
[state-dump] - task request bundles: BundlePullRequestQueue{0 total, 0 active, 0 inactive, 0 unpullable}
[state-dump] - first get request bundle: N/A
[state-dump] - first wait request bundle: N/A
[state-dump] - first task request bundle: N/A
[state-dump] - num objects queued: 0
[state-dump] - num objects actively pulled (all): 0
[state-dump] - num objects actively pulled / pinned: 0
[state-dump] - num bundles being pulled: 0
[state-dump] - num pull retries: 0
[state-dump] - max timeout seconds: 0
[state-dump] - max timeout request is already processed. No entry.
[state-dump] 
[state-dump] WorkerPool:
[state-dump] - registered jobs: 1
[state-dump] - process_failed_job_config_missing: 0
[state-dump] - process_failed_rate_limited: 0
[state-dump] - process_failed_pending_registration: 0
[state-dump] - process_failed_runtime_env_setup_failed: 0
[state-dump] - num PYTHON workers: 96
[state-dump] - num PYTHON drivers: 1
[state-dump] - num PYTHON pending start requests: 0
[state-dump] - num PYTHON pending registration requests: 0
[state-dump] - num object spill callbacks queued: 0
[state-dump] - num object restore queued: 0
[state-dump] - num util functions queued: 0
[state-dump] - num JAVA workers: 0
[state-dump] - num JAVA drivers: 0
[state-dump] - num JAVA pending start requests: 0
[state-dump] - num JAVA pending registration requests: 0
[state-dump] - num object spill callbacks queued: 0
[state-dump] - num object restore queued: 0
[state-dump] - num util functions queued: 0
[state-dump] - num idle workers: 88
[state-dump] TaskDependencyManager:
[state-dump] - task deps map size: 0
[state-dump] - get req map size: 0
[state-dump] - wait req map size: 0
[state-dump] - local objects map size: 0
[state-dump] WaitManager:
[state-dump] - num active wait requests: 0
[state-dump] Subscriber:
[state-dump] Channel WORKER_OBJECT_LOCATIONS_CHANNEL
[state-dump] - cumulative subscribe requests: 0
[state-dump] - cumulative unsubscribe requests: 0
[state-dump] - active subscribed publishers: 0
[state-dump] - cumulative published messages: 0
[state-dump] - cumulative processed messages: 0
[state-dump] Channel WORKER_REF_REMOVED_CHANNEL
[state-dump] - cumulative subscribe requests: 0
[state-dump] - cumulative unsubscribe requests: 0
[state-dump] - active subscribed publishers: 0
[state-dump] - cumulative published messages: 0
[state-dump] - cumulative processed messages: 0
[state-dump] Channel WORKER_OBJECT_EVICTION
[state-dump] - cumulative subscribe requests: 0
[state-dump] - cumulative unsubscribe requests: 0
[state-dump] - active subscribed publishers: 0
[state-dump] - cumulative published messages: 0
[state-dump] - cumulative processed messages: 0
[state-dump] num async plasma notifications: 0
[state-dump] Event stats:
[state-dump] Global stats: 44317 total (112 active)
[state-dump] Queueing time: mean = 2.557 ms, max = 10.510 s, min = -0.000 s, total = 113.302 s
[state-dump] Execution time:  mean = 121.453 us, total = 5.382 s
[state-dump] Event stats:
[state-dump] 	NodeManagerService.grpc_server.ReportWorkerBacklog - 17382 total (0 active), Execution time: mean = 175.218 us, total = 3.046 s, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	NodeManagerService.grpc_server.ReportWorkerBacklog.HandleRequestImpl - 17382 total (0 active), Execution time: mean = 15.727 us, total = 273.362 ms, Queueing time: mean = 27.450 us, max = 1.617 ms, min = 2.226 us, total = 477.143 ms
[state-dump] 	RaySyncer.OnDemandBroadcasting - 1797 total (1 active), Execution time: mean = 5.643 us, total = 10.140 ms, Queueing time: mean = 190.997 us, max = 267.446 ms, min = -0.000 s, total = 343.221 ms
[state-dump] 	NodeManager.CheckGC - 1797 total (1 active), Execution time: mean = 1.546 us, total = 2.778 ms, Queueing time: mean = 194.829 us, max = 267.454 ms, min = 4.102 us, total = 350.108 ms
[state-dump] 	ObjectManager.UpdateAvailableMemory - 1796 total (0 active), Execution time: mean = 2.186 us, total = 3.925 ms, Queueing time: mean = 28.073 us, max = 3.267 ms, min = 1.507 us, total = 50.419 ms
[state-dump] 	RayletWorkerPool.deadline_timer.kill_idle_workers - 899 total (1 active), Execution time: mean = 12.854 us, total = 11.556 ms, Queueing time: mean = 227.814 us, max = 171.556 ms, min = -0.000 s, total = 204.805 ms
[state-dump] 	MemoryMonitor.CheckIsMemoryUsageAboveThreshold - 719 total (1 active), Execution time: mean = 136.576 us, total = 98.198 ms, Queueing time: mean = 411.488 us, max = 271.296 ms, min = -0.000 s, total = 295.860 ms
[state-dump] 	ClientConnection.async_read.ProcessMessageHeader - 436 total (97 active), Execution time: mean = 3.814 us, total = 1.663 ms, Queueing time: mean = 255.646 ms, max = 10.510 s, min = 10.155 us, total = 111.462 s
[state-dump] 	ClientConnection.async_read.ProcessMessage - 339 total (0 active), Execution time: mean = 1.009 ms, total = 341.898 ms, Queueing time: mean = 9.524 us, max = 211.620 us, min = 1.917 us, total = 3.229 ms
[state-dump] 	NodeManager.CheckForUnexpectedWorkerDisconnects - 180 total (1 active), Execution time: mean = 69.098 us, total = 12.438 ms, Queueing time: mean = 25.880 us, max = 70.907 us, min = -0.000 s, total = 4.658 ms
[state-dump] 	NodeManager.deadline_timer.spill_objects_when_over_threshold - 180 total (1 active), Execution time: mean = 1.818 us, total = 327.164 us, Queueing time: mean = 91.671 us, max = 1.866 ms, min = 6.446 us, total = 16.501 ms
[state-dump] 	NodeManagerService.grpc_server.GetResourceLoad.HandleRequestImpl - 180 total (0 active), Execution time: mean = 59.031 us, total = 10.626 ms, Queueing time: mean = 25.995 us, max = 67.591 us, min = 8.511 us, total = 4.679 ms
[state-dump] 	NodeManagerService.grpc_server.GetResourceLoad - 180 total (0 active), Execution time: mean = 229.558 us, total = 41.320 ms, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	NodeManager.ScheduleAndDispatchTasks - 180 total (1 active), Execution time: mean = 7.115 us, total = 1.281 ms, Queueing time: mean = 26.030 us, max = 101.641 us, min = 8.871 us, total = 4.685 ms
[state-dump] 	NodeManager.deadline_timer.flush_free_objects - 180 total (1 active), Execution time: mean = 3.245 us, total = 584.148 us, Queueing time: mean = 90.468 us, max = 1.867 ms, min = 8.912 us, total = 16.284 ms
[state-dump] 	ClientConnection.async_write.DoAsyncWrites - 98 total (0 active), Execution time: mean = 687.888 ns, total = 67.413 us, Queueing time: mean = 24.592 us, max = 141.185 us, min = 8.594 us, total = 2.410 ms
[state-dump] 	ObjectManager.ObjectAdded - 97 total (0 active), Execution time: mean = 8.183 us, total = 793.773 us, Queueing time: mean = 33.141 us, max = 254.797 us, min = 6.663 us, total = 3.215 ms
[state-dump] 	NodeManagerService.grpc_server.GetSystemConfig - 97 total (0 active), Execution time: mean = 315.658 us, total = 30.619 ms, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	NodeManagerService.grpc_server.GetSystemConfig.HandleRequestImpl - 97 total (0 active), Execution time: mean = 21.951 us, total = 2.129 ms, Queueing time: mean = 28.149 us, max = 186.585 us, min = 4.908 us, total = 2.730 ms
[state-dump] 	ObjectManager.ObjectDeleted - 97 total (0 active), Execution time: mean = 11.465 us, total = 1.112 ms, Queueing time: mean = 47.495 us, max = 498.503 us, min = 13.070 us, total = 4.607 ms
[state-dump] 	ClusterResourceManager.ResetRemoteNodeView - 61 total (1 active), Execution time: mean = 3.195 us, total = 194.919 us, Queueing time: mean = 24.542 us, max = 101.586 us, min = 9.618 us, total = 1.497 ms
[state-dump] 	NodeManager.deadline_timer.record_metrics - 36 total (1 active), Execution time: mean = 296.118 us, total = 10.660 ms, Queueing time: mean = 166.310 us, max = 1.663 ms, min = 11.378 us, total = 5.987 ms
[state-dump] 	NodeManager.deadline_timer.debug_state_dump - 18 total (1 active), Execution time: mean = 890.532 us, total = 16.030 ms, Queueing time: mean = 27.226 us, max = 79.652 us, min = 10.295 us, total = 490.067 us
[state-dump] 	PeriodicalRunner.RunFnPeriodically - 14 total (0 active), Execution time: mean = 153.356 us, total = 2.147 ms, Queueing time: mean = 3.139 ms, max = 8.780 ms, min = 28.507 us, total = 43.949 ms
[state-dump] 	RaySyncer.BroadcastMessage - 9 total (0 active), Execution time: mean = 107.004 us, total = 963.036 us, Queueing time: mean = 202.333 ns, max = 476.000 ns, min = 67.000 ns, total = 1.821 us
[state-dump] 	 - 9 total (0 active), Execution time: mean = 455.222 ns, total = 4.097 us, Queueing time: mean = 36.859 us, max = 104.566 us, min = 17.093 us, total = 331.732 us
[state-dump] 	NodeManagerService.grpc_server.RequestWorkerLease - 8 total (0 active), Execution time: mean = 261.558 us, total = 2.092 ms, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	NodeManagerService.grpc_server.RequestWorkerLease.HandleRequestImpl - 8 total (0 active), Execution time: mean = 91.695 us, total = 733.562 us, Queueing time: mean = 19.210 us, max = 24.229 us, min = 14.724 us, total = 153.682 us
[state-dump] 	WorkerPool.PopWorkerCallback - 8 total (0 active), Execution time: mean = 19.403 us, total = 155.223 us, Queueing time: mean = 5.593 us, max = 6.692 us, min = 4.868 us, total = 44.743 us
[state-dump] 	NodeManager.deadline_timer.print_event_loop_stats - 3 total (1 active, 1 running), Execution time: mean = 579.871 us, total = 1.740 ms, Queueing time: mean = 9.043 us, max = 15.160 us, min = 11.968 us, total = 27.128 us
[state-dump] 	ray::rpc::NodeInfoGcsService.grpc_client.CheckAlive - 3 total (0 active), Execution time: mean = 471.035 us, total = 1.413 ms, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	NodeManager.GcsCheckAlive - 3 total (1 active), Execution time: mean = 77.964 us, total = 233.892 us, Queueing time: mean = 525.168 us, max = 823.175 us, min = 752.330 us, total = 1.576 ms
[state-dump] 	ray::rpc::NodeInfoGcsService.grpc_client.CheckAlive.OnReplyReceived - 3 total (0 active), Execution time: mean = 19.294 us, total = 57.881 us, Queueing time: mean = 23.533 us, max = 25.482 us, min = 22.319 us, total = 70.599 us
[state-dump] 	ray::rpc::InternalPubSubGcsService.grpc_client.GcsSubscriberPoll - 2 total (1 active), Execution time: mean = 717.121 ms, total = 1.434 s, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	ray::rpc::InternalPubSubGcsService.grpc_client.GcsSubscriberCommandBatch - 2 total (0 active), Execution time: mean = 597.795 us, total = 1.196 ms, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	ray::rpc::InternalPubSubGcsService.grpc_client.GcsSubscriberCommandBatch.OnReplyReceived - 2 total (0 active), Execution time: mean = 105.510 us, total = 211.019 us, Queueing time: mean = 759.244 us, max = 1.502 ms, min = 16.763 us, total = 1.518 ms
[state-dump] 	RaySyncerRegister - 2 total (0 active), Execution time: mean = 1.485 us, total = 2.970 us, Queueing time: mean = 160.000 ns, max = 208.000 ns, min = 112.000 ns, total = 320.000 ns
[state-dump] 	ray::rpc::JobInfoGcsService.grpc_client.AddJob.OnReplyReceived - 1 total (0 active), Execution time: mean = 28.433 us, total = 28.433 us, Queueing time: mean = 155.737 us, max = 155.737 us, min = 155.737 us, total = 155.737 us
[state-dump] 	ray::rpc::NodeInfoGcsService.grpc_client.RegisterNode - 1 total (0 active), Execution time: mean = 1.113 ms, total = 1.113 ms, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	ray::rpc::JobInfoGcsService.grpc_client.GetAllJobInfo - 1 total (0 active), Execution time: mean = 500.844 us, total = 500.844 us, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	ray::rpc::NodeInfoGcsService.grpc_client.GetAllNodeInfo.OnReplyReceived - 1 total (0 active), Execution time: mean = 120.718 us, total = 120.718 us, Queueing time: mean = 17.721 us, max = 17.721 us, min = 17.721 us, total = 17.721 us
[state-dump] 	ray::rpc::NodeInfoGcsService.grpc_client.RegisterNode.OnReplyReceived - 1 total (0 active), Execution time: mean = 234.981 us, total = 234.981 us, Queueing time: mean = 22.270 us, max = 22.270 us, min = 22.270 us, total = 22.270 us
[state-dump] 	ray::rpc::JobInfoGcsService.grpc_client.GetAllJobInfo.OnReplyReceived - 1 total (0 active), Execution time: mean = 13.212 us, total = 13.212 us, Queueing time: mean = 16.939 us, max = 16.939 us, min = 16.939 us, total = 16.939 us
[state-dump] 	ray::rpc::JobInfoGcsService.grpc_client.AddJob - 1 total (0 active), Execution time: mean = 660.368 us, total = 660.368 us, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	ray::rpc::InternalKVGcsService.grpc_client.GetInternalConfig - 1 total (0 active), Execution time: mean = 1.084 ms, total = 1.084 ms, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	ray::rpc::NodeInfoGcsService.grpc_client.GetAllNodeInfo - 1 total (0 active), Execution time: mean = 710.941 us, total = 710.941 us, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	ray::rpc::InternalPubSubGcsService.grpc_client.GcsSubscriberPoll.OnReplyReceived - 1 total (0 active), Execution time: mean = 151.102 us, total = 151.102 us, Queueing time: mean = 17.047 us, max = 17.047 us, min = 17.047 us, total = 17.047 us
[state-dump] 	Subscriber.HandlePublishedMessage_GCS_JOB_CHANNEL - 1 total (0 active), Execution time: mean = 42.544 us, total = 42.544 us, Queueing time: mean = 140.637 us, max = 140.637 us, min = 140.637 us, total = 140.637 us
[state-dump] 	NodeManager.GCTaskFailureReason - 1 total (1 active), Execution time: mean = 0.000 s, total = 0.000 s, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s
[state-dump] 	ray::rpc::InternalKVGcsService.grpc_client.GetInternalConfig.OnReplyReceived - 1 total (0 active), Execution time: mean = 15.242 ms, total = 15.242 ms, Queueing time: mean = 39.430 us, max = 39.430 us, min = 39.430 us, total = 39.430 us
[state-dump] DebugString() time ms: 0
[state-dump] 
[state-dump] 
[2025-06-25 17:20:55,337 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:21:00,341 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:21:05,345 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:21:10,348 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:21:15,352 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:21:20,356 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:21:25,359 W 1412746 1412746] (raylet) memory_monitor.cc:198: Got negative used memory for cgroup -1, setting it to zero
[2025-06-25 17:21:25,958 I 1412746 1412746] (raylet) node_manager.cc:1523: Disconnecting client, graceful=true, disconnect_type=3, has_creation_task_exception=false worker_id=01000000ffffffffffffffffffffffffffffffffffffffffffffffff
[2025-06-25 17:21:25,958 I 1412746 1412746] (raylet) node_manager.cc:1615: Driver (pid=1411652) is disconnected. worker_id=01000000ffffffffffffffffffffffffffffffffffffffffffffffff job_id=01000000
[2025-06-25 17:21:25,962 W 1412746 1412784] (raylet) store.cc:368: Disconnecting client due to connection error with code 2: End of file
[2025-06-25 17:21:25,962 I 1412746 1412746] (raylet) node_manager.cc:1018: The leased worker 8dca9a155c99a3e84b38827c052f68988a74457d6aa8d84bfd9bc5dc is killed because the owner process 01000000ffffffffffffffffffffffffffffffffffffffffffffffff died.
[2025-06-25 17:21:25,962 I 1412746 1412746] (raylet) node_manager.cc:1018: The leased worker 99106b7c6a0e4c9f67770b0a311d40ea69a3c2a3a8073779b2171f56 is killed because the owner process 01000000ffffffffffffffffffffffffffffffffffffffffffffffff died.
[2025-06-25 17:21:25,962 I 1412746 1412746] (raylet) node_manager.cc:1018: The leased worker 3d251100376b2a11709a5f9e62446b6489df8d9c136c7110db6f66ec is killed because the owner process 01000000ffffffffffffffffffffffffffffffffffffffffffffffff died.
[2025-06-25 17:21:25,962 I 1412746 1412746] (raylet) node_manager.cc:1018: The leased worker 80f2de78ef35c57f741af61456e5d3a45466df25c003c93d5f59315f is killed because the owner process 01000000ffffffffffffffffffffffffffffffffffffffffffffffff died.
[2025-06-25 17:21:25,962 I 1412746 1412746] (raylet) node_manager.cc:1018: The leased worker 4502209fa338fc7cbdb9afc95d1539c25f3fe23ca5ccd054a6ecc677 is killed because the owner process 01000000ffffffffffffffffffffffffffffffffffffffffffffffff died.
[2025-06-25 17:21:25,962 I 1412746 1412746] (raylet) node_manager.cc:1018: The leased worker 47434baa29d4649191f8ef66612fe3b5b79786a91dae93615d8a7f74 is killed because the owner process 01000000ffffffffffffffffffffffffffffffffffffffffffffffff died.
[2025-06-25 17:21:25,962 I 1412746 1412746] (raylet) node_manager.cc:1018: The leased worker 90af73dab71f69691c7b9ae8e3f7551ed2f44584052fbdb01fe5b34b is killed because the owner process 01000000ffffffffffffffffffffffffffffffffffffffffffffffff died.
[2025-06-25 17:21:25,962 I 1412746 1412746] (raylet) node_manager.cc:1018: The leased worker 3797b72263f2acfea34665de8bb2a29d883adbf97b03e7348f868e1c is killed because the owner process 01000000ffffffffffffffffffffffffffffffffffffffffffffffff died.
[2025-06-25 17:21:25,963 I 1412746 1412746] (raylet) worker_pool.cc:724: Job 01000000 already started in worker pool.
[2025-06-25 17:21:25,963 I 1412746 1412746] (raylet) node_manager.cc:559: The leased worker  is killed because the job 01000000 finished. worker_id=8dca9a155c99a3e84b38827c052f68988a74457d6aa8d84bfd9bc5dc
[2025-06-25 17:21:25,963 I 1412746 1412746] (raylet) node_manager.cc:559: The leased worker  is killed because the job 01000000 finished. worker_id=99106b7c6a0e4c9f67770b0a311d40ea69a3c2a3a8073779b2171f56
[2025-06-25 17:21:25,963 I 1412746 1412746] (raylet) node_manager.cc:559: The leased worker  is killed because the job 01000000 finished. worker_id=3d251100376b2a11709a5f9e62446b6489df8d9c136c7110db6f66ec
[2025-06-25 17:21:25,963 I 1412746 1412746] (raylet) node_manager.cc:559: The leased worker  is killed because the job 01000000 finished. worker_id=80f2de78ef35c57f741af61456e5d3a45466df25c003c93d5f59315f
[2025-06-25 17:21:25,964 I 1412746 1412746] (raylet) node_manager.cc:559: The leased worker  is killed because the job 01000000 finished. worker_id=4502209fa338fc7cbdb9afc95d1539c25f3fe23ca5ccd054a6ecc677
[2025-06-25 17:21:25,964 I 1412746 1412746] (raylet) node_manager.cc:559: The leased worker  is killed because the job 01000000 finished. worker_id=47434baa29d4649191f8ef66612fe3b5b79786a91dae93615d8a7f74
[2025-06-25 17:21:25,964 I 1412746 1412746] (raylet) node_manager.cc:559: The leased worker  is killed because the job 01000000 finished. worker_id=90af73dab71f69691c7b9ae8e3f7551ed2f44584052fbdb01fe5b34b
[2025-06-25 17:21:25,964 I 1412746 1412746] (raylet) node_manager.cc:559: The leased worker  is killed because the job 01000000 finished. worker_id=3797b72263f2acfea34665de8bb2a29d883adbf97b03e7348f868e1c
[2025-06-25 17:21:25,976 I 1412746 1412746] (raylet) node_manager.cc:1523: Disconnecting client, graceful=true, disconnect_type=1, has_creation_task_exception=false worker_id=47434baa29d4649191f8ef66612fe3b5b79786a91dae93615d8a7f74
[2025-06-25 17:21:25,976 I 1412746 1412746] (raylet) node_manager.cc:1523: Disconnecting client, graceful=true, disconnect_type=1, has_creation_task_exception=false worker_id=90af73dab71f69691c7b9ae8e3f7551ed2f44584052fbdb01fe5b34b
[2025-06-25 17:21:25,977 I 1412746 1412746] (raylet) node_manager.cc:1523: Disconnecting client, graceful=true, disconnect_type=1, has_creation_task_exception=false worker_id=3d251100376b2a11709a5f9e62446b6489df8d9c136c7110db6f66ec
[2025-06-25 17:21:25,977 I 1412746 1412746] (raylet) node_manager.cc:1523: Disconnecting client, graceful=true, disconnect_type=1, has_creation_task_exception=false worker_id=99106b7c6a0e4c9f67770b0a311d40ea69a3c2a3a8073779b2171f56
[2025-06-25 17:21:25,979 I 1412746 1412746] (raylet) node_manager.cc:1523: Disconnecting client, graceful=true, disconnect_type=1, has_creation_task_exception=false worker_id=8dca9a155c99a3e84b38827c052f68988a74457d6aa8d84bfd9bc5dc
[2025-06-25 17:21:25,979 I 1412746 1412746] (raylet) node_manager.cc:1523: Disconnecting client, graceful=true, disconnect_type=1, has_creation_task_exception=false worker_id=80f2de78ef35c57f741af61456e5d3a45466df25c003c93d5f59315f
[2025-06-25 17:21:25,979 I 1412746 1412746] (raylet) node_manager.cc:1523: Disconnecting client, graceful=true, disconnect_type=1, has_creation_task_exception=false worker_id=4502209fa338fc7cbdb9afc95d1539c25f3fe23ca5ccd054a6ecc677
[2025-06-25 17:21:25,979 I 1412746 1412746] (raylet) node_manager.cc:1523: Disconnecting client, graceful=true, disconnect_type=1, has_creation_task_exception=false worker_id=3797b72263f2acfea34665de8bb2a29d883adbf97b03e7348f868e1c
[2025-06-25 17:21:26,023 W 1412746 1412746] (raylet) node_manager.cc:567: Failed to send exit request to worker : RpcError: RPC Error message: Socket closed; RPC Error details:  rpc_code: 14. Killing it using SIGKILL instead. worker_id=90af73dab71f69691c7b9ae8e3f7551ed2f44584052fbdb01fe5b34b
[2025-06-25 17:21:26,024 W 1412746 1412784] (raylet) store.cc:368: Disconnecting client due to connection error with code 2: End of file
[2025-06-25 17:21:26,024 W 1412746 1412746] (raylet) node_manager.cc:567: Failed to send exit request to worker : RpcError: RPC Error message: Socket closed; RPC Error details:  rpc_code: 14. Killing it using SIGKILL instead. worker_id=3d251100376b2a11709a5f9e62446b6489df8d9c136c7110db6f66ec
[2025-06-25 17:21:26,024 W 1412746 1412784] (raylet) store.cc:368: Disconnecting client due to connection error with code 2: End of file
[2025-06-25 17:21:26,024 W 1412746 1412746] (raylet) node_manager.cc:567: Failed to send exit request to worker : RpcError: RPC Error message: Socket closed; RPC Error details:  rpc_code: 14. Killing it using SIGKILL instead. worker_id=47434baa29d4649191f8ef66612fe3b5b79786a91dae93615d8a7f74
[2025-06-25 17:21:26,024 W 1412746 1412784] (raylet) store.cc:368: Disconnecting client due to connection error with code 2: End of file
[2025-06-25 17:21:26,024 W 1412746 1412746] (raylet) node_manager.cc:567: Failed to send exit request to worker : RpcError: RPC Error message: Socket closed; RPC Error details:  rpc_code: 14. Killing it using SIGKILL instead. worker_id=99106b7c6a0e4c9f67770b0a311d40ea69a3c2a3a8073779b2171f56
[2025-06-25 17:21:26,025 W 1412746 1412784] (raylet) store.cc:368: Disconnecting client due to connection error with code 2: End of file
[2025-06-25 17:21:26,025 W 1412746 1412746] (raylet) node_manager.cc:567: Failed to send exit request to worker : RpcError: RPC Error message: Socket closed; RPC Error details:  rpc_code: 14. Killing it using SIGKILL instead. worker_id=4502209fa338fc7cbdb9afc95d1539c25f3fe23ca5ccd054a6ecc677
[2025-06-25 17:21:26,025 W 1412746 1412746] (raylet) node_manager.cc:567: Failed to send exit request to worker : RpcError: RPC Error message: Socket closed; RPC Error details:  rpc_code: 14. Killing it using SIGKILL instead. worker_id=80f2de78ef35c57f741af61456e5d3a45466df25c003c93d5f59315f
[2025-06-25 17:21:26,026 W 1412746 1412746] (raylet) node_manager.cc:567: Failed to send exit request to worker : RpcError: RPC Error message: Socket closed; RPC Error details:  rpc_code: 14. Killing it using SIGKILL instead. worker_id=8dca9a155c99a3e84b38827c052f68988a74457d6aa8d84bfd9bc5dc
[2025-06-25 17:21:26,027 W 1412746 1412784] (raylet) store.cc:368: Disconnecting client due to connection error with code 2: End of file
[2025-06-25 17:21:26,027 W 1412746 1412784] (raylet) store.cc:368: Disconnecting client due to connection error with code 2: End of file
[2025-06-25 17:21:26,027 W 1412746 1412784] (raylet) store.cc:368: Disconnecting client due to connection error with code 2: End of file
[2025-06-25 17:21:26,027 W 1412746 1412746] (raylet) node_manager.cc:567: Failed to send exit request to worker : RpcError: RPC Error message: Socket closed; RPC Error details:  rpc_code: 14. Killing it using SIGKILL instead. worker_id=3797b72263f2acfea34665de8bb2a29d883adbf97b03e7348f868e1c
[2025-06-25 17:21:26,028 W 1412746 1412784] (raylet) store.cc:368: Disconnecting client due to connection error with code 2: End of file
[2025-06-25 17:21:26,031 I 1412746 1412746] (raylet) main.cc:504: received SIGTERM. Existing local drain request = None
[2025-06-25 17:21:26,031 I 1412746 1412746] (raylet) main.cc:307: Raylet graceful shutdown triggered, reason = EXPECTED_TERMINATION, reason message = received SIGTERM
[2025-06-25 17:21:26,031 I 1412746 1412746] (raylet) main.cc:310: Shutting down...
[2025-06-25 17:21:26,031 I 1412746 1412746] (raylet) accessor.cc:518: Unregistering node node_id=c3065f0a91075821625dad700db11cc1befa3100fb68c9da57a85f4b
[2025-06-25 17:21:26,033 I 1412746 1412746] (raylet) accessor.cc:768: Received notification for node, IsAlive = 0 node_id=c3065f0a91075821625dad700db11cc1befa3100fb68c9da57a85f4b
[2025-06-25 17:21:26,070 C 1412746 1412746] (raylet) node_manager.cc:953: [Timeout] Exiting because this node manager has mistakenly been marked as dead by the GCS: GCS failed to check the health of this node for 5 times. This is likely because the machine or raylet has become overloaded.
*** StackTrace Information ***
/home/shuzuan/miniconda3/envs/comfyui-xdit/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0xe19cca) [0x5ad57abb2cca] ray::operator<<()
/home/shuzuan/miniconda3/envs/comfyui-xdit/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0xe1c089) [0x5ad57abb5089] ray::RayLog::~RayLog()
/home/shuzuan/miniconda3/envs/comfyui-xdit/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x2cec4b) [0x5ad57a067c4b] ray::raylet::NodeManager::NodeRemoved()
/home/shuzuan/miniconda3/envs/comfyui-xdit/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x611725) [0x5ad57a3aa725] ray::gcs::NodeInfoAccessor::HandleNotification()
/home/shuzuan/miniconda3/envs/comfyui-xdit/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x78aad8) [0x5ad57a523ad8] EventTracker::RecordExecution()
/home/shuzuan/miniconda3/envs/comfyui-xdit/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x785cb7) [0x5ad57a51ecb7] std::_Function_handler<>::_M_invoke()
/home/shuzuan/miniconda3/envs/comfyui-xdit/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x786116) [0x5ad57a51f116] boost::asio::detail::completion_handler<>::do_complete()
/home/shuzuan/miniconda3/envs/comfyui-xdit/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0xdf5c3b) [0x5ad57ab8ec3b] boost::asio::detail::scheduler::do_run_one()
/home/shuzuan/miniconda3/envs/comfyui-xdit/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0xdf81c9) [0x5ad57ab911c9] boost::asio::detail::scheduler::run()
/home/shuzuan/miniconda3/envs/comfyui-xdit/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0xdf86e2) [0x5ad57ab916e2] boost::asio::io_context::run()
/home/shuzuan/miniconda3/envs/comfyui-xdit/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x1fa6cf) [0x5ad579f936cf] main
/lib/x86_64-linux-gnu/libc.so.6(+0x29d90) [0x7f6ae7e29d90]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80) [0x7f6ae7e29e40] __libc_start_main
/home/shuzuan/miniconda3/envs/comfyui-xdit/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x25b547) [0x5ad579ff4547]

