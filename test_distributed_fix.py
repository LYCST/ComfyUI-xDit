#!/usr/bin/env python3
"""
æµ‹è¯•åˆ†å¸ƒå¼åˆå§‹åŒ–ä¿®å¤
=======================

æµ‹è¯•æ–°çš„åˆ†å¸ƒå¼åè°ƒæœºåˆ¶æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import os
import sys
import torch
import logging
import time

# è®¾ç½®ç¯å¢ƒ
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "custom_nodes", "comfyui_xdit_multigpu"))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

def test_distributed_initialization():
    """æµ‹è¯•åˆ†å¸ƒå¼åˆå§‹åŒ–"""
    try:
        from xdit_runtime import XDiTDispatcher, SchedulingStrategy
        
        logger.info("ğŸš€ Testing distributed initialization fix...")
        
        # é…ç½®GPUè®¾å¤‡ï¼ˆæµ‹è¯•æ—¶ç”¨å°‘ä¸€äº›ï¼‰
        gpu_devices = [0, 1, 2, 3]  # 4ä¸ªGPUæµ‹è¯•
        model_path = "models/checkpoints/flux/flux1-dev.safetensors"
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
        if not os.path.exists(model_path):
            logger.warning(f"Model not found: {model_path}")
            model_path = "/fake/path/for/testing"  # æµ‹è¯•ç”¨å‡è·¯å¾„
        
        # åˆ›å»ºdispatcher
        dispatcher = XDiTDispatcher(
            gpu_devices=gpu_devices,
            model_path=model_path,
            strategy="Hybrid",
            scheduling_strategy=SchedulingStrategy.ROUND_ROBIN
        )
        
        logger.info(f"Created dispatcher for {len(gpu_devices)} GPUs")
        
        # åˆå§‹åŒ–
        logger.info("Initializing dispatcher...")
        start_time = time.time()
        
        success = dispatcher.initialize()
        
        init_time = time.time() - start_time
        logger.info(f"Initialization completed in {init_time:.2f}s")
        
        if success:
            logger.info("âœ… Dispatcher initialization succeeded!")
            
            # è·å–çŠ¶æ€
            status = dispatcher.get_status()
            logger.info(f"Status: {status}")
            
            # æµ‹è¯•ç®€å•æ¨ç†ï¼ˆåº”è¯¥fallbackåˆ°ComfyUIï¼‰
            logger.info("Testing inference fallback...")
            try:
                dummy_latent = torch.randn(1, 4, 64, 64)
                result = dispatcher.run_inference(
                    model_state_dict={},
                    conditioning_positive=[],
                    conditioning_negative=[],
                    latent_samples=dummy_latent,
                    num_inference_steps=1,
                    guidance_scale=1.0,
                    seed=42
                )
                
                if result is None:
                    logger.info("âœ… Correctly returned None for fallback")
                else:
                    logger.info(f"âœ… Got result: {result.shape}")
                    
            except Exception as e:
                logger.info(f"Expected fallback behavior: {e}")
            
            # æ¸…ç†
            dispatcher.cleanup()
            logger.info("âœ… Cleanup completed")
            
        else:
            logger.error("âŒ Dispatcher initialization failed")
            return False
            
        return True
        
    except Exception as e:
        logger.error(f"Test failed: {e}")
        logger.exception("Full traceback:")
        return False

def test_single_gpu_mode():
    """æµ‹è¯•å•GPUæ¨¡å¼"""
    try:
        from xdit_runtime import XDiTDispatcher, SchedulingStrategy
        
        logger.info("ğŸš€ Testing single GPU mode...")
        
        # å•GPUé…ç½®
        gpu_devices = [0]
        model_path = "/fake/path/for/testing"
        
        dispatcher = XDiTDispatcher(
            gpu_devices=gpu_devices,
            model_path=model_path,
            strategy="Hybrid",
            scheduling_strategy=SchedulingStrategy.ROUND_ROBIN
        )
        
        success = dispatcher.initialize()
        
        if success:
            logger.info("âœ… Single GPU initialization succeeded!")
            
            # æµ‹è¯•æ¨ç†
            dummy_latent = torch.randn(1, 4, 64, 64)
            result = dispatcher.run_inference(
                model_state_dict={},
                conditioning_positive=[],
                conditioning_negative=[],
                latent_samples=dummy_latent,
                num_inference_steps=1,
                guidance_scale=1.0,
                seed=42
            )
            
            if result is None:
                logger.info("âœ… Single GPU correctly returned None for fallback")
            
            dispatcher.cleanup()
            return True
        else:
            logger.error("âŒ Single GPU initialization failed")
            return False
            
    except Exception as e:
        logger.error(f"Single GPU test failed: {e}")
        return False

def test_ray_availability():
    """æµ‹è¯•Rayå¯ç”¨æ€§"""
    try:
        import ray
        logger.info("âœ… Ray is available")
        
        if ray.is_initialized():
            logger.info("Ray already initialized")
        else:
            logger.info("Ray not initialized yet")
            
        return True
    except ImportError:
        logger.warning("âš ï¸ Ray not available")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("="*50)
    logger.info("åˆ†å¸ƒå¼åˆå§‹åŒ–ä¿®å¤æµ‹è¯•")
    logger.info("="*50)
    
    # æ£€æŸ¥CUDA
    if not torch.cuda.is_available():
        logger.error("CUDA not available")
        return False
    
    gpu_count = torch.cuda.device_count()
    logger.info(f"Available GPUs: {gpu_count}")
    
    # æµ‹è¯•Ray
    ray_available = test_ray_availability()
    
    # æµ‹è¯•å•GPUæ¨¡å¼
    logger.info("\n" + "="*30)
    logger.info("æµ‹è¯•å•GPUæ¨¡å¼")
    logger.info("="*30)
    
    single_gpu_success = test_single_gpu_mode()
    
    # æµ‹è¯•å¤šGPUæ¨¡å¼ï¼ˆä»…åœ¨æœ‰è¶³å¤ŸGPUä¸”Rayå¯ç”¨æ—¶ï¼‰
    if gpu_count >= 4 and ray_available:
        logger.info("\n" + "="*30)
        logger.info("æµ‹è¯•å¤šGPUåˆ†å¸ƒå¼åˆå§‹åŒ–")
        logger.info("="*30)
        
        multi_gpu_success = test_distributed_initialization()
    else:
        logger.info(f"Skipping multi-GPU test (GPUs: {gpu_count}, Ray: {ray_available})")
        multi_gpu_success = True  # è·³è¿‡æµ‹è¯•è§†ä¸ºæˆåŠŸ
    
    # æ€»ç»“
    logger.info("\n" + "="*30)
    logger.info("æµ‹è¯•ç»“æœæ€»ç»“")
    logger.info("="*30)
    
    logger.info(f"å•GPUæ¨¡å¼: {'âœ… æˆåŠŸ' if single_gpu_success else 'âŒ å¤±è´¥'}")
    logger.info(f"å¤šGPUæ¨¡å¼: {'âœ… æˆåŠŸ' if multi_gpu_success else 'âŒ å¤±è´¥'}")
    
    overall_success = single_gpu_success and multi_gpu_success
    logger.info(f"æ€»ä½“ç»“æœ: {'âœ… ä¿®å¤æˆåŠŸ' if overall_success else 'âŒ ä»æœ‰é—®é¢˜'}")
    
    return overall_success

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 