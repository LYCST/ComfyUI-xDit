#!/usr/bin/env python3
"""
æµ‹è¯•ä¿®å¤åçš„xDiTé›†æˆ
===================

éªŒè¯æ–°çš„workerå’Œdispatcherè®¾è®¡æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import os
import sys
import torch
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_new_worker_design():
    """æµ‹è¯•æ–°çš„workerè®¾è®¡"""
    print("ğŸ§ª æµ‹è¯•æ–°çš„Workerè®¾è®¡...")
    
    try:
        # æ·»åŠ custom_nodesè·¯å¾„
        custom_nodes_path = os.path.join(os.getcwd(), "custom_nodes", "comfyui_xdit_multigpu")
        if custom_nodes_path not in sys.path:
            sys.path.insert(0, custom_nodes_path)
        
        from xdit_runtime.worker import XDiTWorkerFallback
        
        # åˆ›å»ºworker
        worker = XDiTWorkerFallback(
            gpu_id=0,
            model_path="test_model_path",
            strategy="Hybrid"
        )
        
        # åˆå§‹åŒ–
        success = worker.initialize()
        if not success:
            print("âŒ Workeråˆå§‹åŒ–å¤±è´¥")
            return False
        
        print("âœ… Workeråˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•model wrapping
        dummy_state_dict = {"test": torch.randn(10, 10)}
        dummy_config = {"model_type": "flux"}
        
        wrap_success = worker.wrap_model_for_xdit(dummy_state_dict, dummy_config)
        if wrap_success:
            print("âœ… æ¨¡å‹åŒ…è£…æˆåŠŸ")
        else:
            print("âŒ æ¨¡å‹åŒ…è£…å¤±è´¥")
            return False
        
        # æµ‹è¯•inference
        dummy_conditioning = [torch.randn(1, 77, 768), {}]
        dummy_latents = torch.randn(1, 16, 64, 64)  # Fluxæ ¼å¼
        
        result = worker.run_inference(
            model_state_dict=dummy_state_dict,
            conditioning_positive=dummy_conditioning,
            conditioning_negative=dummy_conditioning,
            latent_samples=dummy_latents
        )
        
        if result is not None:
            print("âœ… æ¨ç†æµ‹è¯•æˆåŠŸ")
            print(f"   è¾“å…¥å½¢çŠ¶: {dummy_latents.shape}")
            print(f"   è¾“å‡ºå½¢çŠ¶: {result.shape}")
        else:
            print("âŒ æ¨ç†æµ‹è¯•å¤±è´¥")
            return False
        
        # æ¸…ç†
        worker.cleanup()
        print("âœ… Workeræ¸…ç†æˆåŠŸ")
        
        return True
        
    except Exception as e:
        print(f"âŒ Workeræµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_new_dispatcher_design():
    """æµ‹è¯•æ–°çš„dispatcherè®¾è®¡"""
    print("\nğŸ§ª æµ‹è¯•æ–°çš„Dispatcherè®¾è®¡...")
    
    try:
        custom_nodes_path = os.path.join(os.getcwd(), "custom_nodes", "comfyui_xdit_multigpu")
        if custom_nodes_path not in sys.path:
            sys.path.insert(0, custom_nodes_path)
        
        from xdit_runtime.dispatcher import XDiTDispatcher, SchedulingStrategy
        
        # åˆ›å»ºdispatcher
        dispatcher = XDiTDispatcher(
            gpu_devices=[0],
            model_path="test_model_path",
            strategy="Hybrid",
            scheduling_strategy=SchedulingStrategy.ROUND_ROBIN
        )
        
        # ä¸è°ƒç”¨initialize()ï¼Œå› ä¸ºé‚£ä¼šå°è¯•åˆ›å»ºRay workers
        # æˆ‘ä»¬åªæµ‹è¯•dispatcherå¯¹è±¡çš„åˆ›å»ºå’ŒçŠ¶æ€
        
        status = dispatcher.get_status()
        print(f"âœ… Dispatcheråˆ›å»ºæˆåŠŸ")
        print(f"   è°ƒåº¦ç­–ç•¥: {status['scheduling_strategy']}")
        print(f"   GPUè®¾å¤‡: {status['gpu_devices']}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Dispatcheræµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_node_import():
    """æµ‹è¯•èŠ‚ç‚¹å¯¼å…¥"""
    print("\nğŸ§ª æµ‹è¯•èŠ‚ç‚¹å¯¼å…¥...")
    
    try:
        custom_nodes_path = os.path.join(os.getcwd(), "custom_nodes", "comfyui_xdit_multigpu")
        if custom_nodes_path not in sys.path:
            sys.path.insert(0, custom_nodes_path)
        
        # æµ‹è¯•å¯¼å…¥ä¸»è¦èŠ‚ç‚¹ç±»
        from nodes import XDiTKSampler, XDiTUNetLoader, XDiTCheckpointLoader
        
        print("âœ… æ‰€æœ‰èŠ‚ç‚¹ç±»å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•INPUT_TYPES
        input_types = XDiTKSampler.INPUT_TYPES()
        if 'required' in input_types and 'optional' in input_types:
            print("âœ… KSamplerè¾“å…¥ç±»å‹å®šä¹‰æ­£ç¡®")
        else:
            print("âŒ KSamplerè¾“å…¥ç±»å‹å®šä¹‰æœ‰é—®é¢˜")
            return False
        
        return True
        
    except Exception as e:
        print(f"âŒ èŠ‚ç‚¹å¯¼å…¥æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_memory_compatibility():
    """æµ‹è¯•å†…å­˜å’Œé€šé“å…¼å®¹æ€§"""
    print("\nğŸ§ª æµ‹è¯•å†…å­˜å’Œé€šé“å…¼å®¹æ€§...")
    
    try:
        # æµ‹è¯•ä¸åŒçš„latentæ ¼å¼
        sd_latents = torch.randn(1, 4, 64, 64)     # æ ‡å‡†SDæ ¼å¼
        flux_latents = torch.randn(1, 16, 64, 64)  # Fluxæ ¼å¼
        
        print(f"âœ… SD Latents: {sd_latents.shape}")
        print(f"âœ… Flux Latents: {flux_latents.shape}")
        
        # æµ‹è¯•GPUå†…å­˜åˆ†é…
        if torch.cuda.is_available():
            device_count = torch.cuda.device_count()
            print(f"âœ… å¯ç”¨GPUæ•°é‡: {device_count}")
            
            for i in range(min(device_count, 4)):  # åªæµ‹è¯•å‰4ä¸ªGPU
                props = torch.cuda.get_device_properties(i)
                memory_gb = props.total_memory / (1024**3)
                print(f"   GPU {i}: {props.name} ({memory_gb:.1f} GB)")
        else:
            print("âš ï¸  CUDAä¸å¯ç”¨")
        
        return True
        
    except Exception as e:
        print(f"âŒ å†…å­˜å…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸ”§ æµ‹è¯•ä¿®å¤åçš„xDiTé›†æˆ")
    print("=" * 40)
    
    tests = [
        test_node_import,
        test_memory_compatibility,
        test_new_worker_design,
        test_new_dispatcher_design,
    ]
    
    passed = 0
    total = len(tests)
    
    for test in tests:
        try:
            if test():
                passed += 1
            else:
                print(f"ğŸ”´ {test.__name__} å¤±è´¥")
        except Exception as e:
            print(f"ğŸ”´ {test.__name__} å¼‚å¸¸: {e}")
    
    print("\n" + "=" * 40)
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ–°çš„è®¾è®¡åº”è¯¥èƒ½è§£å†³ä¹‹å‰çš„é—®é¢˜")
        print("\nğŸ’¡ ä¸»è¦æ”¹è¿›:")
        print("   âœ… ä¸å†å°è¯•ä»å•ä¸ªsafetensorsæ–‡ä»¶åˆ›å»ºpipeline")
        print("   âœ… ç›´æ¥ä½¿ç”¨ComfyUIçš„æ¨¡å‹æ•°æ®ç»“æ„")
        print("   âœ… ä¿æŒåŸå§‹çš„latentæ ¼å¼å’Œé€šé“æ•°")
        print("   âœ… æ”¹è¿›çš„é”™è¯¯å¤„ç†å’Œå›é€€æœºåˆ¶")
        print("\nğŸš€ å»ºè®®ä¸‹ä¸€æ­¥:")
        print("   1. é‡æ–°å¯åŠ¨ComfyUIæµ‹è¯•ä¿®å¤æ•ˆæœ")
        print("   2. æ£€æŸ¥error.logæŸ¥çœ‹æ–°çš„æ—¥å¿—è¾“å‡º")
        print("   3. å°è¯•ä½¿ç”¨xDiTèŠ‚ç‚¹è¿›è¡Œæ¨ç†")
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")

if __name__ == "__main__":
    main() 